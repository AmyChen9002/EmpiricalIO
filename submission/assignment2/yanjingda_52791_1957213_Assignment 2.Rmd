---
title: "Assignment2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Define the parameter variable



```{r step1}
library(ECON6120I)
library(latex2exp)
library(dplyr)
library(tibble)
library(mvtnorm)
library(ggplot2)
library(foreach)
beta_0 <- 1
beta_l <- 0.2
beta_k <- 0.7
alpha <- 0.7
sigma_eta <- 0.2
sigma_nu <- 0.5
sigma_w <- 0.1
delta <- 0.05
```


## 2. Define the production function log_production
```{r step2}
log_production <- function(l, k, omega, eta, beta_0, beta_l, beta_k){
  y <- beta_0 + beta_l * l + beta_k * k + omega + eta
  return(y)
  }
```


## 3. Derive the optimal log labor and define the production function log_production
The optimal problem faced by firm $j$ is
\[\max_{L_{jt}}\mathbb{E}_t(A_{jt}L_{jt}^{\beta_l}K_{jt}^{\beta_k}) - W_tL_{jt}\]
The first order condition is 
\[\mathbb{E}_t(A_{jt}\beta_lL_{jt}^{\beta_l-1}K_{jt}^{\beta_k}) = W_t\]
\[\mathbb{E}_t(exp(\beta_0 + \omega_{jt} + \eta_{jt})\beta_lL_{jt}^{\beta_l-1}K_{jt}^{\beta_k}) = W_t\]
Given the fact that $\omega_{jt}$ is observed,  the conditional distribution of $\beta_0 + \omega_{jt} + \eta_{jt}$ is
\[\beta_0 + \omega_{jt} + \eta_{jt} \sim N(\beta_0 + \omega_{jt}, \sigma_\eta^2)\]
Thus, the first order condition can be written as 
\[exp(\beta_0 + \omega_{jt} + \frac{1}{2}\sigma_\eta^2)\beta_lL_{jt}^{\beta_l-1}K_{jt}^{\beta_k} = W_t\]
Take log of both hands sides, we get
\[l_{jt} = \frac{1}{1 - \beta_l}(\beta_0 + ln\beta_l + \beta_k k_{jt} + \omega_{jt} + \frac{1}{2}\sigma_\eta^2 -w_t)\]
```{r step3}
log_labor_choice <- function(k, wage, omega, beta_0, beta_l, beta_k, sigma_eta){
  l <- 1/(1 - beta_l)*(beta_0 + log(beta_l) + beta_k * k + omega + 1/2 * sigma_eta^2 - log(wage))
  return(l)
  }
```

## 4. Define the log_labor_choice_error
```{r step4}
log_labor_choice_error <- function(k, wage, omega, beta_0, beta_l, beta_k, iota, sigma_eta){
  l_error <- 1/(1 - beta_l)*(beta_0 + log(beta_l) + beta_k * k + omega + iota + 1/2 * sigma_eta^2 - log(wage))
  return(l_error)
}
```

## 5. Define the investment_choice
```{r step5}
gamma <- 0.1
investment_choice <- function(k, omega, gamma, delta){
  I <- (delta + gamma * omega) * exp(k)
  return(I)
}
```

## 6. Draw initial values $k_{j0}$ and $\omega_{j0}$
The stationary distribution of an AR(1) process
\[\omega_{jt} = \alpha\omega_{j,t-1} + \nu_{jt}\]
is $N(0,\frac{\sigma_\nu^2}{1 - \alpha^2})$

```{r step6}
set.seed(1)
df <- expand.grid(j = 1:1000, t = 1) %>%
  tibble::as.tibble() %>%
  dplyr::mutate(k = rnorm(1000, mean =   1, sd = 0.5)) %>%
  dplyr::mutate(omega =  rnorm(1000, 0, sigma_nu/(1-alpha^2)^0.5)) %>%
  dplyr::mutate(wage = 0.5 * rep(1, times = 1000) ) %>%
  dplyr::arrange(j,t)
df
```

## 7. Compute the labor and investment choice of period 1
```{r step7}
df <- df %>%
  dplyr::mutate(iota = rnorm(1000, mean = 0, sd = 0.05)) %>%
  dplyr::mutate(l = log_labor_choice(k, wage, omega,
                                     beta_0, beta_l, beta_k, sigma_eta)) %>%
  dplyr::mutate(l_error = log_labor_choice_error(k, wage, omega, beta_0,
                                                 beta_l, beta_k, iota, sigma_eta)) %>% 
  dplyr::mutate(I = investment_choice(k, omega, gamma, delta)) 
df
```

## 8. Draw ex post shock and compute the output
```{r step8}
df <- df %>%
  dplyr::mutate(eta = rnorm(1000, mean = 0, sd = sigma_eta)) %>%
  dplyr::mutate(y = log_production(l, k, omega, eta, beta_0, beta_l, beta_k)) %>%
  dplyr::mutate(y_error = log_production(l_error, k, omega, eta, beta_0, beta_l, beta_k)) 
df                 
```

## 9. Repeat thie procedure for $t=1,\cdots,10$
```{r step9}
df_T <- df %>%
  dplyr::mutate(nu = rnorm(1000, 0, sigma_nu))

df_last <- df_T
for (i in 1:9){
    df_inter <- expand.grid(j = 1:1000, t = i + 1) %>%
    tibble::as.tibble() %>%
    dplyr::mutate(k = log((1-delta) * exp(df_last$k) + df_last$I)) %>%
    dplyr::mutate(omega =  alpha * df_last$omega + df_last$nu) %>%
    dplyr::mutate(wage = 0.5 * rep(1, times = 1000) ) %>%
    dplyr::mutate(iota = rnorm(1000, mean = 0, sd = 0.05)) %>%
    dplyr::mutate(l = log_labor_choice(k, wage, omega,
                                     beta_0, beta_l, beta_k, sigma_eta)) %>%
    dplyr::mutate(l_error = log_labor_choice_error(k, wage, omega, beta_0,
                                                 beta_l, beta_k, iota, sigma_eta)) %>%
    dplyr::mutate(I = investment_choice(k, omega, gamma, delta)) %>%
    dplyr::mutate(eta = rnorm(1000, mean = 0, sd = sigma_eta)) %>%
    dplyr::mutate(y = log_production(l, k, omega, eta, beta_0, beta_l, beta_k)) %>%
    dplyr::mutate(y_error = log_production(l_error, k, omega, eta, beta_0, beta_l,             beta_k)) %>% 
    dplyr::mutate(nu = rnorm(1000, 0, sigma_nu)) %>%
    dplyr::arrange(j,t)
    df_last <- df_inter
    df_T = bind_rows(df_T, df_inter)
}
df_T

```
##10. Summary statistics
```{r step10}
summary_1 <- psych::describe(df_T)
summary_T <- summary_1[,c('n','mean','sd','min','max')]
print(summary_T, digits = 7)
```
The results for $\nu$ is a bit different because I generate $\nu$s for ten period from $t=2$ to $t=11$. If I only generate it for 9 periods, the results will match yours.  But it won't be a issue for the following estimation because the period $T=11$ $\nu$ is not used for other variables.
#6.2. Estimate the parameters
## 1. Simply regress $y_{jt}$ on $k_{jt}$

```{r step21}
linearMod_1  <- lm(formula = y_error ~ l_error + k, data = df_T)
summary(linearMod_1)
```
The upward biases come from the fact that $l_{jt}$ and $k_{jt}$ are positive correlated with the expected shock $\omega_{jt}$ which can't oberved by econometricians.  This violates the OLS assumptions that $E(\epsilon|X)=0,$ which leads to upward biased estimators.

## 2. Within transform of $y_{jt}$, $l_{jt}$ and $k_{jt}$
```{r step22}
df_T_within <- df_T %>%
  group_by(j) %>%
  dplyr::mutate(dl_error = l_error - mean(l_error)) %>%
  dplyr::mutate(dk = k - mean(k)) %>%
  dplyr::mutate(dy_error = y_error - mean(y_error)) %>%
  dplyr::ungroup()
linearMod_2  <- lm(formula = dy_error ~ dl_error + dk - 1, data = df_T_within)
summary(linearMod_2)
```

## 3. First stage of Olley-Pakes
```{r step23}
bandwidth_error <- np::npplregbw(formula = y_error~l_error + k + I|k+I,data = df_T)
OP_stage_1_error <- np::npplreg(bws = bandwidth_error)
summary(OP_stage_1_error)
```
```{r figure1, echo=FALSE}
qplot(fitted(OP_stage_1_error), df_T$y_error, 
      xlab = TeX('$fitted$'), ylab = TeX('$ actual $'))
```

## 4 Check $\beta_l$ is not identified without optimization error
```{r step24}
bandwidth_1 <- np::npplregbw(formula = y~l + k + I|k+I,data = df_T)
OP_stage_1<- np::npplreg(bws = bandwidth_1)
summary(OP_stage_1)
```
## 5. Compute df_T_1st
```{r step25}
df_T_1st <- df_T[,c('j', 't', 'k', 'l_error', 'I', 'y_error')] %>%
dplyr::mutate(phi_t = y_error - coef(OP_stage_1_error)[1] * l_error
              - resid(OP_stage_1_error)) %>%
dplyr::arrange(j,t) %>%
group_by(j) %>%
dplyr::mutate(y_error_tilde = y_error - coef(OP_stage_1_error)[1] * l_error) %>%
dplyr::mutate(lag_l_error = lag(l_error)) %>%  
dplyr::mutate(lag_k = lag(k)) %>%
dplyr::mutate(lag_I = lag(I)) %>%
dplyr::mutate(phi_t_1 = lag(phi_t))
df_T_1st[,c('j', 't', 'y_error_tilde', 'phi_t_1')]
```
## 6. Moment condition function $g_{JT}(x,df_T_1st)$, where $x=(\alpha,\beta_0,\beta_k)$
```{r step26}
moment_OP_2nd(x = c(alpha, beta_0, beta_k), df_T_1st = df_T_1st)
```
## 7. The GMM optimal function $Q_{JT}(x,df_T_1st),$ where $x=(\alpha,\beta_0,\beta_k)$ 
```{r step27}
object_OP_2nd(x = c(alpha, beta_0, beta_k), df_T_1st = df_T_1st)
```
## 8. Draw the objective function when one of the three parameters $\alpha, \beta_0$ and $\beta_k$ are changed from 0 to 1 by 0.1
Yes, the objective function is minimized around the true values of $\alpha, \beta_0$ and $\beta_k$
```{r step28}
alpha_seq <- seq(0, 1, 0.1)
output <-
  foreach (alpha_input = alpha_seq,
           .combine ="rbind") %do% {
             l <- object_OP_2nd(x = c(alpha_input, beta_0, beta_k), 
                                df_T_1st = df_T_1st)
             return(l)
           }
```
```{r figure2, echo=FALSE}
qplot(alpha_seq, output, 
      xlab = TeX('$\\alpha$'), ylab = TeX('$ Objective $'))
```


```{r step29}
beta_0_seq <- seq(0, 1, 0.1)
output_beta_0 <-
  foreach (beta_0_input = beta_0_seq,
           .combine ="rbind") %do% {
             l <- object_OP_2nd(c(alpha, beta_0_input, beta_k),
                                df_T_1st = df_T_1st)
             return(l)
           }
```
```{r figure3, echo=FALSE}
qplot(beta_0_seq, output_beta_0, 
      xlab = TeX('$\\beta_0$'), ylab = TeX('$ Objective $'))
```

```{r step30}
beta_k_seq <- seq(0, 1, 0.1)
output_beta_k <-
  foreach (beta_k_input = beta_k_seq,
           .combine ="rbind") %do% {
             l <- object_OP_2nd(c(alpha, beta_0, beta_k_input),
                                df_T_1st = df_T_1st)
             return(l)
           }
```
```{r figure4, echo=FALSE}
qplot(beta_k_seq, output_beta_k, 
      xlab = TeX('$\\beta_k$'), ylab = TeX('$ Objective $'))
```

##9. Find the parameters that minimize the objective function
```{r step31}
optim(par = c(0.7, 1, 0.7), fn = object_OP_2nd,
      df_T_1st = df_T_1st, method = 'L-BFGS-B')
```
