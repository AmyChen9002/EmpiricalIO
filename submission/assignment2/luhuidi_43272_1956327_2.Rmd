---
title: 'Assignment 2'
author: "Huidi Lu, 20376663"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(summarytools)
library(np)
library(ECON6120I)
```

## Simulate Data

1. Define variables
```{r}
beta_0 = 1
beta_l = 0.2
beta_k = 0.7
alpha = 0.7
sigma_eta = 0.2
sigma_nu = 0.5
sigma_w = 0.1
delta = 0.05
```

2. Define the `log_production` function
```{r log_prod, eval=FALSE} 
log_production <- function(l,k,omega,eta,beta_0,beta_l,beta_k){
  return(beta_0+beta_l*l+beta_k*k+omega+eta)
}
```

3. Define the `log_labor_choice` function. I derive the formula by taking logrithm of equation 3.3 on both sides in the handout.
```{r log_labor, eval=FALSE}
log_labor_choice <- function(omega, k, w, beta_0, beta_l, beta_k, sigma_eta){
  return( (-log(w)+log(beta_l)+beta_0+omega+beta_k*k+0.5*sigma_eta^2)/(1-beta_l) )
}
```

4. Refine the previous function and name it `log_labor_choice_error`
```{r log_labor_error, eval=FALSE}
log_labor_choice_error <- function(omega, k, w, iota, beta_0, beta_l, beta_k, sigma_eta){
  return( log_labor_choice(omega+iota, k, w, beta_0, beta_l, beta_k, sigma_eta) )
}
```

5. Set $\gamma=0.1$. Define `investment_choice`.
```{r}
gamma = 0.1
```

```{r invest, eval=FALSE}
investment_choice <- function(delta,gamma,omega,K){
  return( (delta+gamma*omega)*K )
}
```

6. We see that $\omega_{jt}=0.7*\omega_{j,t-1}+\nu_{jt}$ where $\nu_{jt}\sim \mathbb{N}(0, 0.5^2)$. Clearly, the stationary distribution of $\omega_{jt}$ is normal with mean $m$ and standard deviation $s$ satisfying $m = 0.7*m+0$ and $s^2 = 0.7^2*s^2+0.5^2$. Therefore, $m=0, s^2=\frac{\sigma_\nu^2}{1-\alpha^2}=\frac{25}{51}$. Thus, we draw $\omega_{jt}$ from $\mathbb{N}(0, \frac{25}{51})$.

```{r}
set.seed(1)
df = tibble(j=rep(1:1000),t=rep(1,1000),k=1+rnorm(1000)/2,omega=rnorm(1000)*sqrt(sigma_nu^2 /(1-alpha^2)), wage=rep(0.5,1000))

df
```

7. 

```{r}
df$iota = 0.05*rnorm(1000)

df$l = log_labor_choice(omega=df$omega, k=df$k, w=df$wage, beta_0, beta_l, beta_k, sigma_eta)
df$l_error = log_labor_choice_error(omega=df$omega, k=df$k, w=df$wage, iota=df$iota, beta_0, beta_l, beta_k, sigma_eta)

df$I = investment_choice(delta=delta, gamma=gamma, omega=df$omega, K=exp(df$k))

df
```

8. Draw ex-post shock and compute the output.

```{r}
df$eta = sigma_eta*rnorm(1000)
df$y = log_production(l=df$l,k=df$k,omega=df$omega,eta=df$eta,beta_0,beta_l,beta_k)
df$y_error = log_production(l=df$l_error,k=df$k,omega=df$omega,eta=df$eta,beta_0,beta_l,beta_k)

df
```

9. Repeat the procedure for $t=1,\cdots10$.

```{r}
df$nu = rep(NA, 1000)

df_T = df

for (T in 2:10){
  df_next = df_T[df_T$t==T-1,]
  df_next$t = T
  df_next$nu = sigma_nu*rnorm(1000)
  df_next$omega = alpha*df_next$omega+df_next$nu
  df_next$k = log( (1-delta)*exp(df_next$k)+df_next$I )
  df_next$I = (delta+gamma*df_next$omega)*exp(df_next$k)
  df_next$l = log_labor_choice(omega=df_next$omega,k=df_next$k,w=df_next$wage,beta_0=beta_0,beta_l=beta_l,beta_k=beta_k, sigma_eta=sigma_eta)
  df_next$iota = 0.05*rnorm(1000)
  df_next$l_error = log_labor_choice_error(omega=df_next$omega,k=df_next$k,w=df_next$wage,iota=df_next$iota,beta_0=beta_0,beta_l=beta_l,beta_k=beta_k,sigma_eta=sigma_eta)
  df_next$eta = sigma_eta*rnorm(1000)
  df_next$y = log_production(l=df_next$l,k=df_next$k,omega=df_next$omega,eta=df_next$eta,beta_0,beta_l,beta_k)
  df_next$y_error = log_production(l=df_next$l_error,k=df_next$k,omega=df_next$omega,eta=df_next$eta,beta_0,beta_l,beta_k)
  df_T = rbind(df_T,df_next)
}
  

kable(descr(df_T,stats=c("n.valid", "mean", "sd", "min", "max"),transpose=TRUE),format="markdown")
```


## Estimation

1. Regress $y_{jt}$ on $l_{jt}$ and $k_{jt}$. There is an upward bias because the efficiency of the firm $\omega_{jt}$ that is unobservable to the researcher is positively correlated with firm inputs. Directly regressing $y_{jt}$ on $l_{jt}$ and $k_{jt}$ will attribute all effects to the observables.

```{r}
ols = lm(formula=y_error~l_error+k, data=df_T)
summary(ols)
```



2. Regressing the first differences.

```{r}
df_T_within = tibble(dy_error = unlist( by(df_T$y_error, df_T$j, function(x){scale(x,scale = FALSE)}) ),
                     dl_error = unlist( by(df_T$l_error, df_T$j, function(x){scale(x,scale = FALSE)}) ), 
                     dk = unlist( by(df_T$k, df_T$j, function(x){scale(x,scale = FALSE)}) ) 
                     )

dols = lm(formula=dy_error~-1+dl_error+dk, data=df_T_within)
summary(dols)
```

3. The OP method. Use `npplreg` function of `np` package to estimate a partially linear model with a multivariate kernel. First use `npplregbw` to obtain the optimal band width and then use `npplreg` to estimate the model under the optimal bandwidth.

```{r, cache=TRUE, results="hide"}
bw_error <- npplregbw(y_error ~ l_error+k+I|k+I, df_T)
```

```{r, cache=TRUE}
res_op1_error <- npplreg(bws=bw_error)
summary(res_op1_error)
qplot(fitted(res_op1_error), df_T$y_error)
```


4. Check that $\beta_l$ is not identified without optimization error. Estimate OP first-stage without optimization error.

```{r, cache=TRUE, results="hide"}
bw_no_error <- npplregbw(y ~ l+k+I|k+I, df_T)
```

```{r, cache=TRUE}
res_op1_no_error <- npplreg(bws=bw_no_error)
summary(res_op1_no_error)
```


5. OP 2nd stage. Save `df_T_1st`

```{r}
df_T_1st = df_T[,c('j','t')]
df_T_1st$y_error_tilde = df_T$y_error-df_T$l_error * coef(res_op1_error)[['l_error']]


phi_t = fitted(res_op1_error)-df_T$l_error * coef(res_op1_error)[['l_error']]

ORDER = order(df_T_1st$j)
df_T_1st = df_T_1st[ORDER,]

df_T_1st$phi_t_1 = unlist( by(phi_t[ORDER], df_T_1st$j, function(x){c(NA,x[1:9])} ) )
df_T_1st
```

6. Define the moment function. Show the value evaluated at the true parameters

```{r moment, eval=FALSE}
moment_OP_2nd <- function(alpha, beta_0, beta_k, df_T, df_T_1st){
  df_T=df_T[order(df_T$j),]
  df_T$k_1 = unlist( by(df_T$k, df_T$j, function(x){c(NA,x[1:9])} ) )
  df_T$I_1 = unlist( by(df_T$I, df_T$j, function(x){c(NA,x[1:9])} ) )
  
  op2resid = df_T_1st$y_error_tilde+(alpha-1)*(beta_0+beta_k*df_T$k)-alpha*df_T_1st$phi_t_1
  
  moments = tibble(m1=op2resid*df_T$k, m2=op2resid*df_T$k_1, m3=op2resid*df_T$I_1)
  
  return( colMeans(moments, na.rm=TRUE) )
}
```

```{r}
moment_OP_2nd(alpha=alpha, beta_0=beta_0, beta_k=beta_k, df_T=df_T, df_T_1st=df_T_1st)
```

7. Define the GMM objective function. Show the value evaluated at the true parameters

```{r objective, eval=FALSE}
objective_OP_2nd <- function(params, df_T, df_T_1st){
  mmt = moment_OP_2nd(params[1], params[2], params[3], df_T, df_T_1st)
  return( mmt %*% mmt )
}
```

```{r}
objective_OP_2nd(params=c(alpha,beta_0,beta_k), df_T=df_T, df_T_1st=df_T_1st)
```

8. Draw the graph when one of $\alpha$, $\beta_0$ and $\beta_k$ is changed from 0 to 1 by 0.1 while others are set at the true value. We see that the objective is indeed minimized at the true value.

```{r}
p1=apply(as.array(0:10/10),1,function(x){objective_OP_2nd(params=c(x,beta_0,beta_k), df_T=df_T, df_T_1st=df_T_1st)})
qplot(0:10/10, p1 ,xlab=expression(alpha), ylab='GMM Objective')

p2=apply(as.array(0:10/10),1,function(x){objective_OP_2nd(params=c(alpha,x,beta_k), df_T=df_T, df_T_1st=df_T_1st)})
qplot(0:10/10, p2 ,xlab=expression(beta_0), ylab='GMM Objective')

p3=apply(as.array(0:10/10),1,function(x){objective_OP_2nd(params=c(alpha,beta_0,x), df_T=df_T, df_T_1st=df_T_1st)})
qplot(0:10/10, p3 ,xlab=expression(beta_k), ylab='GMM Objective')
```

9. Find the parameters using `optim` and `L-BFGS-B` method.

```{r}
optim(runif(3), objective_OP_2nd, df_T=df_T, df_T_1st=df_T_1st, method='L-BFGS-B')
```