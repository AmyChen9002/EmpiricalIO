[
["index.html", "ECON 6120I Topics in Empirical Industrial Organization Chapter 1 Syllabus 1.1 Instructor Information 1.2 General Information 1.3 Required Environment 1.4 Evaluation 1.5 Academic Integrity 1.6 Schedule 1.7 Course Materials", " ECON 6120I Topics in Empirical Industrial Organization Kohei Kawaguchi Last updated: 2019-03-16 Chapter 1 Syllabus 1.1 Instructor Information Instructor: Name: Kohei Kawaguchi. Office: LSK6070, Monday 11:00-12:00. All questions related to the class have to be publicly asked on the discussion board of canvas rather than being privately asked in e-mail. The instructor usually does not reply in the evening, weekends, and holidays. 1.2 General Information 1.2.1 Class Time Date: Monday. Time: 13:30-17:20. Venue: CYTG001. 1.2.2 Description This is a PhD-level course for empirical industrial organization. This course covers various econometric methods used in industrial organization that is often referred to as the structural estimation approach. These methods have been gradually developed since 1980s in parallel with the modernization of industrial organization based on the game theory and now widely applied in antitrust policy, business strategy, and neighboring fields such as labor economics and international economics. This course presumes a good understanding of PhD-level microeconomics and microeconometrics. Participants are expected to understand at least UG-level industrial organization. This course requires participants to write programs mostly in R and sometimes in C++ to implement various econometric methods. In particular, all assignments will involve such a non-trivial programming task. Even though the understanding of these programming languages is not a prerequisite, a sharp learning curve will be required. Some experience in other programming languages will help. Audit without a credit is not admitted for students. 1.2.3 Expectation and Goals The goal of this course is to learn and practice econometric methods for empirical industrial organization. The lecture covers the econometric methods that have been developed between 80s and 00s to estimate primitive parameters governing imperfect competition among firms, such as production and cost function estimation, demand function estimation, merger simulation, entry and exit analysis, and dynamic decision models. The lecture also covers various new methods to recover model primitives in certain mechanisms such as auction, matching, network, and bargaining. The emphasis is put on the former group of methods, because they are the basis for other new methods. Participants will not only understand the theoretical background of the methods but also become able to implement these methods from scratches by writing their own programs. I will briefly discuss the latter class of new methods through reading recent papers. The participants will become able to understand and use these new methods. 1.3 Required Environment Participants should bring their laptop to the class. We have enough extension codes for students. The laptop should have sufficient RAM (at least \\(\\ge\\) 8GB, \\(\\ge\\) 16GB is recommended) and CPU power (at least Core i5 grade, Core i7 grade is recommended). Participants are fully responsible for their hardware issues. Operating System can be arbitrary. The instructor mainly uses OSX High Siera with iMac (Retina 5K, 27-inch, Late 2015) and Macbook Pro (Retina, 15-inch, Early 2017). Please install the following software by the first lecture. Technical issues related to the installment should be resolved by yourself, because it depends on your local environment. If you had an error, copy and paste the error message on a search engine, and find a solution. This solves 99.9% of the problems. R: https://www.r-project.org/ RStudio: https://www.rstudio.com/ LaTeX: MixTex https://miktex.org/ TeXLive https://www.tug.org/texlive/ MacTeX http://www.tug.org/mactex/ 1.4 Evaluation Assignments (80): In total 8 homework are assigned. Each homework involves the implementation of the methods covered in the class. Each homework has 10 points. The working hour for each homework will be around 10-20 hours. Participation (10): Every time a participant asks a question in the class, after the class, during the office hour, or in the canvas. the participant gets one point, up to 10 points. The participant who asked the question writes the name, ID number, his/her question, and my answer in a discussion board on the course website to claim a point. Referee report (10): Toward the end of the semester, a paper in industrial organization is randomly assigned to each participant. Each participant writes a critical referee report of the assigned paper in A4 2 pages that consists of the summary, contribution, strong and weak points of the paper. Grading is based on the absolute scores: A+ with more than 80 points, A with more than 70 points, A- with more than 60 points, B+ with more than 50 points, B with more than 40 points, B- with more than 30 points and C otherwise. 1.5 Academic Integrity Without academic integrity, there is no serious learning. Thus you are expected to hold the highest standard of academic integrity in the course. You are encouraged to study and do homework in groups. However, no cheating, plagiarism will be tolerated. Anyone caught cheating, plagiarism will fail the course. Please make sure adhere to the HKUST Academic Honor Code at all time (see http://www.ust.hk/vpaao/integrity/). 1.6 Schedule Introduction to structural estimation, R and RStudio Production function estimation I Production function estimation II Demand function estimation I Demand function estimation II Merger Analysis Entry and Exit Single-Agent Dynamics I Single-Agent Dynamics II, I change date due to my business trip Dynamic Game I Dynamic Game II Auction Other Mechanisms 1.7 Course Materials 1.7.1 R and RStudio Grolemund, G., 2014, Hands-On Programming with R, O’Reilly. Free online version is available: https://rstudio-education.github.io/hopr/. Wickham, H., &amp; Grolemund, G., 2017, R for Data Science, O’Reilly. Free online version is available: https://r4ds.had.co.nz/. Boswell, D., &amp; Foucher, T., 2011, The Art of Readable Code: Simple and Practical Techniques for Writing Better Code, O’Reilly. 1.7.2 Handbook Chapters Ackerberg, D., Benkard, C., Berry, S., &amp; Pakes, A. (2007). “Econometric tools for analyzing market outcomes”. Handbook of econometrics, 6, 4171-4276. Athey, S., &amp; Haile, P. A. (2007). “Nonparametric approaches to auctions”. Handbook of Econometrics, 6, 3847-3965. Berry, S., &amp; Reiss, P. (2007). “Empirical models of entry and market structure”. Handbook of Industrial Organization, 3, 1845-1886. Bresnahan, T. F. (1989). “Empirical studies of industries with market power”. Handbook of Industrial Organization, 2, 1011-1057. Hendricks, K., &amp; Porter, R. H. (2007). “An empirical perspective on auctions”. Handbook of Industrial Organization, 3, 2073-2143. Matzkin, R. L. (2007). “Nonparametric identification”. Handbook of Econometrics, 6, 5307-5368. Newey, W. K., &amp; McFadden, D. (1994). “Large sample estimation and hypothesis testing”. Handbook of Econometrics, 4, 2111-2245. Reiss, P. C., &amp; Wolak, F. A. (2007). “Structural econometric modeling: Rationales and examples from industrial organization”. Handbook of Econometrics, 6, 4277-4415. 1.7.3 Books Train, K. E. (2009). Discrete Choice Methods with Simulation, Cambridge university press. Davis, P., &amp; Garces, E. (2010). Quantitative Techniques for Competition and Antitrust Analysis, Princeton University Press. Tirole, J. (1988). The Theory of Industrial Organization, The MIT Press. 1.7.4 Papers The list of important papers are occasionally given during the course. "],
["intro.html", "Chapter 2 Introduction 2.1 Structural Estimation and Counterfactual Analysis 2.2 Setting Up The Environment", " Chapter 2 Introduction 2.1 Structural Estimation and Counterfactual Analysis 2.1.1 Example Igami (2017) “Estimating the Innovator’s Dilemma: Structural Analysis of Creative Destruction in the Hard Disk Drive Industry, 1981-1998”. Question: Does “Innovator’s Dilemma” (Christensen, 1997) or the delay of innovation among incumbents exist? Christensen argued that old winners tend to lag behind entrants even when introducing a new technology is not too difficult, with a case study of the HDD industry. Apple’s smartphone vs. Nokia’s feature phones. Amazon vs. Borders. Kodak’s digital camera. If it exists, what is the reason for that? How do we empirically answer this question? Figure 2.1: Figure 1 of Igam (2017) Hypotheses: Identify potentially competing hypotheses to explain the phenomenon. Cannibalization: Because of cannibalization, the benefits of introducing a new product are smaller for incumbents than for entrants. Different costs: The incumbents may have higher costs for innovation due to the organizational inertia, but at the same time they may have some cost advantage due to accumulated R&amp;D and better financial access. Preemption: The incumbents have additional incentive for innovation to preempt potential rivals. Institutional environment: The impacts of the three components differ across different institutional contexts such as the rules governing patents and market size. Casual empiricists pick up their favorite factors to make up a story. Serious empiricists should try to separate the contributions of each factor from data. To do so, the author develops an economic model that explicitly incorporates the above mentioned factors, while keeping the model parameters flexible enough to let the data tell the sign and size of the effects of each factor on innovation. Economic model: The time is discrete with finite horizon \\(t = 1, \\cdots, T\\). In each year, there is a finite number of firms indexed by \\(i\\). Each firm is in one of the technological states: \\[\\begin{equation} s_{it} \\in \\{\\text{old only, both, new only, potential entrant}\\}, \\end{equation}\\] where the first two states are for incumbents (stick to the old technology or start using the new technology) and the last two states are for actual and potential entrants (enter with the new technology or stay outside the market). In each year: Pre-innovation incumbent (\\(s_{it} =\\) old): exit or innovate by paying a sunk cost \\(\\kappa^{inc}\\) (to be \\(s_{i, t + 1} =\\) both). Post-innovation incumbent (\\(s_{it} =\\) both): exit or stay to be both. Potential entrant (\\(s_{it} =\\) potential entrant): give up entry or enter with the new technology by paying a sunk cost \\(\\kappa^{net}\\) (to be \\(s_{i, t + 1} =\\) new). Actual entrant (\\(s_{it} =\\) new): exit or stay to be new. Given the industry state \\(s_t = \\{s_{it}\\}_i\\), the product market competition opens and the profit of firm \\(i\\), \\(\\pi_t(s_{it}, s_{-it})\\), is realized for each active firm. As the product market competition closes: Pre-innovation incumbents draw private cost shocks and make decisions: \\(a_t^{pre}\\). Observing this, post-innovation incumbents draw private cost shocks and make decisions: \\(a_t^{post}\\). Observing this, actual entrants draw private cost shocks and make decisions: \\(a_t^{act}\\). Observing this, potential entrants draw private cost shocks and make decisions: \\(a_t^{pot}\\). This is a dynamic game. The equilibrium is defined by the concept of Markov-perfect equilibrium (Maskin &amp; Tirole, 1988). The representation of the competing theories in the model: The existence of cannibalization is represented by the assumption that an incumbent maximizes the joint profits of old and new technology products. The size of cannibalization is captured by the shape of profit function. The difference in the cost of innovation is captured by the difference in the sunk costs of innovation. The preemptive incentive for incumbents are embodied in the dynamic optimization problem for each incumbent. Econometric model: The author then turns the economic model into an econometric model. This amounts to specify which part of the economic model is observed/known and which part is unobserved/unknown. The author collects the data set of the HDD industry during 1977-99. Based on the data, the author specify the identities of active firms and their products and the technologies embodied in the products in each year to code their state variables. Moreover, by tracking the change in the state, the author code their action variables. Thus, the state and action variables, \\(s_t\\) and \\(a_t\\). These are the observables. The author does not observe: Profit function \\(\\pi_t(\\cdot)\\). Sunk cost of innovation for pre-innovation incumbents \\(\\kappa^{inc}\\). Sunk cost of entry for potential entrants \\(\\kappa^{net}\\). Private cost shocks. These are the unobservables. Among the unobservables, the profit function and sunk costs are the parameter of interets and the private cost shocks are nuissance parameters in the sense only the knowledge about the distribution of the latter is demanded. Identification: Can we infer the unobservables from the observables and the restrictions on the distribution of observable by the economic theory? The profit function is identified from estimating the demand function for each firm’s product, and estimating the cost function for each firm from using their price setting behavior. The sunk costs of innovation are identified from the conditional probability of innovation across various states. If the cost is low, the probability should be high. Estimation: The identification established that in principle we can uncover the parameters of interests from observables under the restrictions of economic theory. Finally, we apply a statistical method to the econometric model and infer the parameters of interest. Counterfactual analysis: If we can uncover the parameters of interest, we can conduct comparative statics: study the change in the endogenous variables when the exogenous variables including the model parameters are set different. In the current framework, this exercise is often called the counterfactual analysis. What if there was no cannibalization?: An incumbents separately maximizes the profit from old technology and new technology instead of jointly maximizing the profits. Solve the model under this new assumption everything else being equal. Free of cannibalization concerns, 8.95 incumbents start producing new HDDs in the first 10 years, compared with 6.30 in the baseline. The cumulative numbers of innovators among incumbents and entrants differ only by 2.8 compared with 6.45 in the baseline. Thus cannibalization can explain a significant part of the incumbent-entrant innovation gap. What if there was no preemption?: A potential entrant ignores the incumbents’ innovations upon making entry decisions. Without the preemptive motives, only 6.02 incumbents would innovate in the first 10 7ears, compared with 6.30 in the baseline. The cumulative incumbent-entrant innovation gap widen to 8.91 compared with 6.45 in the baseline. The sunk cost of entry is smaller for incumbents than for entrants in the baseline. Interpretations and policy/managerial implication: Despite the cost advantage and the preemptive motives, the speed of innovation is slower among incumbents due to the strong cannibalization effect. Incumbents that attempt to avoid the “innovator’s dilemma” should separate the decision makings between old and new sections inside the organization so that it can avoid the concern for cannibalization. 2.1.2 Recap The structural approach in empirical industrial organization consists of the following components: Research question. Competing hypotheses. Economic model. Econometric model Identification. Data collection. Data cleaning. Estimation. Counterfactual analysis. Coding. Interpretations and policy/managerial implications. The goal of this course is to be familiar with the standard methodology to complete this process. The methodology covered in this class is mostly developed to analyze the standard framework to dynamic or oligopoly competition. The policy implications are centered around competition policies. But the basic idea can be extend to different class of situations such as auction, matching, voting, contract, marketing, and so on. Note that the depth of the research question and the relevance of the policy/managerial implications are the most important part of the research. Focusing on the methodology in this class is to minimize the time to allocate to less important issues and maximize the attention and time to the most valuable part in the future research. Given a research question, what kind of data is necessary to answer the question? Given data, what kind of research questions can you address? Which question can be credibly answered? Which question can be an over-stretch? Given a research question and data, what is the best way to answer the question? What type of problem can you avoid using the method? What is the limitation of your approach? How will you defend the possible referee comments? Given a result, what kinds of interpretation can you credibly derive? What kinds of interpretation can be contested by potential opponents? What kinds of contribution can you claim? To address these issues is necessary to publish a paper and it is necessary to be familiar with the methodology to do so. 2.1.3 Historical Remark The words reduced-form and structural-form date back to the literature of estimation of simultaneous equations in macroeconomics (Hsiao, 1983). Let \\(y\\) be the vector of observed endogenous variables, \\(x\\) be the vector of observed exogenous variables, and \\(\\epsilon\\) be the vector of unobserved exogenous variables. The equilibrium condition for \\(y\\) on \\(x\\) and \\(\\epsilon\\) is often written as: \\[\\begin{equation} Ay + Bx = \\Sigma \\epsilon. \\tag{2.1} \\end{equation}\\] These equations implicitly determine the vector of endogenous variables \\(y\\) . If \\(A\\) is invertible, we can solve the equations for \\(y\\) to obtain: \\[\\begin{equation} y = - A^{-1} B x + A^{-1} \\Sigma \\epsilon. \\tag{2.2} \\end{equation}\\] These equations explicitly determine the vector of endogenous variables \\(y\\). Equation (2.1) is the structural-form and (2.2) is the reduced-form. If \\(y\\) and \\(x\\) are observed and \\(x\\) is of full column rank, then \\(A^{-1}B\\) and \\(A^{-1} \\Sigma A^{-1}\\) will be estimated by regression for (2.2). But this does not mean that \\(A, B\\) and \\(\\Sigma\\) are separately estimated. This was the traditional identification problems. Thus, reduced-form does not mean either of: Regression analysis; Statistical analysis free from economic assumptions. Recent development in this line of literature of identification is found in Matzkin (2007). In econometrics, the idea of imposing restrictions from economic theories seems to have been formalized by the work of Manski (1994) and Matzkin (1994). 2.2 Setting Up The Environment Assume that R, RStudio and LaTex are all installed in the local computer. 2.2.1 RStudio Project The assignments should be conducted inside a project folder for this course. File &gt; New Project...&gt; New Directory &gt; New Directory &gt; R Package using RcppEigen. Name the directory ECON6120I and place in your favorite location. You can open this project from the upper right menu of RStudio or by double clicking the ECON6120I.Rproj file in the ECON6120I directory. This navigates you to the root directory of the project. In the root directory, make folders named: assignment. input. output. figuretable. We will store R functions in R folder, C/C++ functions in src folder, and data in input folder, data generated from the code in output, and figures and tables in figurtable folder. Open src/Makevars and erase the content. Then, write: PKG_CPPFLAGS = -w -std=c++11 -O3 Open src/Makevars.win and erase the content. Then, write: PKG_CPPFLAGS = -w -std=c++11 2.2.2 Basic Programming in R File &gt; New File &gt; R Script to open Untitled file. Ctrl (Cmd) + S to save it with test.R in assignment folder. In the console, type and push enter: 1 + 1 ## [1] 2 100:130 ## [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 ## [18] 117 118 119 120 121 122 123 124 125 126 127 128 129 130 This is the interactive way of using R functionalities. In test.R, write: 1 + 1 Then, save the file and push Run. Alternatively, place the mouse over the 1 + 1 line in test.R file. Then, Ctrl (Cmd) + Enter to run the line. In this way, we can write procedures in the file and send to the console to run. There are functions to conduct basic calculations: 1 + 2 ## [1] 3 2 * 3 ## [1] 6 4 - 1 ## [1] 3 6 / 2 ## [1] 3 2^3 ## [1] 8 We can define objects and assign values to them. a &lt;- 1 a ## [1] 1 a + 2 ## [1] 3 In addition to scalar object, we can define a vector by: 2:10 ## [1] 2 3 4 5 6 7 8 9 10 3:20 ## [1] 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 c(2, 3, 5, 9, 10) ## [1] 2 3 5 9 10 seq(1, 10, 2) ## [1] 1 3 5 7 9 seq is a function with initial value, end values, and the increment value. By typing seq in the help, we can read the manual page of the function. seq {base} means that this function is named seq and is contained in the library called base. Some libraries are automatically called when the R is launched, but some are not. Some libraries are even not installed. We can install a library from a repository called CRAN. install.packages(&quot;ggplot2&quot;) To use the package, we have to load by: library(ggplot2) Use qplot function in ggplot2 library to draw a scatter plot. x &lt;- c(-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.7, 1) y &lt;- x^3 qplot(x, y) - Instead of loading a package by library, you can directly call it as: ggplot2::qplot(x, y) We can write own functions. roll &lt;- function(n) { die &lt;- 1:6 dice &lt;- sample(die, size = n, replace = TRUE) y &lt;- sum(dice) return(y) } roll(1) ## [1] 4 roll(2) ## [1] 5 roll(10) ## [1] 42 roll(10) ## [1] 39 We can set.seed to obtain the same realization of random variables. set.seed(1) roll(10) ## [1] 38 set.seed(1) roll(10) ## [1] 38 When a variable used in a function is not given as its argument, the function calls the variable in the global environment: y &lt;- 1 plus_1 &lt;- function(x) { return(x + y) } plus_1(1) ## [1] 2 plus_1(2) ## [1] 3 However, you should NOT do this. All variables used in a function should be given as its arguments: y &lt;- 1 plus_2 &lt;- function(x, y) { return(x + y) } plus_2(2, 3) ## [1] 5 plus_2(2, 4) ## [1] 6 The best practice is to use findGlobals function in codetools to check global variables in a funciton: library(codetools) findGlobals(plus_1) ## [1] &quot;{&quot; &quot;+&quot; &quot;return&quot; &quot;y&quot; findGlobals(plus_2) ## [1] &quot;{&quot; &quot;+&quot; &quot;return&quot; This function returns the list of global variables used in a function. If this returns a global variable other than the system global gariables, you should include it as the argument of the function. You can write the functions in the files with executing codes. But I recommend you to separate files for writing functions and executing codes. File &gt; New File &gt; R Rcript and name it as functions.R and save to R folder. Cut the function you wrote and paste it in functions.R. There are two ways of calling a function in functions.R from test.R. One way is to use source function. source(&quot;R/functions.R&quot;) When this line is read, the codes in the file are executed. The other way is to bundle functions as a package and load it. Choose Build &gt; Clean and Rebuild. This compiles files in src folder and bundle functions in R folder and build a package named ECON6120I. Now, the functions in R folder and src folder can be used by loading the package by: library(ECON6120I) Best practice: Write functions in the scratch file. As the functions are tested, move them to R/functions.R. Clean and rebuild and load them as a package. 2.2.3 Reproducible Reports using Rmarkdown Reporting in empirical studies involves: Writing texts; Writing formulas; Writing and implementing programs; Demonstrating the results with figures and tables. Moreover, this has to be done in a reproducible manner: Whoever can reproduce the output from the scratch. “Whoever” includes yourself in the future. Because the revision process of structural papers is usually lengthy, you often have to remember the content few weeks or few months later. It is inefficient if you cannot recall what you have done. We use Rmarkdown to achieve this goal. This assumes that you have LaTex installed. Install package Rmarkdown: install.packages(&quot;rmarkdown&quot;) File &gt; New File &gt; R Markdown... &gt; HTML with title Test. Save it in assignment folder with name test.Rmd. From Knit tab, choose Knit to HTML. This outputs the content to html file. You can also choose Knit to PDF from Knit tab to obtain output in pdf file. Reports should be knit to pdf to submit. But you can use html output while writing a report because html is lighter to compile. Refer to the help page for further information. References "],
["production.html", "Chapter 3 Production and Cost Function Estimation 3.1 Motivations 3.2 Analyzing Producer Behaviors 3.3 Production Function Estimation 3.4 Cost Function Estimation", " Chapter 3 Production and Cost Function Estimation 3.1 Motivations Estimating production and cost functions of producers is the cornerstone of economic analysis. Estimating the functions includes to separate the contribution of observed inputs and the other factors, which is often referred to as the productivity. “What determines productivity?” (Syverson, 2011)-type research questions naturally follow. The methods covered in this chapter are widely used across different fields. Some of them are variants from the standard methods. 3.1.1 IO Olley &amp; Pakes (1996): How much did the deregulation in the U.S. telecommunication industry, in particular the divestiture of AT&amp;T in 1984, spurred the productivity growth of the incumbent, facilitated entries, and increased the aggregate productivity? To do so, the authors estimate the plant-level production functions and productivity in the telecommunication industry. Doraszelski &amp; Jaumandreu (2013): What is the role of R&amp;D in determining the differences in productivity across firms and the evolution of firm-level productivity over time? To do so, the authors estimate the firm-level production functions and productivity of Spanish manufacturing firms during 1990s in which the transition probability of a productivity is a function of the R&amp;D activities. 3.1.2 Development Hsieh &amp; Klenow (2009): How large is the misallocation of inputs across manufacturing firms in China and India compared to the U.S? How will the aggregate productivity of China and India change if the degree of misallocation is reduced to the U.S. level? To do so, the authors measure the revenue productivity of firms, which should be the same across firms within an industry if there were no distortion, and the measurement of the revenue productivity requires to estimate the production function. Gennaioli, La Porta, Lopez-de-Silanes, &amp; Shleifer (2013): What are the determinants of regional growth? Do geographic, institutional, cultural, and human capital factors explain the difference across regions? To do so, the authors construct the data set that covers 74% of the world’s surface and 97% of its GDP and estimate the production function in which the above mentioned factors could affect the productivity. 3.1.3 Trade Haskel, Pereira, &amp; Slaughter (2007): Are there spillovers from FDI to domestic firms? To do so, the authors estimate the plant-level production function of the U.K. manufacturing firms during 1973 and 1992 and study how the foreign presence in the U.K. affected the productivity. De Loecker (2011): Does the removal of trade barriers induces efficiency gain for producers? To do so, the author estimate the production functions of Belgian textile industry during 1994-2002 in which the degree of trade protection can affect the productivity level. 3.1.4 Management Bloom &amp; Van Reenen (2007): How do management practices affect the firm productivity? To do so, the authors first estimate the production function and productivity of manufacturing firms in developed countries, and then study how the independently measured management practices of the firms affect the estimated productivity. Braguinsky, Ohyama, Okazaki, &amp; Syverson (2015): How do changes in ownership affect the productivity and profitability of firms? To do so, the authors estimate the production function for various outputs including the physical output, return on capital and labor, and the utilization rate, price level, using the cotton spinners data in Japan during 1896 and 1920. 3.1.5 Education Cunha, Heckman, &amp; Schennach (2010): How do childhood and schooling interventions “produce” the cognitive and non-cognitive skills of children? To do so, the authors estimate the mapping from childhood and schooling interventions to children’s cognitive and non-cognitive skills, the “production function” of childhood environment and education. 3.2 Analyzing Producer Behaviors There are several levels of parameters that govern the behavior of firms: Production function Add factor market structure. Add cost minimization. \\(\\rightarrow\\) Cost function Add product market structure. Add profit maximization. \\(\\rightarrow\\) Supply function (Pricing function) Combine cost and supply (pricing) functions. \\(\\rightarrow\\) Profit function Which parameter to identify? Primitive enough to be invariant to relevant policy changes. e.g. If you conduct a policy experiment that changes the factor market structure, identifying cost functions is not enough. As reduced-form as possible among such specifications. A reduced-form parameter usually can be rationalized by a class of underlying structural parameters and institutional assumptions. Thus, the analysis becomes robust to some misspecifications. e.g. A non-parametric function \\(C(q, w)\\) can represent a cost function of a producer who is not necessarily minimizing the cost. If we derive a cost function from a production function and a factor market structure, then the cost function cannot represent such a non-optimization behavior. 3.3 Production Function Estimation 3.3.1 Cobb-Douglas Specification as a Benchmark Most of the following argument carries over to a general model. For firm \\(j = 1, \\cdots, J\\) and time \\(t = 1, \\cdots, T\\), we observe output \\(Y_{jt}\\), labor \\(L_{jt}\\), and capital \\(K_{jt}\\). We consider an asymptotic of \\(J \\to \\infty\\) for a fixed \\(T\\). Assume Cobb-Douglas production function: \\[\\begin{equation} Y_{jt} = A_{jt} L_{jt}^{\\beta_l} K_{jt}^{\\beta_k}, \\end{equation}\\] where \\(A_{jt}\\) is firm \\(j\\) and time \\(t\\) specific unobserved heterogeneity in the model. Taking the logarithm gives: \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\epsilon_{jt}, \\end{equation}\\] where lowercase symbols represent natural logs of variables and \\(\\ln(A_{jt}) = \\beta_0 + \\epsilon_{jt}\\). This can be regarded as a first-order log-linear approximation of a production function. Linear regression model! May OLS work? 3.3.2 Potential Bias I: Endogeneity \\(\\epsilon_{jt}\\) contains everything that cannot be explained by the observed inputs: better capital may be employed, a worker may have obtained better skills, etc. When the manager of a firm makes an input choice, she should have some information about the realization of \\(\\epsilon_{jt}\\). Thus, the input choice can be correlated with \\(\\epsilon_{jt}\\); for example under static optimization of \\(L_{jt}\\) given \\(K_{jt}\\): \\[\\begin{equation} L_{jt} = \\Bigg[\\frac{p_{jt}}{w_{jt}} \\beta_l \\exp^{\\beta_0 + \\epsilon_{jt}} K_{jt}^{\\beta_k}\\Bigg]^{\\frac{1}{1 - \\beta_l}}. \\end{equation}\\] In this case, OLS estimator for \\(\\beta_l\\) is biased, because when \\(\\epsilon_{jt}\\) is high, \\(l_{jt}\\) is high and thus the increase in output caused by \\(\\epsilon_{jt}\\) is captured as if caused by the increase in labor input. The endogeneity problem was already recognized by Marschak &amp; Andrews (1944). 3.3.3 Potential Bias II: Selection Firms freely enter and exit market. Therefore, a firm that had low \\(\\epsilon_{jt}\\) is likely to exit. However, if firms have high capital \\(K_{jt}\\), it can stay in the market even if the realization of \\(\\epsilon_{jt}\\) is very low. Therefore, conditional on being in the market, there is a correlation between the capital \\(K_{jt}\\) and \\(\\epsilon_{jt}\\). This problem occurs even if the choice of \\(K_{jt}\\) itself is not a function of \\(\\epsilon_{jt}\\). 3.3.4 How to Resolve Endogeneity Bias? Temporarily abstract away from entry and exit. The data is balanced. Panel data. First-order condition for inputs. Instrumental variable. Olley-Pakes approach and its followers/critics. Griliches &amp; Mairesse (1998) is a good survey of the history up to Olley-Pakes approach. Daniel A. Ackerberg, Caves, &amp; Frazer (2015) also offer a good survey and clarify problems and implicit assumptions in Olley-Pakes approach. 3.3.5 Panel Data Assume that \\(\\epsilon_{jt} = \\mu_j + \\eta_{jt}\\), where \\(\\eta_{jt}\\) is uncorrelated with input choices up to period \\(t\\): \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\mu_j + \\eta_{jt}. \\end{equation}\\] Then, by differentiating period \\(t\\) and \\(t - 1\\) equations, we get: \\[\\begin{equation} y_{jt} - y_{j, t - 1}= \\beta_l (l_{jt} - l_{j, t - 1}) + \\beta_k (k_{jt} - k_{j, t - 1}) + (\\eta_{jt} - \\eta_{j, t - 1}). \\end{equation}\\] Then, because \\(\\eta_{jt} - \\eta_{j, t - 1}\\) is uncorrelated either with \\(l_{jt} - l_{j, t - 1}\\) or \\(k_{jt} - k_{j, t - 1}\\), we can identify the parameter. Problem: Restrictive heterogeneity. When there are measurement errors, fixed-effect estimator can generate higher biases than OLS estimator, because measurement errors more likely to survive first-difference and within-transformation. 3.3.6 First-Order Condition for Inputs Use the first-order condition for inputs as the moment condition (McElroy, 1987). Closely related to the cost function estimation literature. Need to specify the factor market structure and the nature of the optimization problem for a firm. Recently being center of attention again as one of the solutions to the “collinearity problem” discussed below. 3.3.7 Instrumental Variable Borrow the idea from the first-order condition approach that the input choices are affected by some exogenous variables. If we have instrumental variables that affect inputs but are uncorrelated with errors \\(\\epsilon_{jt}\\), then we can identify the parameter by an instrumental variable method. One candidate for the instrumental variables: input prices. Input price affect input decision. Input price is not correlated with \\(\\epsilon_{jt}\\) if the factor product market is competitive and \\(\\epsilon_{jt}\\) is an idiosyncratic shock to a firm. Problems: Input prices often lack cross-sectional variation. Cross-sectional variation is often due to unobserved input quality. Another candidate for the instrumental variables: lagged inputs. If \\(\\epsilon_{jt}\\) does not have auto-correlation, lagged inputs are not correlated with the current shock. If there are adjustment costs for inputs, then lagged inputs are correlated with the current inputs. Problem: If \\(\\epsilon_{jt}\\) has auto-correlation, all lagged inputs are correlated with the errors: For example, if \\(\\epsilon_{jt}\\) is AR(1), \\(\\epsilon_{jt} = \\alpha \\epsilon_{j, t - 1} + \\nu_{j, t - 1} = \\cdots \\alpha^l \\epsilon_{j, t - l} + \\nu_{j, t - 1} + \\cdots, \\alpha^{l - 1} \\nu_{j, t - l}\\) for any \\(l\\). 3.3.8 Olley-Pakes Approach Exploit restrictions from the economic theory (Olley &amp; Pakes, 1996). Write \\(\\epsilon_{jt} = \\omega_{jt} + \\eta_{jt}\\), where \\(\\omega_{jt}\\) is an anticipated shock and \\(\\eta_{jt}\\) is an ex-post shock. Inputs are correlated with \\(\\omega_{jt}\\) but not with \\(\\eta_{jt}\\) The model is written as: \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\omega_{jt} + \\eta_{jt}. \\end{equation}\\] OP use economic theory to derive a valid proxy for the anticipated shock \\(\\omega_{jt}\\). 3.3.9 Assumption I: Information Set The firm’s information set at \\(t\\), \\(I_{jt}\\), includes current and past productivity shocks \\(\\{\\omega_{j\\tau}\\}_{\\tau = 0}^t\\) but does not include future productivity shocks \\(\\{\\omega_{j\\tau}\\}_{\\tau = t + 1}^{\\infty}\\). The transitory shocks \\(\\eta_{jt}\\) satisfy \\(\\mathbb{E}\\{\\eta_{jt}|I_{jt}\\} = 0\\). 3.3.10 Assumption II: First Order Markov Productivity shocks evolve according to the distribution: \\[\\begin{equation} p(\\omega_{j, t + 1}|I_{jt}) = p(\\omega_{j, t + 1}|\\omega_{jt}), \\end{equation}\\] and the distribution is known to firms and stochastically increasing in \\(\\omega_{jt}\\). Then: \\[\\begin{equation} \\omega_{jt} = \\mathbb{E}\\{\\omega_{jt}|\\omega_{j, t - 1}\\} + \\nu_{jt}, \\end{equation}\\] and: \\[\\begin{equation} \\mathbb{E}\\{\\nu_{jt}|I_{j, t - 1}\\} = 0, \\end{equation}\\] by construction. 3.3.11 Assumption III: Timing of Input Choices Firms accumulate capital according to: \\[\\begin{equation} k_{jt} = \\kappa(k_{j, t - 1}, i_{j, t - 1}), \\end{equation}\\] where investment \\(i_{j, t - 1}\\) is chosen in period \\(t - 1\\). Labor input \\(l_{jt}\\) is non-dynamic and chosen at \\(t\\). This assumption characterizes and distinguishes labor and capital. Intuitively, it takes a full period for new capital to be ordered, delivered, and installed. 3.3.12 Assumption IV: Scalar Unobservable Firms’ investment decisions are given by: \\[\\begin{equation} i_{jt} = f_t(k_{jt}, \\omega_{jt}). \\end{equation}\\] This assumption places strong implicit restrictions on additional firm-specific unobservables. No across firm unobserved heterogeneity in adjustment cost of capital, in demand and labor market conditions, or in other parts of the production function. Okay with across time unobserved heterogeneity. 3.3.13 Assumption IV: Strict Monotonicity The investment policy function \\(f_t(k_{jt}, \\omega_{jt})\\) is strictly increasing in \\(\\omega_{jt}\\). This holds if the realization of higher \\(\\omega_{jt}\\) implies higher expectation for future productivity (Assumption III) and if the marginal product of capital is increasing in the expectation for future productivity. To verify the latter condition in a given game is often not easy. 3.3.14 Two-step Approach: The First Step Insert \\(\\omega_{jt} = h(k_{jt}, i_{jt})\\) to the original equation to get: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_l l_{jt} + \\underbrace{\\beta_0 + \\beta_k k_{jt} + h(k_{jt}, i_{jt})}_{\\text{unknown function of $k_{jt}$ and $i_{jt}$}} + \\eta_{jt}\\\\ &amp; \\equiv \\beta_l l_{jt} + \\phi(k_{jt}, i_{jt}) + \\eta_{jt}. \\end{split} \\end{equation}\\] This is a partially linear model: see Ichimura &amp; Todd (2007) for reference. Because \\(l_{jt}, k_{jt}\\) and \\(i_{jt}\\) are uncorrelated with \\(\\eta_{jt}\\), we can identify \\(\\beta_l\\) and \\(\\phi(\\cdot)\\) by exploiting the moment condition: \\[\\begin{equation} \\begin{split} &amp; \\mathbb{E}\\{\\eta_{jt}|l_{jt}, k_{jt}, i_{jt}\\} = 0\\\\ &amp; \\Leftrightarrow \\mathbb{E}\\{y_{jt} - \\beta_l l_{jt} - \\phi(k_{jt}, i_{jt}) |l_{jt}, k_{jt}, i_{jt}\\} = 0. \\end{split} \\end{equation}\\] if there is enough variation in \\(l_{jt}, k_{jt}\\) and \\(i_{jt}\\). This “if there is enough variation” part is actually problematic. Discuss later. Let \\(\\beta_l^0\\) and \\(\\phi^0\\) be the identified true parameters. 3.3.15 Two-step Approach: The Second Step Note that: \\[\\begin{equation} \\omega_{jt} \\equiv \\phi(k_{jt}, i_{jt}) - \\beta_0 - \\beta_k k_{jt}. \\end{equation}\\] Therefore, we have: \\[\\begin{equation} \\begin{split} &amp;y_{jt} - \\beta_l^0 l_{jt} \\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + \\omega_{jt} + \\eta_{jt}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + g(\\omega_{j, t - 1}) + \\nu_{jt} + \\eta_{jt}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + g[\\phi^0(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})] + \\nu_{jt} + \\eta_{jt}. \\end{split} \\end{equation}\\] \\(\\nu_{jt}\\) and \\(\\eta_{jt}\\) are independent of the covariates. This is a multiple-index model with indices \\(\\beta_0 + \\beta_1 k_{jt}\\) and \\(\\beta_0 + \\beta_1 k_{j, t - 1}\\) where parameters of two indices are restricted to be the same: see Ichimura &amp; Todd (2007) for reference. We can identify \\(\\beta_0, \\beta_k\\) and \\(g\\) by exploiting the moment condition: \\[\\begin{equation} \\begin{split} &amp; \\mathbb{E}\\{\\nu_{jt} + \\eta_{jt}|k_{jt}, k_{j, t - 1}, i_{j, t - 1}\\} = 0\\\\ &amp; \\Leftrightarrow \\mathbb{E}\\{\\nu_{jt} + \\eta_{jt}|k_{jt}, k_{j, t - 1}, i_{j, t - 1}\\} = 0. \\end{split} \\end{equation}\\] 3.3.16 Identification of the Anticipated Shocks If \\(\\phi, \\beta_0, \\beta_k\\) are identified, then \\(\\omega_{jt}\\) is also identified by: \\[\\begin{equation} \\omega_{jt} \\equiv \\phi(k_{jt}, i_{jt}) - \\beta_0 - \\beta_k k_{jt}. \\end{equation}\\] 3.3.17 Two-Step Estimation of Olley &amp; Pakes (1996). First step: Estimate \\(\\beta_L\\) and \\(\\phi\\) in : \\[\\begin{equation} \\begin{split} y_{jt} = \\beta_l l_{jt} + \\phi(k_{jt}, i_{jt}) + \\eta_{jt}. \\end{split} \\end{equation}\\] by approximating \\(\\phi\\) with some basis functions, say, polynomials or splines: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_l l_{jt} + \\sum_{p = 1}^P \\gamma_p \\phi_p(k_{jt}, i_{jt}) + \\left[\\phi(k_{jt}, i_{jt}) - \\sum_{n = 1}^N \\gamma_n \\phi_n(k_{jt}, i_{jt})\\right] + \\eta_{jt}\\\\ &amp; = \\beta_l l_{jt} + \\sum_{p = 1}^P \\gamma_p \\phi_p(k_{jt}, i_{jt}) + \\tilde{\\eta}_{jt} \\end{split} \\end{equation}\\] where \\(P \\to \\infty\\) when the sample size goes to infinity. e.g. second-order polynomial approximation: \\[\\begin{equation} \\begin{split} &amp; \\phi_1(k_{jt}, i_{jt}) = k_{jt}, \\phi_2(k_{jt}, i_{jt}) = i_{jt}\\\\ &amp; \\phi_3(k_{jt}, i_{jt}) = k_{jt}^2, \\phi_4(k_{jt}, i_{jt}) = i_{jt}^2\\\\ &amp; \\phi_5(k_{jt}, i_{jt}) = k_{jt} i_{jt}. \\end{split} \\end{equation}\\] Once the basis functions are fixed, estimation is the same as the linear model. But the inference (the computation of the standard deviation) is difference, because of the approximation error. See Chen (2007) for reference. Let \\(\\hat{\\beta}_l\\) and \\(\\hat{\\phi}\\) be the estimates from the first step. Second step: Estimate \\(\\beta_0\\), \\(\\beta_k\\), and \\(g\\) in: \\[\\begin{equation} \\begin{split} y_{jt} - \\hat{\\beta}_l l_{jt}&amp; = \\beta_0 + \\beta_k k_{jt} + g[\\hat{\\phi}(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})] + \\nu_{jt} + \\eta_{jt}\\\\ &amp;+ [\\beta_l - \\hat{\\beta}_l] l_{jt}\\\\ &amp;+ \\left\\{g[\\phi(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})] - g[\\hat{\\phi}(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})]\\right\\}\\\\ &amp; = \\beta_0 + \\beta_k k_{jt} + g[\\hat{\\phi}(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})] + \\nu_{jt} + \\tilde{\\eta}_{jt} \\end{split} \\end{equation}\\] by approximating \\(g\\) by some basis functions, say, polynomials or splines. 3.3.18 From An Economic Models to An Econometric Model Starting from economic model with some unobserved heterogeneity, we reach some reduced-form model. If the resulting model belongs to a class of econometric models whose identification and estimation are established, we can simply apply the existing methods. 3.3.19 How to Resolve Selection Bias Use propensity score to correct selection bias: Ahn &amp; Powell (1993). At the beginning of period \\(t\\), after observing \\(\\omega_{jt}\\), firm \\(j\\) decides whether to continue the business (\\(\\chi_{jt} = 1\\)) or exit (\\(\\chi_{jt} = 0)\\). Assume that the difference between continuation and exit values is strictly increasing in \\(\\omega_{jt}\\). Then, there is a threshold \\(\\underline{\\omega}(k_{jt})\\) such that: \\[\\begin{equation} \\chi_{jt} = \\begin{cases} 1 &amp;\\text{ if } \\omega_{jt} \\ge \\underline{\\omega}(k_{jt})\\\\ 0 &amp;\\text{ otherwise.} \\end{cases} \\end{equation}\\] We can only observe firms that satisfy \\(\\chi_{jt} = 1\\). 3.3.20 Correction in the First Step In the first step, we need no correction because: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{y_{jt}|l_{jt}, k_{jt}, i_{jt}, \\chi_{jt} = 1 \\}\\\\ &amp;=\\beta_l l_{jt} + \\phi(k_{jt}, i_{jt}) + \\mathbb{E}\\{\\eta_{jt}|\\chi_{jt} = 1\\}\\\\ &amp;= \\beta_l l_{jt} + \\phi(k_{jt}, i_{jt}). \\end{split} \\end{equation}\\] Ex-post shock \\(\\eta_{jt}\\) is independent of continuation/exit decision. Therefore, we can identify \\(\\beta_l\\) and \\(\\phi(\\cdot)\\) as in the previous case. 3.3.21 Correction in the Second Step I: The Source of Bias One the other hand, we need correction in the second step, because: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{y_{jt} - \\beta_l^0 l_{jt}|k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\chi_{jt} = 1\\} \\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + g[\\phi^0(k_{jt}, i_{jt}) - (\\beta_0 + \\beta_k k_{jt})]\\\\ &amp; + \\mathbb{E}\\{\\nu_{jt} + \\eta_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\chi_{jt} = 1\\}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + g[\\phi^0(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})]\\\\ &amp; + \\mathbb{E}\\{\\nu_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1} , \\chi_{jt} = 1\\}. \\end{split} \\end{equation}\\] and \\[\\begin{equation} \\mathbb{E}\\{\\nu_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\chi_{jt} = 1 \\} \\neq 0, \\end{equation}\\] since anticipated shock matters continuation/exit decision in period \\(t\\). 3.3.22 Correction in the Second Step II: Conditional Exit Probability Let’s see that the conditional expectation: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{\\omega_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\chi_{jt} = 1 \\}\\\\ &amp;=\\mathbb{E}\\{\\omega_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\omega_{jt} \\ge \\underline{\\omega}(k_{jt}) \\}\\\\ &amp;=\\int_{\\underline{\\omega}(k_{jt})} \\omega_{jt} \\frac{p(\\omega_{jt}|\\omega_{j, t - 1})}{\\int_{\\underline{\\omega}(k_{jt})} p(\\omega|\\omega_{j, t - 1}) d\\omega } d \\omega_{jt}\\\\ &amp;\\equiv \\tilde{g}(\\omega_{j, t - 1}, \\underline{\\omega}(k_{jt})), \\end{split} \\end{equation}\\] is a function of \\(\\omega_{j, t - 1}\\) and \\(\\underline{\\omega}(k_{jt})\\). 3.3.23 Correction in the Second Step III: Invertibility in Threshold The propensity of continuation conditional on observed information up to period \\(t - 1\\): \\[\\begin{equation} \\begin{split} P_{jt} &amp;\\equiv \\mathbb{P}\\{\\chi_{jt} = 1|\\mathcal{I}_{j, t - 1}\\}\\\\ &amp;= \\mathbb{P}\\{\\omega_{jt} \\ge \\underline{\\omega}(k_{jt}) |\\mathcal{I}_{j, t - 1}\\}\\\\ &amp;= \\mathbb{P}\\{g(\\omega_{j, t - 1}) + \\nu_{jt} \\ge \\underline{\\omega}[(1 - \\delta) k_{j, t - 1} + i_{j, t - 1}]|\\mathcal{I}_{j, t - 1} \\}\\\\ &amp;= \\mathbb{P}\\{ \\chi_{jt} = 1| i_{j, t - 1}, k_{j, t - 1}\\}. \\end{split} \\end{equation}\\] \\(\\rightarrow\\) It suffices to condition on \\(i_{j, t - 1}, k_{j, t - 1}\\). We also have: \\[\\begin{equation} P_{jt} = \\mathbb{P}\\{\\chi_{jt} = 1| \\omega_{j, t - 1}, \\underline{\\omega}(k_{jt})\\}, \\end{equation}\\] and it is invertible in \\(\\underline{\\omega}(k_{jt})\\), that is, \\[\\begin{equation} \\underline{\\omega}(k_{jt}) \\equiv \\psi(P_{jt}, \\omega_{j, t - 1}). \\end{equation}\\] 3.3.24 Correction in the Second Step IV: Controlling the Threshold Now, he have: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{y_{jt} - \\beta_l^0 l_{jt}|k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1}, \\chi_{jt} = 1\\} \\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + \\mathbb{E}\\{\\omega_{jt}| k_{jt}, i_{jt}, k_{j, t - 1}, l_{j, t - 1} , \\chi_{jt} = 1\\}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + \\tilde{g}(\\omega_{j, t - 1}, \\underline{\\omega}(k_{jt}))\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + \\tilde{g}(\\omega_{j, t - 1}, \\psi(P_{jt}, \\omega_{j, t - 1}))\\\\ &amp;\\equiv \\beta_0 + \\beta_k k_{jt} + \\tilde{\\tilde{g}}(\\omega_{j, t - 1}, P_{jt})\\\\ &amp;= \\beta_0 + \\beta_k k_{jt} + \\tilde{\\tilde{g}}[\\phi^0(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1}), P_{jt}]. \\end{split} \\end{equation}\\] At the end, the only difference is to include \\(P_{jt}\\) as a covariate. \\(P_{jt}\\) is a known function of \\(i_{j, t - 1}\\) and \\(k_{j, t - 1}\\). Even if we condition on \\(P_{jt} = p\\), there are still many combinations of \\(i_{j, t - 1}\\) and \\(k_{j, t - 1}\\) that gives \\(P_{jt} = p\\). With this remaining variation, we can identify \\(\\beta_0\\), \\(\\beta_k\\), and \\(\\tilde{\\tilde{g}}\\) by the same argument as the case without selection, for each \\(P_{jt} = p\\). 3.3.25 Three Step Estimation of Olley &amp; Pakes (1996) Zero step: Estimate the propensity score: \\[\\begin{equation} P_{jt} = 1\\{\\chi_{jt} = 1| i_{j, t - 1}, k_{j, t - 1}\\}, \\end{equation}\\] by a kernel estimator. Insert the resulting estimates \\(\\widehat{P}_{jt}\\) into the first and second steps. 3.3.26 Zero Investment Problem One of the key assumptions in OP method was invertibility between anticipated shock and investment: \\[\\begin{equation} \\omega_{jt} = i^{-1}(k_{jt}, i_{jt}) \\equiv h(k_{jt}, i_{jt}). \\end{equation}\\] However, in micro data, zero investment is a rule rather than exceptions. Then, the invertibility does not hold globally: there are some region of the anticipated shock in which the investment takes value zero. 3.3.27 Tackle Zero Investment Problem I: Discard Some Data Discard a data \\((j, t)\\) such that \\(i_{j, t - 1} = 0\\). Use a data \\((j, t)\\) such that \\(i_{j, t - 1} &gt; 0\\). Then, invertibility recovers on this selected sample. This cause bias in the estimator because \\(\\nu_{jt}\\) in : \\[\\begin{equation} \\beta_0 + \\beta_l k_{jt} + g[\\phi^0(k_{j, t - 1}, i_{j, t - 1}) - (\\beta_0 + \\beta_k k_{j, t - 1})] + \\nu_{jt} + \\eta_{jt}, \\end{equation}\\] is independent of the event up to \\(t - 1\\), including \\(i_{j, t - 1}\\). However, this cause information loss. The loss is high if the proportion of the sample such that \\(i_{j, t - 1} = 0\\) is high. 3.3.28 Tackle Zero Investment Problem II: Use Another Proxy Investment is just a possible proxy for the anticipated shock. Intermediate inputs can be used as proxies as well (Levinsohn &amp; Petrin, 2003). The problem is that these intermediate inputs are included in the gross production function, whereas investment is excluded. Let \\(m_{jt}\\) be the log material input, and assume that the production function takes the form of: \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\beta_m m_{jt} + \\omega_{jt} + \\eta_{jt}. \\end{equation}\\] In addition, assume that the optimal policy function for \\(m_{jt}\\) is strictly monotonic in the ex-ante shock, and hence is invertible: \\[\\begin{equation} m_{jt} = m(k_{jt}, \\omega_{jt}) \\Leftrightarrow \\omega_{jt} = m^{-1}(m_{jt}, k_{jt}) \\equiv h(m_{jt}, k_{jt}). \\tag{3.1} \\end{equation}\\] First step: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\beta_m m_{jt} + h(m_{jt}, k_{jt}) + \\eta_{jt}\\\\ &amp;= \\beta_l l_{jt} + \\phi(m_{jt}, k_{jt}) + \\eta_{jt}. \\end{split} \\end{equation}\\] We can identify \\(\\beta_l\\) and \\(\\phi\\) by exploiting the moment condition: \\[\\begin{equation} \\begin{split} &amp; \\mathbb{E}\\{\\eta_{jt}|l_{jt}, m_{jt}, k_{jt}, i_{jt}\\} = 0\\\\ &amp; \\Leftrightarrow \\mathbb{E}\\{y_{jt} - \\beta_0 - \\beta_l l_{jt} - \\phi(m_{jt}, k_{jt}) |l_{jt}, m_{jt}, k_{jt}\\} = 0, \\end{split} \\end{equation}\\] if there is enough variation in \\(l_{jt}, m_{jt}, k_{jt}\\). Second step: \\[\\begin{equation} \\begin{split} &amp;y_{jt} - \\beta_l^0 l_{jt}\\\\ &amp; = \\beta_k k_{jt} + \\beta_m m_{jt} + g[\\phi^0(m_{j, t - 1}, k_{j, t - 1}) - \\beta_k k_{j, t - 1} - \\beta_m m_{j, t - 1}]\\\\ &amp; + \\nu_{jt} + \\eta_{jt}. \\end{split} \\end{equation}\\] We can identify \\(\\beta_k\\), \\(\\beta_m\\), and \\(g\\) by exploiting the moment condition: \\[\\begin{equation} \\begin{split} \\mathbb{E}\\{\\nu_{jt} + \\eta_{jt} | k_{jt}, m_{j, t - 1}, k_{j,t - 1}\\} = 0. \\end{split} \\end{equation}\\] Because \\(m_{jt}\\) is correlated with \\(\\nu_{jt}\\), the moment should not condition on \\(m_{jt}\\). The identification of \\(\\beta_{m}\\) comes from \\(\\beta_m m_{j, t - 1}\\). 3.3.29 One-step Estimation of Olley &amp; Pakes (1996) and Levinsohn &amp; Petrin (2003) Levinsohn &amp; Petrin (2003) can be estimated in the similar two-step method. We can jointly estimate the parameters in first and second steps to improve the efficiency (Wooldridge, 2009). We estimate under the assumptions of Olley &amp; Pakes (1996): \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_1 l_{jt} + \\beta_k k_{jt} + \\omega_{jt} + \\eta_{jt}. \\end{equation}\\] The first step exploits the following moment: \\[\\begin{equation} \\mathbb{E}\\{\\eta_{jt}|l_{jt}, k_{jt}, i_{jt}\\} = 0, \\end{equation}\\] that is: \\[\\begin{equation} \\mathbb{E}\\{y_{jt} - \\beta_1 l_{jt} - \\beta_0 - \\beta_k k_{jt} - \\omega(k_{jt}, i_{jt})|l_{jt}, k_{jt}, i_{jt}\\} = 0. \\tag{3.2} \\end{equation}\\] We can reinforce the moment condition as: \\[\\begin{equation} \\mathbb{E}\\{\\eta_{jt}|l_{jt}, k_{jt}, i_{jt}, \\cdots, l_{j1}, k_{j1}, i_{j1}\\} = 0 \\end{equation}\\] if we assume that lagged inputs are correlated with the current inputs and \\(\\eta_{jt}\\) is independent. The second step exploits the following moment: \\[\\begin{equation} \\mathbb{E}\\{\\nu_{jt}|k_{jt}, i_{j, t - 1}, l_{j, t - 1}\\} = 0, \\end{equation}\\] that is: \\[\\begin{equation} \\mathbb{E}\\{y_{jt} - \\beta_0 - \\beta_1 l_{jt} - \\beta_k k_{jt} - g[\\omega(k_{j,t - 1}, i_{j, t - 1})]|k_{jt}, i_{j, t - 1}, l_{j, t - 1}\\} = 0. \\tag{3.3} \\end{equation}\\] We can reinforce the moment condition as: \\[\\begin{equation} \\mathbb{E}\\{\\nu_{jt}|k_{jt}, i_{j, t - 1}, l_{j, t - 1}, \\cdots, k_{j1}, i_{j1}, l_{j1}\\} = 0, \\end{equation}\\] if we assume that lagged input are correlated with the current inputs and \\(\\nu_{jt} + \\eta_{jt}\\) are independent. We can construct a GMM estimator based on equations (3.2) and (3.3). The one-step estimator can be more efficient but can be computationally heavier than the two-step estimator. 3.3.30 Scalar Unobservable Problem: Finite-order Markov Process Borrow the idea of using the first-order condition to resolve the collinearity problem (Gandhi et al., 2017). We have assumed that anticipated shocks follow a first-order Markov process: \\[\\begin{equation} \\omega_{jt} = g(\\omega_{j, t - 1}) + \\nu_{jt}. \\end{equation}\\] However, it may be true that it has more than one lags, for example: \\[\\begin{equation} \\omega_{jt} = g(\\omega_{j, t - 1}, \\omega_{j, t - 2}) + \\nu_{jt}. \\end{equation}\\] Then, we need proxies as many as the number of unobservables: \\[\\begin{equation} \\begin{pmatrix} i_{jt} \\\\ m_{jt} \\end{pmatrix} = \\Gamma(k_{jt}, \\omega_{jt}, \\omega_{j, t - 1}), \\end{equation}\\] such that the policy function for the proxies is a bijection in \\((\\omega_{jt}, \\omega_{j, t - 1})\\). Then, we can have: \\[\\begin{equation} \\omega_{jt} = \\Gamma_1^{-1}(k_{jt}, i_{jt}, m_{jt}). \\end{equation}\\] The reminder goes as in the standard OP method. 3.3.31 Scalar Unobservable Problem: Demand and Productivity Shocks There may be a demand shock \\(\\mu_{jt}\\) that also follows first-order Markov process. Then, the policy function depend both on \\(\\mu_{jt}\\) and \\(\\omega_{jt}\\). We again need proxies as many as the number of unobservable. Suppose that we can observe the price of the firm \\(p_{jt}\\). Inverting the policy function: \\[\\begin{equation} \\begin{pmatrix} i_{jt}\\\\ p_{jt} \\end{pmatrix} = \\Gamma(k_{jt}, \\omega_{jt}, \\mu_{jt}). \\end{equation}\\] yields: \\[\\begin{equation} \\omega_{jt} = \\Gamma_1^{- 1}(k_{jt}, i_{jt}, p_{jt}). \\end{equation}\\] If \\(\\omega_{jt}\\) only depends on \\(\\omega_{j, t - 1}\\) but not on \\(\\mu_{j, t - 1}\\), then the second step of the modified OP method is to estimate: \\[\\begin{equation} \\begin{split} y_{jt} - \\hat{\\beta}_l l_{jt} &amp;= \\beta_0 + \\beta_k k_{jt}\\\\ &amp; + g(\\omega_{j, t - 1}) + \\nu_{jt} + \\eta_{jt}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt}\\\\ &amp; + g(\\hat{\\phi}_{j, t - 1} - \\beta_0 - \\beta_k k_{j, t - 1}) + \\nu_{jt} + \\eta_{jt}. \\end{split} \\end{equation}\\] It goes as in the standard OP method. If \\(\\omega_{jt}\\) depends both on \\(\\omega_{j, t - 1}\\) and \\(\\mu_{j, t - 1}\\), the second step regression equation will be: \\[\\begin{equation} \\begin{split} y_{jt} - \\hat{\\beta}_l l_{jt} &amp;= \\beta_0 + \\beta_k k_{jt}\\\\ &amp; + g(\\omega_{j, t - 1}, \\mu_{j, t - 1}) + \\nu_{jt} + \\eta_{jt}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt}\\\\ &amp; + g(\\hat{\\phi}_{j, t - 1} - \\beta_0 - \\beta_k k_{j, t - 1}, \\mu_{j, t - 1}) + \\nu_{jt} + \\eta_{jt}. \\end{split} \\end{equation}\\] We still have to control \\(\\mu_{j, t - 1}\\) in the second step. Invert the policy function for \\(\\mu_{j, t - 1}\\) to get: \\[\\begin{equation} \\mu_{j, t - 1} = \\Gamma_2^{- 1}(k_{j, t - 1}, i_{j, t - 1}, p_{j, t - 1}), \\end{equation}\\] and plug it into the second step regression equation to get: \\[\\begin{equation} \\begin{split} &amp;y_{jt} - \\hat{\\beta}_l l_{jt}\\\\ &amp;= \\beta_0 + \\beta_k k_{jt}\\\\ &amp;+g(\\hat{\\phi}_{j, t - 1} - \\beta_0 - \\beta_k k_{j, t - 1}, \\Gamma_2^{- 1}(k_{j, t - 1}, i_{j, t - 1}, p_{j, t - 1})) + \\nu_{jt} + \\eta_{jt}. \\end{split} \\end{equation}\\] The parameters \\(\\beta_0\\) and \\(\\beta_k\\) be identified only with this observation, because \\(\\Gamma_2^{-1}\\) is function: it can mean any function of \\((k_{j, t - 1}, i_{j, t - 1}, p_{j, t - 1})\\). To estimate such a model, we jointly estimate the demand function along with the production function. At this point, we do not investigate it further because we have not yet learned how to estimate the demand function. For now just keep in mind that: There has to be as many proxies as the dimension of the unobservable state variables. It is okay that the unobservable state variable includes a demand shock. It can be problematic when the unobservable demand shock affect the evolution of the anticipated productivity shock. 3.3.32 Collinearity Problem The collinearity problem is formally pointed out by Daniel A. Ackerberg et al. (2015). This paper is finally published in 2015, but has been circulated since 2005. We assumed that \\(k_{jt}\\) and \\(\\omega_{jt}\\) are state variables. Then the policy function for labor input should take the form of: \\[\\begin{equation} l_{jt} = l(k_{jt}, \\omega_{jt}). \\end{equation}\\] However, because \\(\\omega_{jt} = h(i_{jt}, k_{jt})\\), we have: \\[\\begin{equation} l_{jt} = l[k_{jt}, h(i_{jt}, k_{jt})] = \\tilde{l}(i_{jt}, k_{jt}). \\end{equation}\\] Therefore, in the first stage, we encounter a multicollinearity problem: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_0 + \\beta_l \\tilde{l}(i_{jt}, k_{jt}) + \\phi(i_{jt}, k_{jt}) + \\eta_{jt}\\\\ &amp;\\equiv \\tilde{\\phi}(i_{jt}, k_{jt}). \\end{split} \\end{equation}\\] Thus, \\(\\beta_l\\) cannot be identified in the first step. The second step becomes: \\[\\begin{equation} y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + g[\\tilde{\\phi}(i_{j, t - 1}, k_{j, t - 1}) - \\beta_0 - \\beta_l l_{j, t - 1} - \\beta_k k_{jt}] + \\nu_{jt} + \\eta_{jt} \\end{equation}\\] Because \\(l_{jt}\\) is correlated with \\(\\nu_{jt}\\), moment can only condition on \\(l_{j, t - 1}\\). However, conditioning on \\(k_{j, t - 1}\\) and \\(i_{j, t - 1}\\), again there is no remaining variation in \\(l_{j, t - 1}\\). Therefore, \\(\\beta_l\\) cannot be identified either in the second step. \\(\\beta_l\\) cannot be identified! 3.3.33 Tackle Collinearity Problem: Peculiar Assumptions To make Olley-Pakes/Levinsohn-Petrin approach workable, we need peculiar data generating process for \\(l_{jt}\\). Consider Levinsohn-Petrin framework. There is an optimization error in \\(l_{jt}\\). If it is not i.i.d over time, it becomes a state variable and enters to the policy for \\(m_{jt}\\), violating the scalar unobserved heterogeneity assumption of \\(m_{jt}\\). If there is an optimization error for \\(m_{jt}\\), this again violates the scalar unobserved heterogeneity assumption. \\(k_{jt}\\) is realized, \\(\\omega_{jt}\\) is observed, \\(m_{jt}\\) and \\(i_{jt}\\) are determined, a new i.i.d. unexpected shock is observed, \\(l_{jt}\\) is determined, and \\(\\eta_{jt}\\) is observed. If it is not i.i.d over time, it becomes a state variable and enters to the policy for \\(m_{jt}\\), violating the scalar unobserved heterogeneity assumption. \\(k_{jt}\\) is realized, an unexpected shock is observed, \\(l_{jt}\\) is determined, \\(\\omega_{jt}\\) is observed, \\(m_{jt}\\) and \\(i_{jt}\\) are determined, and \\(\\eta_{jt}\\) is observed (Daniel A Ackerberg (2016) recommends this assumption). In this case, the unexpected shock can be serially correlated, because it suffices to know \\(k_{jt}\\), \\(i_{jt}\\), \\(l_{jt}\\) to decide \\(m_{jt}\\). It does not have to predict the future unexpected shock based on the realization of the current shock because \\(m_{jt}\\) is a static decision. This changes the optimal policy function of \\(m_{jt}\\) (3.1) to: \\[\\begin{equation} m_{jt} = m(k_{jt}, \\omega_{jt}, l_{jt}). \\end{equation}\\] The first step: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + h(k_{jt}, m_{jt}, l_{jt}) + \\eta_{jt}\\\\ &amp;= \\psi(k_{jt}, m_{jt}, l_{jt}) + \\eta_{jt}.\\\\ \\Rightarrow &amp; \\mathbb{E}\\{y_{jt} - \\psi(k_{jt}, m_{jt}, l_{jt})|k_{jt}, m_{jt}, l_{jt}\\} = 0. \\end{split} \\end{equation}\\] The second step: \\[\\begin{equation} \\begin{split} y_{jt} &amp;= \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + g[\\psi(k_{j, t - 1}, m_{j, t - 1}, l_{j, t - 1}) - \\beta_0 - \\beta_l l_{j, t - 1} - \\beta_k k_{j, t - 1}] + \\nu_{jt} + \\eta_{jt}\\\\ \\Rightarrow &amp; \\mathbb{E}\\{y_{jt} - \\beta_0 - \\beta_l l_{jt} - \\beta_k k_{jt} - g[\\psi(k_{j, t - 1}, m_{j, t - 1}, l_{j, t - 1}) - \\beta_0 - \\beta_l l_{j, t - 1} - \\beta_k k_{j, t - 1}]|k_{j, t - 1}, i_{j, t - 1}, l_{j, t - 1}, m_{j, t - 1}\\} \\end{split} \\end{equation}\\] \\(m_{jt}\\) has to be excluded from the production function, i.e., it has to be a value-added production function. Otherwise, \\(\\beta_m m_{jt}\\) and \\(\\beta_m m_{j, t - 1}\\) appear in the second step. Because \\(m_{jt}\\) is correlated with \\(\\nu_{jt}\\), the only hope is to vary \\(m_{j, t - 1}\\). But there is no additional variation in \\(m_{j, t - 1}\\) conditional on \\(k_{j, t - 1}\\), \\(i_{j, t - 1}\\), and \\(l_{j, t - 1}\\). 3.3.34 Tackle Collinearity Problem: Share Regression How to avoid the peculiar assumptions on shocks and timing of decisions? How to identify gross production function avoiding the third assumption by Daniel A. Ackerberg et al. (2015)? Return to the old literature using the first-order condition. Let \\(w_t\\) be wage and \\(p_t\\) be the product price. Assume that the factor market is competitive. Then, the first-order condition for profit maximization with respect to \\(L_{jt}\\) is: \\[\\begin{equation} \\begin{split} &amp;P_t F_L(L_{jt}, K_{jt})e^{\\omega_{jt}} \\mathbb{E} e^{\\eta_{jt}} = w_t\\\\ &amp;\\Leftrightarrow \\frac{P_t F_L(L_{jt}, K_{jt})e^{\\omega_{jt}} \\mathbb{E} e^{\\eta_{jt}}}{F(L_{jt}, K_{jt}) } = \\frac{w_t}{F(L_{jt}, K_{jt}) }\\\\ &amp;\\Leftrightarrow \\frac{F_L(L_{jt}, K_{jt}) L_{jt}}{F(L_{jt}, K_{jt}) e^{\\eta_{jt}} } = \\frac{w_t L_{jt}}{P_t \\underbrace{F(L_{jt}, K_{jt}) e^{\\omega_{jt}} e^{\\eta_{jt}}}_{Y_{jt}} }, \\end{split} \\end{equation}\\] where the right hand side is expenditure share to the labor, which is observed. Furthermore, on the left hand side, we only have \\(\\eta_{jt}\\), which is independent of inputs. Let \\(s_{jt}\\) be the log of expenditure share to the labor, and take a log of the previous equation gives: \\[\\begin{equation} \\begin{split} s_{jt} &amp;= \\log [F_L(L_{jt}, K_{jt}) L_{jt} \\mathbb{E} e^{\\eta_{jt}} / F(L_{jt}, K_{jt})] - \\eta_{jt}\\\\ &amp; = \\log(\\beta_l) + \\ln \\mathbb{E} e^{\\eta_{jt}} - \\eta_{jt}. \\end{split} \\end{equation}\\] Remember that the coefficient in the Cobb-Douglas function is equal to the expenditure share. In general, share regression provides additional variation to identify the elasticity of anticipated production with respect to the labor. Then we can follow the standard OP method to recover other parameters. 3.4 Cost Function Estimation 3.4.1 Cost Function: Duality Given a function \\(y = F(x)\\) such that: Add factor market structure. Add cost minimization. \\(\\rightarrow\\) There exists a unique cost function \\(c = C(y, p)\\): Positivity: positive for positive input prices and a positive. Homogeneity: homogeneous of degree one in the input prices. Monotonicity: increasing in the input prices and in the level of output. Concavity: concave in the input prices. Given a function \\(c = C(y, p)\\) such that: Positivity: positive for positive input prices and a positive. Homogeneity: homogeneous of degree one in the input prices. Monotonicity: increasing in the input prices and in the level of output. Concavity: concave in the input prices. \\(\\rightarrow\\) There exists a unique production function \\(F(x)\\) that yields \\(C(y, p)\\) as a solution to the cost minimization problem: \\[\\begin{equation} C(y, p) = \\min_{x} p&#39;x \\text{ s.t. } F(x) \\ge y. \\end{equation}\\] If the latter condition holds, the function \\(C\\) is said to be integrable. It is rare that you can find a closed-form cost function of a production function. It makes sense to start from cost function. The duality ensures that there is a one-to-one mapping between a class of cost function and a class of production function. If you accept competitive factor markets and cost minimization, identifying a cost function is equivalent to identifying a production function. We used this idea in the last slides to identify the parameters regarding static decision variables. See Jorgenson (1986) for the literature in this topic up to the mid 80s. 3.4.2 Translog Cost Function One of the popular specifications: \\[\\begin{equation} \\begin{split} \\ln c &amp;= \\alpha_0 + \\alpha_p&#39; \\ln p + \\alpha_y \\ln y + \\frac{1}{2} \\ln p&#39; B_{pp} \\ln p\\\\ &amp; + \\ln p&#39; \\beta_{py} \\ln y + \\frac{1}{2}\\beta_{yy}(\\ln y)^2. \\end{split} \\end{equation}\\] It assumes that the first and second order elasticities are constant. A second-order (log) Taylor approximation of a general cost function. 3.4.3 Translog Cost Function: Integrability Translog cost function is known to be integrable if the following conditions hold: Homogeneity: the cost shares and the cost flexibility are homogeneity of degree zero: \\(B_{pp}1 = 0\\), \\(\\beta_{py}&#39;1 = 0\\). Cost exhaustion: the sum of cost shares is equal to unity: \\(\\alpha_p&#39;1 = 1\\), \\(B_{pp}&#39;1 = 0\\), \\(\\beta_{py}&#39;1 = 0\\). Symmetry: the matrix of share elasticities, biases of scale, and the cost flexibility elasticity is symmetric: \\[\\begin{equation} \\begin{pmatrix} B_{pp} &amp; \\beta_{py}\\\\ \\beta_{py}&#39; &amp; \\beta_{yy} \\end{pmatrix} = \\begin{pmatrix} B_{pp} &amp; \\beta_{py}\\\\ \\beta_{py}&#39; &amp; \\beta_{yy} \\end{pmatrix}&#39;. \\end{equation}\\] Monotonicity: The matrix of share elasticities \\(B_{pp} + vv&#39; - diag(v)\\) is positive semi-definite. 3.4.4 Two Approaches Cost data approach. Use accounting cost data. It does not depend on behavioral assumption. One can impose restrictions of assuming cost minimization. The accounting cost data may not represent economic cost. Revealed preference approach. Assume decision problem for firms. Assume profit maximization. Reveal the costs from firm’s equilibrium strategy. It depends on structural assumptions. It reveals the cost as perceived by firms. 3.4.5 Cost Data Approach Estimating a cost function using cost data from accounting data. McElroy (1987) is one of the most flexible and robust frameworks. The approach is somewhat getting less popular in IO researchers. Recently, the approach is not popular among IO researchers. I one of the reasons for this is that IO researchers believe cost data taken from accounting information does not capture all the costs firms face. However, it is good to know the classical literature because it sometimes gives a new insight. cf. Byrne, Imai, Jain, Sarafidis, &amp; Hirukawa (2015) : Propose a novel method to combine accounting cost data to estimate demand and cost function jointly without using instrumental variable approach. 3.4.6 Revealed Preference Approach Another approach is to reveal the marginal cost from firm’s price/quantity setting behavior assuming it is maximizing profit. A parameter affects economic agent’s action. Therefore, economic agent’s action reveals the information about the parameter. See Timothy F. Bresnahan (1981) and Timothy F. Bresnahan (1989) for reference. We have shown that the assumption on the factor market and cost function minimization gives restriction on the cost parameters. We may further assume the product market structure and profit maximization to identify cost parameters. Example: In a competitive market, the equilibrium price is equal to the marginal cost. Therefore, the marginal cost is identified from prices. What if the competition is imperfect? 3.4.7 Single-product Monopolist This approach requires researcher to specify the decision problem of a firm. Assume that the firm is a single-product monopolist. Let \\(D(p)\\) be the demand function. Let \\(C(q)\\) be the cost function. Temporarily, assume that we know the demand function. We learn how to estimate demand functions in coming weeks. The only unknown parameter is the cost function. The monopolist solves: \\[\\begin{equation} \\max_{p} D(p)p - C(D(p)). \\end{equation}\\] The first-order condition w.r.t. \\(p\\) for profit maximization is: \\[\\begin{equation} \\begin{split} &amp;D(p) + pD&#39;(p) - C&#39;(D(p)) D&#39;(p) = 0.\\\\ &amp;\\Leftrightarrow C&#39;(D(p)) = \\underbrace{\\frac{D(p) + pD&#39;(p)}{D&#39;(p)}}_{\\text{$p$ is observed and $D(p)$ is known.}} \\end{split} \\end{equation}\\] This identifies the marginal cost . To trace out the entire marginal cost function, you need a demand shifter \\(Z\\) that changes the equilibrium: \\(D(p, Z)\\). \\[\\begin{equation} C&#39;(D(p, z)) = \\frac{D(p, z) + pD&#39;(p, z)}{D&#39;(p, z)} \\end{equation}\\] This identifies the marginal cost function . If the equilibrium quantities cover the domain of the marginal cost function when the demand shifter \\(Z\\) moves around, then it identifies the entire marginal cost function. 3.4.8 Unobserved Heterogeneity in the Cost Function Previously we did not consider any unobserved heterogeneity in the cost function. Now suppose that the cost function is given by: \\[\\begin{equation} C(q) = \\tilde{C}(q) + q \\epsilon + \\mu, \\end{equation}\\] and \\(\\epsilon\\) and \\(\\mu\\) are not observed. Moreover, because it includes anticipated shocks, it is likely to be correlated with input decisions and hence the output. The first-order condition w.r.t. \\(p\\) for profit maximization is: \\[\\begin{equation} \\begin{split} &amp;D(p, z) + pD&#39;(p, z) - [\\tilde{C}&#39;(D(p, z)) + \\epsilon]D&#39;(p, z) = 0.\\\\ &amp;\\Leftrightarrow \\tilde{C}&#39;(D(p, z)) = \\frac{D(p, z) + pD&#39;(p, z)}{D&#39;(p,z)} - \\epsilon. \\end{split} \\end{equation}\\] Take the expectation conditional on \\(Z = z\\): \\[\\begin{equation} \\tilde{C}&#39;(D(p, z)) = \\frac{D(p, z) + pD&#39;(p, z)}{D&#39;(p, z)} - \\mathbb{E}\\{\\epsilon|Z = z\\}. \\end{equation}\\] If \\(Z\\) and \\(\\epsilon\\) is independent, then the last term becomes zero and we can follow the same argument as before to trace out the marginal cost function. 3.4.9 Multi-product Monopolist Case Demand for good \\(j\\) is \\(D_j(p)\\) given a price vector \\(p\\). Cost for producing a vector of good \\(q\\) is \\(C(q)\\). Demand function is but cost function is not known. The monopolist solves: \\[\\begin{equation} \\max_{p} \\sum_{j = 1}^J p_j D_j(p) - C(D_1(p), \\cdots, D_J(p)). \\end{equation}\\] The first-order condition w.r.t. \\(p_i\\) for profit maximization is: \\[\\begin{equation} \\begin{split} &amp;D_i(p) + \\sum_{j = 1}^J p_j \\frac{\\partial D_j(p)}{\\partial p_i} = \\sum_{j = 1}^J \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_j} \\frac{\\partial D_j(p)}{\\partial p_i}.\\\\ &amp;= \\begin{pmatrix} \\frac{\\partial D_1(p)}{\\partial p_i} &amp; \\cdots &amp; \\frac{\\partial D_J(p)}{\\partial p_i} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_1}\\\\ \\vdots\\\\ \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_J} \\end{pmatrix} \\end{split} \\end{equation}\\] Summing up, the first-order condition w.r.t. \\(p\\) is summarized as: \\[\\begin{equation} \\begin{split} &amp;\\begin{pmatrix} D_1(p) + \\sum_{j = 1}^J p_j \\frac{\\partial D_j(p)}{\\partial p_1}\\\\ \\vdots\\\\ D_J(p) + \\sum_{j = 1}^J p_j \\frac{\\partial D_j(p)}{\\partial p_J} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial D_1(p)}{\\partial p_1} &amp; \\cdots &amp; \\frac{\\partial D_J(p)}{\\partial p_1}\\\\ \\vdots\\\\ \\frac{\\partial D_1(p)}{\\partial p_J} &amp; \\cdots &amp; \\frac{\\partial D_J(p)}{\\partial p_J} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_1}\\\\ \\vdots\\\\ \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_J} \\end{pmatrix}\\\\ &amp;\\Leftrightarrow \\begin{pmatrix} \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_1}\\\\ \\vdots\\\\ \\frac{\\partial C(D_1(p), \\cdots, D_J(p))}{\\partial q_J} \\end{pmatrix} = \\underbrace{\\begin{pmatrix} \\frac{\\partial D_1(p)}{\\partial p_1} &amp; \\cdots &amp; \\frac{\\partial D_J(p)}{\\partial p_1}\\\\ \\vdots\\\\ \\frac{\\partial D_1(p)}{\\partial p_J} &amp; \\cdots &amp; \\frac{\\partial D_J(p)}{\\partial p_J} \\end{pmatrix}^{-1} \\begin{pmatrix} D_1(p) + \\sum_{j = 1}^J p_j \\frac{\\partial D_j(p)}{\\partial p_1}\\\\ \\vdots\\\\ D_J(p) + \\sum_{j = 1}^J p_j \\frac{\\partial D_j(p)}{\\partial p_J} \\end{pmatrix}.}_{\\text{$p$ is observed and $D(p)$s are known.}} \\end{split} \\end{equation}\\] Hence, the cost function is identified. Including unobserved heterogeneity in the cost function causes the same problem as in the previous case. 3.4.10 Oligopoly There are firm \\(j = 1, \\cdots, J\\) and they sell product \\(j = 1, \\cdots, J\\), that is, firm = product (for simplicity). Consider a price setting game. When the price vector is \\(p\\), demand for product \\(j\\) is given by \\(D_j(p)\\). The cost function for firm \\(j\\) is \\(C_j(q_j)\\). Given other firms’ price \\(p_{-j}\\), firm \\(j\\) solves: \\[\\begin{equation} \\max_{p_j} D_j(p) p_j - C_j(D_j(p)). \\end{equation}\\] The first-order condition w.r.t. \\(p_j\\) for profit maximization is: \\[\\begin{equation} \\begin{split} &amp;D_j(p) + \\frac{\\partial D_j(p)}{\\partial p_j} p_j = \\frac{\\partial C_j(D_j(p))}{\\partial q_j} \\frac{\\partial D_j(p)}{\\partial p_j}.\\\\ &amp;\\frac{\\partial C_j(D_j(p))}{\\partial q_j} = \\underbrace{\\frac{\\partial D_j(p)}{\\partial p_j}^{-1}[D_j(p) + \\frac{\\partial D_j(p)}{\\partial p_j} p_j ]}_{\\text{$p$ is observed and $D_j(p)$ is known}}. \\end{split} \\end{equation}\\] In Nash equilibrium, these equations jointly hold for all firms \\(j = 1, \\cdots, J\\).] Including unobserved heterogeneity in the cost function causes the same problem as in the previous case. References "],
["demand.html", "Chapter 4 Demand Function Estimation 4.1 Motivations 4.2 Analyzing Consumer Behaviors 4.3 Continuous Choice 4.4 Discrete Choice", " Chapter 4 Demand Function Estimation 4.1 Motivations From demand function and utility maximization assumption, we can reveal the preference of the decision maker. Thus, estimating demand function is necessary for evaluating the consumer welfare. In IO, estimating the price elasticity of demand is specifically important, because it determines the market power of a monopolist and the size of the dead-weight loss. In macroeconomics, estimating demand is in important to determine the price level, because the price level is the minimum expenditure for a consumer to achieve the certain level of utility. In marketing, estimating demand is necessary to design the optimal pricing, advertising, and all the other marketing interventions. In principle, the theory can be applied to whatever decisions other than the consumer choice. Nevo (2000): How do the hypothetical mergers in the ready-to-eat cereal industry affect the market price, markup, and consumer surplus? To do so, the authors estimate the demand for ready-to-eat cereals and the cost functions for each product. Then, the authors conduct counterfactual simulations of mergers to quantify the effects. Chung &amp; Alcácer (2002): To what extent do firms go abroad to access technology available in other locations? To study this issue, the authors estimate the firms’ locational choice when going abroad. Rysman (2004): In Yellow pages, how do consumers evaluate the advertisement on it, and how do advertisers value consumer usage? To study this, the author simultaneously estimate the consumer demand for usage of a directory, advertiser demand for advertising, and a publisher’s first-order condition. Gentzow (2004): Are online and print newspapers substitutes or complements? To study this, the author estimate a demand function in which online and print newspapers can be either substitutes or complements. Bayer et al. (2007): How is the preference of people for schools and neighborhoods? How is this capitalized into housing prices? To do so, the authors estimate the discrete choice of residents over locations. To deal with the endogeneity between the neighborhood and the unobserved attributes of the location, the authors use the discontinuity at the school attendance zone. Archak, Ghose, &amp; Ipeirotis (2011): How does the information embedded in product reviews the consumer choice? To study this, the authors estimate the discrete choice model of consumers in which the text information from the product reviews are included as the product attributes. Holmes (2011): Wal-Mart maintain high store density. How large is the economy of density and the sales cannibalization? To study this, the author first estimate the demand function across neighborhood Wal-Mart to capture the sales cannibalization, and then estimate the cost structure from their entry and exit behaviors. Handbury &amp; Weinstein (2014): Urban and rural areas differ in available products. How does the price difference change if the heterogeneity in the product availability is incorporated? To do so, the authors estimate the demand function at each location, and the construct the spatial price index based on the available products at each location. 4.2 Analyzing Consumer Behaviors Alternative set. Utility function. Add system of choice sets. Aad utility maximization. \\(\\rightarrow\\) Demand function. In case of producer behavior, there was a chance to directly observe the output of the most primitive function, the production function. In case of consumer behavior, we never directly observe the output of the most primitive function, the utility function. We can at most identify demand functions. Revealed preference theory: Samuelson (1938), Houthakker (1950), Richter (1966), Afriat (1967), Varian (1982). If the demand function is derived from a preference by maximizing the preference, the demand function should satisfy some restrictions. If the assumption is true, we can recover part of the preference from the demand function. 4.3 Continuous Choice The alternative set \\(\\mathcal{X}\\) is a subset of \\(\\mathbb{R}^J\\). The utility function \\(u\\) is rational, monotone, and continuous on \\(\\mathcal{X}\\). The choice sets are given by a system of linear budget set: \\[ \\mathcal{B}(p, w) = \\{q \\in \\mathcal{X}: p \\cdot q \\le w\\}. \\] If choice sets are non-linear, the following duality approach needs to be modified. 4.3.1 Duality between Utility and Expenditure Functions It is rather a special case that we can derive a closed form solution to a utility maximization problem. We can use the first-order conditions as moment conditions for identification. \\[\\begin{equation} \\frac{\\partial u(q)}{\\partial q_i} = \\lambda p_i, i = 1, \\cdots, J. \\end{equation}\\] The derivation of a demand function from the identified utility function in general require a numerical simulation, which can be bothering. As well as the duality between production and cost functions, we have the same duality theorem for utility and expenditure functions. There is a one-to-one mapping between a class of utility functions and a class of expenditure functions. Therefore, it is okay to start from an expenditure function. It is rare that we can recover the utility function associated with an expenditure function in a closed form. But it is not often required for analysis. Moreover, we can easily derive other important functions from the expenditure functions. Let \\(p\\) be the price vector and \\(u\\) be the target utility level. Let \\(u(q)\\) be a utility function. An expenditure function associated with the utility function is defined by: \\[\\begin{equation} e(u, p) = \\min_{q} p \\cdot q, u(q) \\ge u. \\end{equation}\\] Let \\(x\\) be the total expenditure such that: \\[\\begin{equation} x = e(u, p). \\end{equation}\\] We can start the analysis by specifying this function instead of the utility function. 4.3.2 Deriving Other Functions It is easy to derive other functions from an expenditure function. Indirect utility function: invert the expenditure function to get: \\[\\begin{equation} u = e^{-1}(p, x) \\equiv v(p, x). \\end{equation}\\] Hicksian demand function: apply Shepard’s lemma: \\[\\begin{equation} q_i = \\frac{\\partial e(u, p)}{\\partial p_i} \\equiv h_i(u, p). \\end{equation}\\] Marshallian demand function: insert Hicksian demand function to the expenditure function: \\[\\begin{equation} q_i = h_i(v(p, x), p) \\equiv d_i(p, x). \\end{equation}\\] 4.3.3 Starting from an Indirect Utility Function It is almost equivalent to start from an indirect utility function. An indirect utility function with the utility function is defined by: \\[\\begin{equation} v(p, x) \\equiv \\max_{q} u(q), p&#39;q \\le x. \\end{equation}\\] We can derive Marshallian demand function by Roy’s identity: \\[\\begin{equation} q_i = \\frac{- \\partial v(p, x)/\\partial p_i}{\\partial v(p, x)/\\partial x} \\equiv d_i(p, x). \\end{equation}\\] 4.3.4 Expenditure Share Equation Let’s start from an expenditure function \\(e(p, x)\\). By Shepard’s lemma, we have: \\[\\begin{equation} \\frac{\\partial \\ln e(u, p)}{\\partial \\ln p_i} = \\frac{\\partial e(u, p)}{\\partial p_i} \\frac{p_i}{e(u, p)} = \\frac{p_i q_i}{x} \\equiv w_i. \\end{equation}\\] We call this an expenditure share equation. The estimation is based on the share equations. 4.3.5 Almost Ideal Demand System (AIDS) Based on Deaton &amp; Muellbauer (1980). See Angus Deaton &amp; John Muellbauer (1980) for further reference. Consider an expenditure function that satisfies the following useful conditions: It allows aggregation (this motivation is less important in recent days). It gives an arbitrary first-order approximation to any demand system. It can satisfy the restrictions of utility maximization. It can be used to test the restrictions of utility maximization. 4.3.6 PIGLOG Class PIGLOG (price-independent generalized logarithmic) class (Muellbauer, 1976). \\[\\begin{equation} \\ln e(u, p) = (1 - u) \\ln a(p) + u\\ln b(p), \\end{equation}\\] where \\(a(p)\\) and \\(b(p)\\) are arbitrary linear homogeneous concave functions. Consider households that differ in total income. PIGLOC form ensures that the aggregate demand can be written in the same form where the total income is replaced with the sum of household total income. The derivatives should be given free parameters for the model to be an arbitrary first-order approximation to any demand system. In AIDS, we specify \\(a(p)\\) and \\(b(p)\\) as: \\[\\begin{equation} \\begin{split} \\ln a(p) &amp;\\equiv a_0 + \\sum_{k} \\alpha_k \\ln p_k + \\frac{1}{2}\\sum_{k} \\sum_{j} \\gamma_{kj}^* \\ln p_k \\ln p_j\\\\ \\ln b(p) &amp;\\equiv \\ln a(p) + \\beta_0 \\prod_{k} p_k^{\\beta_k}. \\end{split} \\end{equation}\\] 4.3.7 Derive the Share Equation I By Roy’s identify, we can derive the associated share equation: \\[\\begin{equation} w_i \\equiv \\frac{\\partial \\ln e(u, p)}{\\partial \\ln p_i} = \\alpha_i + \\sum_{j} \\gamma_{ij} \\log p_j + \\beta_i u \\beta_0 \\prod_{k} p_k^{\\beta_k}, \\end{equation}\\] where \\[\\begin{equation} \\gamma_{ij} = \\frac{1}{2}(\\gamma_{ij}^* + \\gamma_{ji}^*). \\end{equation}\\] 4.3.8 Derive the Share Equation II Insert indirect utility function \\(u = v(p, x)\\) to this to get: \\[\\begin{equation} w_i = \\alpha_i + \\sum_{j} \\gamma_{ij} \\ln p_j + \\beta_i \\ln \\frac{x}{P}, \\end{equation}\\] where \\[\\begin{equation} \\ln P \\equiv \\alpha_0 + \\sum_{k} \\alpha_k \\ln p_k + \\frac{1}{2} \\sum_{j} \\sum_{k} \\gamma_{kj} \\ln p_k \\ln p_j. \\end{equation}\\] \\(P\\) is a price index associated with the given preference. With the specification of Richard Stone (1954), it becomes \\(\\ln P = \\sum_{j} x_j \\ln p_j\\). It can be used as an approximation. 4.3.9 Specify the Detail II It can satisfy the restrictions of utility maximization. It can be used to test the restrictions of utility maximization. \\(\\sum_{j} x_j = 1\\): \\[\\begin{equation} \\sum_{j} \\alpha_j = 1, \\sum_{j} \\gamma_{jk} = 0, \\sum_{j} \\beta_j = 0. \\end{equation}\\] \\(e(u, p)\\) is linear homogeneous in \\(p\\): \\[\\begin{equation} \\sum_{j} \\gamma_{ij} = 0. \\end{equation}\\] Symmetry: \\[\\begin{equation} \\gamma_{ij} = \\gamma_{ji}. \\end{equation}\\] 4.3.10 Estimation We can estimate parameters based on the share equations. If we use aggregate data, the aggregate error term is correlated with the price vector. Therefore, we need at least as many instrumental variables as the dimension of the price vector. With valid instrumental variables, we can estimate the model with GMM. If we use household-level data, the household-specific errors controlling for aggregate errors will not be correlated with the price vector if the price is determined in a competitive market. 4.3.11 From Product Space Approach to Characteristics Space Approach The framework up to here is called product space approach because the utility has been defined over a product space. When there are \\(J\\) goods, there are \\(J^2\\) parameters for prices. One way to resolve this issue is to introduce a priori knowledge about the preference. For example, we can introduce a priori segmentation with separability. It is hard to evaluate the effect of introducing new product. Again, we have to a priori decide which segment/product is similar to the new product. This leads us to the characteristics space approach (K. J. Lancaster, 1966; Muth, 1966): Consumption is an activity in which goods are inputs and in which the output is a collection of characteristics. Utility ranks collections of characteristics and only to rank collections of goods indirectly through the characteristics that they possesses. There are \\(k = 1, \\cdots, K\\) activities. The activity \\(y\\) requires to consume \\(x = A y\\) products. The activity \\(y\\) generates \\(z = B y\\) characteristics. The budget constraint is \\(p \\cdot x \\le 1\\). The utility is defined over the characteristics \\(u(z)\\). The consumer’s problem is: \\[ \\max_y u(z) \\] s.t. \\[ p \\cdot x \\le 1, x = Ay, z = By, x, y, z \\ge 0. \\] Then, only the dimension of characteristics matters and the value of new products can be evaluated by the contribution to the production of characteristics. The early application includes S. Rosen (1974), Muellbauer (1974), Gorman (1980). The nonparametric analysis based on the reveals preference is Blow, Browning, &amp; Crawford (2008). 4.3.12 From Continuous Choice Approach to Discrete Choice Approach The aggregate demand is a collection of choice across consumers and within consumers over time. It makes sense to model individual choices and then aggregate rather than directly modeling the aggregate demand. The resulting aggregate demand will satisfy restrictions that are consistent with the underlying consumer choice model. If there is an interaction across choices, the aggregation is not trivial. This is especially true when aggregating choices within consumers. For now, assume that each choice is independent. 4.4 Discrete Choice 4.4.1 Discrete Choice Approach Let \\(u(q, z_i)\\) be the utility of a consumer over \\(J + 1\\) dimensional consumption bundle \\(q\\) characterized by consumer characteristics \\(z_i\\). The consumer solves: \\[\\begin{equation} V(p, y_i, z_i) = \\max_{q}u(q, z_i), \\text{ s.t. } p&#39;q \\le y_i. \\end{equation}\\] Alternative \\(0\\) is an outside good. Normalize \\(p_0 = 1\\). We call alternatives \\(j = 1, \\cdots, J\\) inside goods. The choice space is restricted on: \\[\\begin{equation} \\begin{split} Q = \\{q:&amp; q_0 \\in [0, M], q_j \\in \\{0, 1\\}, j = 1, \\cdots, J,\\\\ &amp; q_j q_k = 0, \\forall j \\neq k, j, k &gt; 0, M &lt; \\infty\\}. \\end{split} \\end{equation}\\] 4.4.2 Discrete Choice Approach The budget constraint reduces to: \\[\\begin{equation} \\begin{cases} q_0 + p_j q_j = y &amp;\\text{ if } q_j = 1, j &gt; 0\\\\ q_0 = y &amp;\\text{ otherwise}. \\end{cases} \\end{equation}\\] Hence, \\[\\begin{equation} q_0 = y - \\sum_{j = 1}^J p_j q_j. \\end{equation}\\] 4.4.3 Discrete Choice Approach The utility maximization problem can be written as: \\[\\begin{equation} V(p, y_i, z_i) = \\max_{j = 0, 1, \\cdots, j} v_j(p_j, y_i, z_i), \\end{equation}\\] where \\[\\begin{equation} \\begin{split} &amp;v_j(p_j, y_i, z_i)\\\\ &amp; = \\begin{cases} u(y_i - p_j, 0, \\cdots, \\underbrace{1}_{q_j}, \\cdots, 0, z_i) &amp;\\text{ if }j &gt; 0,\\\\ u(y_i, 0, \\cdots, 0, z_i) &amp;\\text{ if }j = 0, \\end{cases} \\end{split} \\end{equation}\\] is called the . 4.4.4 Characteristics Space Approach Preference is defined over the characteristics of alternatives, \\(x_j\\): Car: vehicle, engine power, model-year, car maker, etc. PC: CPU power, number of cores, memory, HDD volume, etc. The choice-specific indirect utility is a function of the characteristics of the alternative: \\[\\begin{equation} \\begin{split} v_j(p_j, y_i, z_i) &amp;=u(y_i - p_j, 0, \\cdots, \\underbrace{1}_{q_j}, \\cdots, 0, z_i)\\\\ &amp;= u^*(y_i - p_j, x_j, z_i)\\\\ &amp;\\equiv v(p_j, x_j, y_i, z_i). \\end{split} \\end{equation}\\] 4.4.5 Weak Separability and Income Effect We usually focus on a particular product category such as cars, PCs, cereals, detergents, and so on. Assume that the preference is separable between the category in question (inside goods) and other categories (outside goods). \\(u(q) = u[q_I, v(q_O)]\\): \\(q_I\\): the consumption vector of inside goods. \\(q_O\\): the consumption vector of outside goods. Increasing in \\(v_O = v(q_O)\\). \\(p = (p_I, p_O)\\). \\(p_I\\): the price vector of inside goods. \\(p_O\\): the price vector of outside goods. When \\(y_O\\) is left for the outside goods, the conditional demand for the outside goods \\(q_O(y_O, p_O)\\) exists. Inserting this into the utility function gives: \\[\\begin{equation} u\\{q_I, v[q_O(y_O, p_O)]\\} \\equiv \\tilde{u}(q_I, y_O; p_O). \\end{equation}\\] 4.4.6 Weak Separability and Income Effect Thus, how the preference for the outside good is modeled determines how the individual income affects the choice. \\[\\begin{equation} \\begin{split} &amp;u(y_i - p_j, x_j, z_i) = \\tilde{u}(x_j, z_i) + \\alpha(y_i - p_j).\\\\ &amp;u(y_i - p_j, x_j, z_i) = \\tilde{u}(x_j, z_i) + \\alpha \\ln (y_i - p_j). \\end{split} \\end{equation}\\] In the first example, the income level does not affect the choice because the term \\(\\alpha y_i\\) is common and constant across choices (there is no income effect). We often do not observe income of a consumer, \\(y_i\\). Remember that the price of a product enters because we here consider indirect utility function. 4.4.7 Utility Function Normalization The location of utility function is often normalized by setting: \\[\\begin{equation} u(y^*, 0, \\cdots, 0, z^*) = 0, \\end{equation}\\] for certain choice of \\((y^*, z^*)\\). 4.4.8 Aggregation of the Individual Demand Let \\(q(p, x, y_i, z_i) = \\{q_j(p, x, y_i, z_i)\\}_{j = 0, \\cdots, J}\\) be the demand function of consumer \\(i\\), that is: \\[\\begin{equation} q_j(p, x, y_i, z_i) = 1 \\Leftrightarrow j = \\text{argmax}_{j = 0, 1, \\cdots, j} v(p_j, x_j, y_i, z_i). \\end{equation}\\] Let \\(f(y, z)\\) be the joint distribution of the income and other consumer characteristics. The aggregate demand for good \\(j\\) is: \\[\\begin{equation} \\sigma_j(p, x) \\equiv N \\int q_j(p, x, y, z) f(y, z) dy dz, \\end{equation}\\] where \\(N\\) is the population. 4.4.9 Horizontal Product Differentiation horizontal product differentiation: consumers do not agree on the ranking of the choices. There are two convenience stores \\(j = 1, 2\\) on a street \\([0, 1]\\). Let \\(z_i\\) be the location of consumer \\(i\\) and \\(x_j\\) be the location of the choice on a street \\([0, 1]\\) with \\(x_1 &lt; x_2\\). A consumer has a preference such that: \\[\\begin{equation} v_{ij} \\equiv v(p_j, x_j, y_i, z_i) \\equiv s - t |z_i - x_j| - p_j. \\end{equation}\\] 4.4.10 Horizontal Product Differentiation Suppose that the prices are low enough that entire consumers on the street are willing to buy either from the stores. Consumer \\(i\\) buys from store \\(1\\) if and only if: \\[\\begin{equation} \\begin{split} &amp;v(p_1, x_1, y_i, z_i) \\ge v(p_2, x_2, y_i, z_i)\\\\ &amp;\\Leftrightarrow s - t |z_i - x_1| - p_1 \\ge s - t |z_i - x_2|- p_2\\\\ &amp;\\Leftrightarrow z_i \\le \\frac{p_2 - p_1}{2 t} + \\frac{x_1 + x_2}{2} \\equiv \\overline{z}_1(p_1, p_2). \\end{split} \\end{equation}\\] Let \\(f(z_i)\\) be \\(U[0, 1]\\). Then, the aggregate demand for store 1 is: \\[\\begin{equation} \\begin{split} \\sigma_1(p, x) = N \\int_{0}^{\\overline{z}_1(p_1, p_2)} d z_i = N\\overline{z}_1(p_1, p_2). \\end{split} \\end{equation}\\] 4.4.11 Vertical Product Differentiation Vertical product differentiation: Consumers agree on the ranking of the choices. Consumers can have different willingness to pay. Timothy F Bresnahan (1987) analyzed automobile demand with this framework. There are \\(J\\) goods and consumer \\(i\\) has a utility such as: \\[\\begin{equation} v_{ij} \\equiv v(p_j, x_j, y_i, z_i) = z_i x_j - p_j, \\end{equation}\\] where \\(x_j\\) is a quality of product \\(j\\) and \\(z_i\\) is the consumer’s willingness to pay for the quality with \\(x_j &lt; x_{j + 1}\\). Consumers’ problem is: \\[\\begin{equation} \\max\\{0, z_i x_1 - p_1, \\cdots, z_i x_J - p_J \\}. \\end{equation}\\] 4.4.12 Vertical Product Differentiation Consumer \\(i\\) prefers good \\(j + 1\\) to good \\(j\\) if and only if: \\[\\begin{equation} \\begin{split} &amp;v(p_{j + 1}, wx{j + 1}, y_i, z_i) \\ge v(p_j, x_j, y_i, z_i)\\\\ &amp;\\Leftrightarrow z_i x_{j + 1} - p_{j + 1} \\ge z_i x_j - p_j\\\\ &amp;\\Leftrightarrow z_i \\ge \\frac{p_{j + 1} - p_j}{x_{j + 1} - x_j} \\equiv \\Delta_j. \\end{split} \\end{equation}\\] So consumer \\(i\\) purchases good \\(j\\) if and only if \\(z_i \\in [\\Delta_{j - 1}, \\Delta_j)\\) and buys nothing if: \\[\\begin{equation} z_i \\le \\Delta_0 \\equiv \\min\\{p_1/x_1, \\cdots p_J/x_j\\}. \\end{equation}\\] Letting \\(F(z)\\) be the distribution function of \\(z\\), the aggregate demand for good \\(j\\) is: \\[\\begin{equation} \\sigma_j(p, x, z) = N[F(\\Delta_{j}) - F(\\Delta_{j - 1})]. \\end{equation}\\] 4.4.13 Econometric Models So far there was no econometrics. Next we define what are observable and unobservable, and what are known and unknown. Then consider how to identify and estimate the model. 4.4.14 Multinomial Logit Model: Preference Shock This originates at D. L. Mcfadden (1974). See Train (2009) for reference. Suppose that there is some unobservable component in consumer characteristics. In reality, consumers choice change somewhat randomly. Let’s capture such a preference shock by consider the following model: \\[\\begin{equation} v(p_j, x_j, y_i, z_i) + \\epsilon_{ij}, \\end{equation}\\] with some random vector: \\[\\begin{equation} \\epsilon_i \\equiv (\\epsilon_{i0}, \\cdots, \\epsilon_{iJ})&#39; \\sim G. \\end{equation}\\] At this point, \\(G\\) can be any distribution and the shocks can be dependent across \\(j\\) within \\(i\\). \\(p, x, y_i, z_i\\) are observed but \\(\\epsilon_{ij}\\) are unobserved. When the realization of the preference shock is given, the consumer choice is: \\[ q_j(p, x, y_i, z_i, \\epsilon_{i}) \\equiv 1\\{j = \\text{argmax}_{k = 0, \\cdots, J} v(p_k, x_k, y_i, z_i) + \\epsilon_{ik}\\} \\] for \\(k = 0, \\cdots, J\\). The choice probability as observed by econometrician is: \\[ \\sigma_j(p, x, y_i, z_i) \\equiv \\int q_j(p, x, y_i, z_i, \\epsilon_{i}) dG(\\epsilon_i). \\] 4.4.15 Multinomial Logit Model: Distributional Assumption Now assume the followings: \\(\\epsilon_{ij}\\) are independent across \\(j\\): \\(G(\\epsilon_i) = \\prod_{j = 0, \\cdots, J} G_j(\\epsilon_{ij})\\). \\(\\epsilon_{ij}\\) are identical across \\(j\\): \\(G_j(\\epsilon_{ij}) = \\overline{G}(\\epsilon_{ij})\\). \\(\\overline{G}\\) is a type-I extreme value. \\(\\rightarrow\\) The density \\(g(\\epsilon_{ij}) = \\exp[-\\exp(-\\epsilon_{ij}) - \\epsilon_{ij}]\\). This is called the (homoskedastic) multinomial logit model. Setting the variance of \\(\\epsilon_{ij}\\) at 1 for some \\(j\\) is a scale normalization. By dropping some of the assumptions, we can have heteroskedastic multinomial logit model, generalized extreme value model, and so on. Another popular distribution assumption is to assume a multivariate normal distribution of \\(\\epsilon_i\\). This case is called the multinomial probit model. 4.4.16 Multinomial Logit Model: Choice Probability The of consumer \\(i\\) of good \\(j\\) is: \\[\\begin{equation} \\begin{split} \\sigma_j(p, x, y_i, z_i) &amp; \\equiv \\mathbb{P}\\{j = \\text{argmax}_{k = 0, 1, \\cdots, J} v(p_k, x_k, y_i, z_i) + \\epsilon_{ik} \\}\\\\ &amp;=\\mathbb{P}\\{v(p_j, x_j, y_i, z_i) - v(p_k, x_k, y_i, z_i) \\ge \\epsilon_{ik} - \\epsilon_{ij}, \\forall k \\neq j\\}\\\\ &amp; = \\text{...after some algebra: leave as an exercise...}\\\\ &amp;= \\frac{\\exp[v(p_j, x_j, y_i, z_i) ]}{\\sum_{k = 0}^J \\exp[v(p_k, x_k, y_i, z_i)] }. \\end{split} \\end{equation}\\] For example, if: \\[\\begin{equation} v(p_k, x_k, y_i, z_i) = \\beta_i&#39;x_k + \\alpha_i (y_i - p_k), \\end{equation}\\] \\[\\begin{equation} \\begin{pmatrix} \\beta_i \\\\ \\alpha_i \\end{pmatrix} = \\begin{pmatrix} \\beta_0 \\\\ \\alpha_0 \\end{pmatrix} + \\begin{pmatrix} \\Gamma\\\\ \\pi&#39; \\end{pmatrix} z_i. \\end{equation}\\] Then, we have: \\[\\begin{equation} \\begin{split} \\sigma_{j}(p, x, y_i, z_i) &amp;= \\frac{\\exp[\\beta_i&#39;x_j + \\alpha_i (y_i - p_j) ]}{\\sum_{k = 0}^J \\exp[\\beta_i&#39;x_k + \\alpha_i (y_i - p_k) ]}\\\\ &amp;= \\frac{\\exp[\\beta_i&#39;x_j - \\alpha_i p_j]}{\\sum_{k = 0}^J \\exp[\\beta_i&#39;x_k - \\alpha_i p_k]} \\end{split} \\end{equation}\\] If we normalize the characteristics vector so that \\(w_0 = 0\\) holds for the outside option, it becomes: \\[ \\sigma_{j}(p, x, y_i, z_i) = \\frac{\\exp[\\beta_i&#39;x_j - \\alpha_i p_j]}{1 + \\sum_{k = 1}^J \\exp[\\beta_i&#39;x_k - \\alpha_i p_k]} \\] 4.4.17 Multinomial Logit Model: Inclusive Value The expected utility for consumer \\(i\\) before the preference shocks are drawn under multinomial logit model is given by: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{\\max_{j = 0, \\cdots, J} v(p_j, x_j, y_i, z_i) + \\epsilon_{ij}\\} \\\\ &amp;= \\text{ ...after some algebra: leave as an exercise...}\\\\ &amp;= \\ln \\Bigg\\{\\sum_{j = 0}^J \\exp[v(p_j, x_j, y_i, z_i)] \\Bigg\\} + constant. \\end{split} \\end{equation}\\] This is sometimes called the inclusive value of the choice set. 4.4.18 Maximum Likelihood Estimation of Multinomial Logit Model Suppose we observe a sequence of income \\(y_i\\), consumer characteristics \\(z_i\\), choice \\(q_{i}\\), product characteristics \\(x_j\\) and price \\(p_j\\). \\(q_i = (q_{i0}, \\cdots, q_{iJ})&#39;\\) and \\(q_{ij} = 1\\) if \\(j\\) is chosen and \\(0\\) otherwise. The parameter of interest is the mean indirect utility function \\(v\\). Then the log likelihood of \\(\\{q_i\\}_{i = 1}^N\\) conditional on \\(\\{y_i, z_i\\}_{i = 1}^N\\) and \\(\\{x_j,p_j\\}_{j = 1}^J\\) is: \\[\\begin{equation} \\begin{split} l(v; q, y, z, w) &amp;= \\sum_{i = 1}^N \\ln \\mathbb{P}\\{q_i = q(p, x, y_i, z_i)|p, x, y_i, z_i\\}\\\\ &amp; = \\sum_{i = 1}^N \\log \\Bigg\\{ \\prod_{j = 0}^{J} \\sigma_{j}(p, x, y_i, z_i)^{q_{ij}} \\Bigg\\}\\\\ &amp;= \\sum_{i = 1}^N \\sum_{j = 0}^J \\log \\sigma_{j}(p, x, y_i, z_i)^{q_{ij}}. \\end{split} \\end{equation}\\] We can estimate the parameters by finding parameters that maximize the log likelihood. 4.4.19 Nonlinear Least Square Estimation of Multinomial Logit Model The multinomial logit model can be estimated by nonlinear least square method as well. Suppose that the share of product \\(j\\) among consumers with characteristics \\(z\\) and income \\(y\\) was: \\[\\begin{equation} \\sigma_j(p, x, y, z). \\end{equation}\\] Note that: \\[\\begin{equation} \\begin{split} \\ln \\sigma_{j}(p, x, y, z) &amp;= \\ln \\Bigg\\{ \\frac{\\exp[v(p_j, x_j, y, z) ]}{\\sum_{k = 0}^J \\exp[v(p_k, x_k, y, z)] } \\Bigg\\}\\\\ &amp;= v(p_j, x_j, y, z) - \\ln\\Bigg\\{ \\sum_{k = 0}^J \\exp[v(p_k, x_k, y, z)] \\Bigg\\}. \\end{split} \\end{equation}\\] Moreover, because of the location normalization of the utility function, \\[\\begin{equation} \\sigma_{0}(p, x, y, z) = \\frac{1}{\\sum_{k = 0}^J \\exp[v(p_j, x_k, y, z)] }. \\end{equation}\\] Hence, \\[\\begin{equation} \\ln \\sigma_{j}(p, x, y, z) - \\ln \\sigma_{0}(p, x, y, z) = v(p, x_j, y, z). \\end{equation}\\] The left-hand variables are observed in the data. Let \\(s_j(y, z)\\) be the share of product \\(j\\) among consumers with characteristics \\(z\\) and income \\(y\\) in the data. This can be calculated from the consumer-level data. More importantly, if there is the total sales data for each demographic, we can use this approach. Then, we can estimate the parameter by NLLS such that: \\[\\begin{equation} \\min \\sum_{(y, z)} \\sum_{j = 1}^J \\{\\ln[s_{j}(y, z)/s_{0}(y, z)] - v(p_j, x_j, y, z)\\}^2. \\end{equation}\\] If \\(v\\) is linear in parameter, it is the ordinal least squares: \\[\\begin{equation} v(p_j, x_j, y_m) = \\beta_i&#39; x_j - \\alpha_i p_j. \\end{equation}\\] \\[\\begin{equation} \\ln[s_{j}(y, z)/s_{0}(y, z)] = \\beta_i&#39; x_j - \\alpha_i p_j. \\end{equation}\\] 4.4.20 IIA Problem Multinomial logit problem is intuitive and easy to implement. However, there are several problems in the model. The most important problem is the problem. Notice that: \\[\\begin{equation} \\frac{\\sigma_j(p, x, y, z)}{\\sigma_{k}(p, x, y, z)} = \\frac{\\exp[v(p_j, x_j, y, z)]}{\\exp[v(p_k, x_k, y, z)]}. \\end{equation}\\] The ratio of choice probabilities between two alternatives depend only on the mean indirect utility of these two alternatives and independent of irrelevant alternatives (IIA). Why is this a problem? 4.4.21 Blue Bus and Red Bus Problem Suppose that you can go to a town by bus or by train. Half of commuters use a bus and the other half use a train. The existing bus was blue. Now, the county introduced a red bus, which is identical to the existing blue bus. No one take care of the color of bus. So the mean indirect utility of blue bus and red bus are equal. What is the new share across blue bus, red bus, and train? IIA \\(\\to\\) share of blue bus = share of train. Buses are identical \\(\\to\\) share of blue bus = share of red bus. Therefore, shares have to be 1/3, respectively. But shouldn’t it be that train keeps half share and bus have half share in total? 4.4.22 Restrictive Price Elasticity IIA property restrict price elasticities in an unfavorable manner. This is a serious problem because the main purpose for us to estimate demand functions is to identify the price elasticity. Let \\(v(p_j, x_j, y, z) = \\beta_z&#39;x_j - \\alpha_z p_j\\). Then, we have: \\[\\begin{equation} e_{jk} = \\begin{cases} -\\alpha p_{j} (1 - \\sigma_j(p, x, y, z)) &amp;\\text{ if } k = j\\\\ \\alpha p_{k} \\sigma_k(p_k, x_k, y, z) &amp;\\text{ if } k \\neq j. \\end{cases} \\end{equation}\\] The price elasticity is completely determined by the existing choice probabilities of the relevant alternatives. Suppose that there are coca cola, Pepsi cola, and a coffee. The shares were 1/2, 1/6, 1/3, respectively. Suppose that the price of coca cola increased. We expect that they instead purchase Pepsi cola because Pepsi cola is more similar to coca cola than coffee. However, according to the previous result, twice more consumers substitute to coffee rather than to Pepsi cola. 4.4.23 Monotonic Inclusive Value Suppose that there is a good whose mean indirect utility is \\(v\\). The inclusive value for this choice set is \\(\\ln[1 + \\exp(v)]\\). Suppose that we put \\(J\\) same goods on the shelf and consumer can choose any of them. The inclusive value is \\(\\ln[1 + J \\exp(v)]\\). We just added the same goods. But the expected utility of consumer increases monotonically in the number of alternatives. 4.4.24 The Source of the Problem The source of the problem is that there is no correlation in the preference shock across products. When the preference shock to coca cola is high, the preference shock to Pepsi cola should be high, while the preference shock to coffee should be relatively independent. Because the expected value of the maximum of the preference shocks increases according to the number of alternatives, the inclusive value becomes increasing in the number of alternatives. However, the preference shocks should be the same for the same good. Then, the the expected value of the maximum of the preference shock should not increase even if we add the same products on the shelf. 4.4.25 Correlation in Preference Shocks Therefore, the preference shock should be such that: preference shocks between two alternative should be more correlated when they are closer in the characteristics space. So we have to allow the covariance matrix of the preference shock to be free parameters. If we allow flexible covariance matrix, the curse of dimensionality in the number of alternatives comes back: The dimensionality of the covariance matrix is \\(J^2\\). Another way is to remove \\(\\epsilon_{ij}\\): it is called a pure characteristics model (S. Berry &amp; Pakes, 2007). But the pure characteristics model is computationally not straightforward. We explore the way of introducing mild correlation across similar products in the preference shocks. 4.4.26 Observed and Unobserved Consumer Heterogeneity Consider beverage demand and let \\(x_j = \\text{carbonated}_j\\) and \\(z_i = \\text{teenager}_i\\). Suppose that the mean indirect utility is: \\[\\begin{equation} v(p_j, x_j, y_i, z_i) = \\beta_i (\\text{carbonated})_j - \\alpha_i p_j, \\end{equation}\\] \\[\\begin{equation} \\beta_i = 0.1 + 0.2 \\cdot (\\text{teenager})_i. \\end{equation}\\] The mean utility of a carbonated drink for a teenager is 0.3 but only 0.1 for others. When coca cola was not available, teenager will substitute more to Pepsi cola than non-teenagers. IIA holds at the market-segment level but not at the market level. How to avoid IIA at the market-segment level?: Introduce unobserved consumer heterogeneity. Suppose that the mean indirect utility is: \\[\\begin{equation} \\beta_i = 0.1 + 0.2 \\cdot (\\text{teenager})_i + \\nu_i. \\end{equation}\\] Consumers with high \\(\\nu_i\\) values carbonated drinks more than those with low \\(\\nu_i\\) values. When coca cola was not available, consumers with high \\(\\nu_i\\) will substitute more to Pepsi cola than those with low \\(\\nu_i\\) values. IIA holds at the market-segment-\\(\\nu\\) level but not at the market-segment level. In the above example, “\\(0.2 \\cdot (\\text{carbonated})_i\\)” captures the consumer heterogeneity by observed characteristics and “\\(\\nu_i\\)” by unobserved characteristics. 4.4.27 Mixed Logit Model Suppose that the mean indirect utility is: \\[\\begin{equation} v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i) = \\beta_i&#39; x_j - \\alpha_i p_j, \\end{equation}\\] with \\[\\begin{equation} (\\beta_i, \\alpha_i) \\sim f(\\beta_i, \\alpha_i|y_i, z_i). \\end{equation}\\] If \\(\\epsilon_{ij}\\) is drawn i.i.d. from type-I extreme value distribution, the choice probability of good \\(j\\) by consumer \\(i\\) conditional on \\(p, x, y_i, z_i\\) is: \\[\\begin{equation} \\sigma_{j}(p, x, y_i, z_i) = \\int_{\\beta_i, \\alpha_i} \\frac{\\exp[v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i)]}{\\sum_{k = 0}^J \\exp[v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i)]} f(\\beta_i, \\alpha_i|y_i, z_i) d\\beta_i d\\alpha_i. \\end{equation}\\] This is called the mixed-logit model. If the distribution of \\(\\epsilon_{ij}\\) is different, it is no longer mixed logit. Conditional on \\((\\beta_i, \\alpha_i)\\) the choice probability is written in the same way with the multinomial logit model. \\(\\beta_i, \\alpha_i\\) are marginal out, because econometrician does not observe them. 4.4.28 Mixed Logit Model : Parametric Assumptions It is often assumed that: \\[\\begin{equation} v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i) = \\beta_i&#39; x_j - \\alpha_i p_j. \\end{equation}\\] McFadden &amp; Train (2000) showed that any discrete choice models that are consistent with the random utility maximization can be arbitrarily closely approximated by this class of mixed-logit model. The distribution of \\(\\beta_i\\) and \\(\\alpha_i\\) is often assumed to be: \\[\\begin{equation} \\begin{split} &amp;\\beta_i = \\beta_0 + \\Gamma z_i + \\Sigma \\nu_i,\\\\ &amp;\\alpha_i = \\alpha_0 + \\pi&#39; z_i + \\omega \\upsilon_i, \\end{split} \\end{equation}\\] where \\(\\nu_i\\) and \\(\\upsilon_i\\) are i.i.d. standard normal random vectors. 4.4.29 Mixed Logit Model: IIA There is no IIA at the market-segment level: \\[\\begin{equation} \\frac{\\sigma_{j}(p, x, y, z)}{\\sigma_{l}(p, x, y_i, z_i)} = \\frac{\\int_{\\beta_i, \\alpha_i} \\frac{\\exp[v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i)]}{\\sum_{k = 0}^J \\exp[v(p_k, x_k, y_i, z_i, \\beta_i, \\alpha_i)]} f(\\beta_i, \\alpha_i|y_i) d\\beta_i d\\alpha_i}{\\int_{\\beta_i, \\alpha_i} \\frac{\\exp[v(p_l, x_l, y_i, z_i, \\beta_i, \\alpha_i)]}{\\sum_{k = 0}^J \\exp[v(p_k, x_k, y_i, z_i, \\beta_i, \\alpha_i)]} f(\\beta_i, \\alpha_i|y_i) d\\beta_i d\\alpha_i}. \\end{equation}\\] The share ratio depends on the price and characteristics of all the other products. 4.4.30 Mixed Logit Moel: Price Elasticities Let: \\[\\begin{equation} v(p_j, x_j, y_i, z_i, \\beta_i, \\alpha_i) = \\beta_i&#39; x_j - \\alpha_i p_j. \\end{equation}\\] The price elasticities of the choice probabilities conditional on \\(p, x, y_i, z_i\\) is: \\[\\begin{equation} e_{jk} = \\begin{cases} -\\frac{p_j}{\\sigma_j} \\int \\alpha_i \\sigma_{ij}(1 - \\sigma_{ij})f(\\beta_i, \\alpha_i|y_i, z_i) d\\beta_i d\\alpha_i &amp;\\text{ if } j = k\\\\ \\frac{p_k}{\\sigma_j} \\int \\alpha_i \\sigma_{ij} \\sigma_{ik} f(\\beta_i, \\alpha_i|y_i, z_i) d\\beta_i d\\alpha_i &amp;\\text{ otherwise}, \\end{cases} \\end{equation}\\] where \\[\\begin{equation} \\sigma_{ij} = \\frac{\\exp(\\beta_i&#39;x_j - \\alpha_i p_j)}{\\sum_{k = 0}^J \\exp(\\beta_i&#39;x_k - \\alpha_i p_k)}. \\end{equation}\\] The price elasticity depends on the density of unobserved consumer types. 4.4.31 Simulated Maximum Likelihood Estimation of the Mixed Logit Model The choice probability of the mixed logit model is an integration of the multinomial logit choice probability. This is not derived analytically in general. We can use simulation to evaluate the choice probability: Draw \\(R\\) values of \\(\\beta\\) and \\(\\alpha\\), \\(\\{\\beta^r, \\alpha^r \\}_{r = 1}^R\\). Compute the multinomial choice probabilities associated with \\((\\beta^r, \\alpha^r)\\) for each \\(r = 1, \\cdots, R\\). Approximate the choice probability with the mean of the simulated multinomial choice share: \\[\\begin{equation} \\sigma_{j}(p, x, y_i, z_i) \\approx \\hat{\\sigma}_{j}(p, x, y_i, z_i) \\equiv \\frac{1}{R} \\sum_{r = 1}^R \\frac{\\exp[v(p_j, x_j, y_i, z_i, \\beta^r, \\alpha^r)]}{\\sum_{k = 0}^J \\exp[v(p_k, x_k, y_i, z_i, \\beta^r, \\alpha^r)]}. \\end{equation}\\] This is one of the numerical integration: Monte Carlo integration. Another approach is to use quadrature. See Judd (1998) for reference. 4.4.32 Simulated Maximum Likelihood Estimation of the Mixed Logit Model There are \\(t = 1, \\cdots, T\\) markets and there \\(i = 1, \\cdots, N\\) consumers in each market. Let \\(\\mathcal{J}_t\\) be the set of products that are available in market \\(t\\). Suppose that we observe income \\(y_{it}\\), characteristics \\(z_{it}\\), and choice \\(q_{it}\\) for each consumer in a market. Suppose that we observe product characteristics \\(x_{jt}\\) and price \\(p_{jt}\\) of each product in each market. The simulated conditional log likelihood is: \\[\\begin{equation} \\begin{split} &amp;\\sum_{i = 1}^N \\sum_{t = 1}^T \\ln \\mathbb{P}\\{q_{it} = q(p_t, x_t, y_{it}, z_{it})|p_t, x_t, y_{it}, z_{it}\\} \\\\ &amp;\\approx \\sum_{i = 1}^N \\ln \\Bigg\\{ \\prod_{j \\in \\mathcal{J}_t \\cup \\{0\\}} \\hat{\\sigma}_{j}(p_t, x_t, y_{it}, z_{it})^{q_{itj}} \\Bigg\\}. \\end{split} \\end{equation}\\] We find parameters that maximize the simulated conditional log likelihood. 4.4.33 Simulated Non-linear Least Square Estimation of the Mixed Logit Model Suppose that we only know the sales or share at the market-segment level. That is, we only observe the share of product \\(j\\) in market \\(t\\) among consumers of characteristics \\(z\\) and income \\(y\\), \\(s_{jt}(y, z)\\). Then we can estimate the parameter by: \\[\\begin{equation} \\min \\sum_{t = 1}^T \\sum_{j \\in \\mathcal{J}_t \\cup \\{0\\}} \\sum_{(y, z) \\in \\mathcal{Y} \\times \\mathcal{Z}} \\{s_{jt}(y, z) - \\hat{\\sigma}_{j}(p_t, x_t, y, z)\\}^2. \\end{equation}\\] 4.4.34 Nested Logit Model: A Special Case of Mixed Logit Model Let \\(w_{j1}, \\cdots, w_{jG}\\) be the indicator of product category, i.e., \\(w_{jg}\\) takes value 1 if good \\(j\\) belong to category \\(g\\) and 0 otherwise. e.g., car category = {Sports, Luxury, Large, Midsize, Small}. We have: \\[\\begin{equation} v(p, x_j, y_i, z_i) = \\beta&#39;x_j - \\alpha_i p_j + \\sum_{g = 1}^G \\zeta_{ig} w_{jg} + \\epsilon_{ij}. \\end{equation}\\] If \\(\\zeta_{ig}\\) takes high value, the consumer attaches higher value to the category. When a product in category \\(g\\) was not available, consumers with high \\(\\zeta_{ig}\\) will substitute more to the other products in the same category than consumers with low \\(\\zeta_{ig}\\). 4.4.35 Nested Logit Model: Distributional Assumption Let \\[\\begin{equation} \\varepsilon_{ij} \\equiv \\sum_{g = 1}^G \\zeta_{ig} w_{jg} + \\epsilon_{ij}. \\end{equation}\\] Under certain distributional assumption on \\(\\zeta_{ig}\\) and \\(\\epsilon_{ij}\\), the term \\(\\varepsilon_{ij}\\) have a cumulative distribution (Cardell, 1997): \\[\\begin{equation} F(\\varepsilon_i) = \\exp\\Bigg\\{- \\sum_{g = 1}^G \\Bigg(\\sum_{j \\in \\text{ category } g} \\exp[-\\varepsilon_{ij}/\\lambda_g] \\Bigg)^{\\lambda_g} \\Bigg\\}. \\end{equation}\\] 4.4.36 Nested Logit Model: Choice Probability Under this distributional assumption, the choice probability is: \\[\\begin{equation} \\sigma_{j}(p, x, y_i, z_i) = \\frac{\\exp[v(p, x_j, y_i, z_i)/\\lambda_g] \\Bigg(\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]\\Bigg)^{\\lambda_g - 1}}{\\sum_{g = 1}^G \\Bigg(\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]\\Bigg)^{\\lambda_g}}, \\end{equation}\\] if good \\(j\\) belongs to category \\(g\\). The higher \\(\\lambda_g \\in [0, 1]\\) implies lower correlation within category \\(g\\). \\(\\lambda_g = 1\\) for all \\(g\\) coincides with the multinomial logit model. 4.4.37 Nested Logit Model: Decomposition of the Choice Probability The choice probability can be decomposed into two parts: \\[\\begin{equation} \\sigma_{j}(p, x, y_i, z_i) = \\frac{\\exp[v(p, x_j, y_i, z_i)/\\lambda_g]}{\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]} \\frac{\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]^{\\lambda_g}}{\\sum_{g = 1}^G \\Bigg(\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]\\Bigg)^{\\lambda_g}}. \\end{equation}\\] Letting: \\[ I_{g}(p, x, y_i, z_i) \\equiv \\log \\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g], \\] we have: \\[\\begin{equation} \\sigma_{j}(p, x, y_i, z_i) = \\frac{\\exp[v(p, x_j, y_i, z_i)/\\lambda_g]}{\\sum_{k \\in \\text{ category } g} \\exp[v(p, x_k, y_i, z_i)/\\lambda_g]} \\frac{\\exp[\\lambda_g I_{g}(p, x, y_i, z_i)]}{\\sum_{g = 1}^G \\exp[\\lambda_g I_{g}(p, x, y_i, z_i)]}. \\end{equation}\\] The second first term can be interpreted as the probability of choosing product \\(j\\) conditional on choosing category \\(g\\) and the second term as the probability of choosing category \\(g\\). 4.4.38 Discrete Choice Model with Unobserved Fixed Effects We have assumed that good \\(j\\) is characterized by a vector of observed characteristics \\(x_j\\). Can econometrician observe all the relevant characteristics of the products in the choice set? Maybe no. For example, econometrician may not observe brand values that are created by advertisement and recognized by consumers. Such unobserved product characteristics is likely to be correlated with the price. This can cause endogeneity problems. In the following, we consider the situation where only market-segment level share data is available. Because we can construct the market-share level data from individual choice level data, all the arguments should go through with the individual choice level data. 4.4.39 Unobserved Fixed Effects in Multinomial Logit Model To fix the idea, let’s revisit the multinomial logit model. For now, we do not consider either observed or unobserved consumer heterogeneity. Including observed heterogeneity is straightforward. We discuss how to include unobserved heterogeneity in the subsequent sections. Suppose that the indirect utility function of good \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[\\begin{equation} \\beta&#39; x_{jt} - \\alpha p_{jt} - \\xi_{jt} + \\epsilon_{ik}, \\end{equation}\\] \\(\\epsilon_{ik}\\) is i.i.d. Type-I extreme value. \\(\\xi_{jt}\\) is the unobserved product-market-specific fixed effect of product \\(j\\) in market \\(t\\), which can be correlated with \\(p_{jt}\\). We hold the assumption that \\(x_{jt}\\) is uncorrelated with \\(\\xi_{jt}\\). The choice probability of good \\(j\\) for this consumer and hence the choice share in this market is: \\[\\begin{equation} \\sigma_j(p_t, x_t, \\xi_t) = \\frac{\\exp(\\beta&#39; x_j - \\alpha p_{jt} + \\xi_{jt})}{1 + \\sum_{k = 1}^J\\exp(\\beta&#39; x_k - \\alpha p_{kt} + \\xi_{kt} ) }. \\end{equation}\\] How to deal with the endogeneity between \\(p_{jt}\\) and \\(\\xi_{jt}\\)? 4.4.40 Instrumental Variables and Inversion Suppose that we have a vector of instrumental variables \\(w_{jt}\\) such that: \\[\\begin{equation} \\mathbb{E}\\{\\xi_{jt}|w_{jt}\\} = 0. \\end{equation}\\] In a liner model, we invert the model for the unobserved fixed effects: \\[\\begin{equation} \\xi_{jt} = y_{jt} - \\beta&#39;x_{jt}, \\end{equation}\\] Notice that the unobserved fixed effect is written as a function of parameters and data. Then we exploit the moment condition by: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{\\xi_{jt}|w_{jt}\\} = 0,\\\\ &amp;\\Rightarrow \\mathbb{E}\\{ \\xi_{jt} w_{jt}\\} = 0,\\\\ &amp;\\Leftrightarrow \\mathbb{E}\\{(y_{jt} - \\beta&#39;x_{jt}) w_{jt} \\} = 0 \\end{split} \\end{equation}\\] We can estimate \\(\\beta\\) by finding the value that makes the sample analogue of the above expectation zero. 4.4.41 Inversion in Multinomial Logit Model Can we invert the multinomial model for \\(\\xi_{jt}\\)? We have: \\[\\begin{equation} \\begin{split} &amp;\\ln [\\sigma_{jt}(p_t, x_t, \\xi_t) / \\sigma_{0t}(p_t, x_t, \\xi_t)] = \\beta&#39; x_j - \\alpha p_{jt} + \\xi_{jt}\\\\ &amp;\\Leftrightarrow \\xi_{jt} = \\ln [\\sigma_j(p_t, x_t, \\xi_t) / \\sigma_0(p_t, x_t, \\xi_t)] - [\\beta&#39; x_j - \\alpha p_{jt}]. \\end{split} \\end{equation}\\] Therefore, the moment condition can be written as: \\[\\begin{equation} \\begin{split} &amp;\\mathbb{E}\\{\\xi_{jt}|w_{jt}\\} = 0,\\\\ &amp;\\Rightarrow \\mathbb{E}\\{\\xi_{jt} w_{jt}\\} = 0,\\\\ &amp;\\Leftrightarrow \\mathbb{E}\\{(\\ln [\\sigma_{jt}(p_t, x_t, \\xi_t) / \\sigma_{0t}(p_t, x_t, \\xi_t)] - [\\beta&#39; x_j - \\alpha p_{jt}]) w_{jt} \\} = 0. \\end{split} \\end{equation}\\] We can evaluate the sample analogue of the expectation by replacing the theoretical choice probability \\(\\sigma\\) with the observed share \\(s\\). At the end, it is no different from the linear model where the dependent variable is \\(\\ln s_{jt}/s_{0t}\\). 4.4.42 Market-invariant Product-specific Fixed Effects Furthermore, if you can assume \\(\\xi_{jt} = \\xi_j\\), then \\[\\begin{equation} \\ln [\\sigma_j(p_t, x_t, \\xi_t) / \\sigma_0(p_t, x_t, \\xi_t)] = \\beta&#39; x_{jt} - \\alpha p_{jt} + \\xi_{j}. \\end{equation}\\] This is nothing but a linear regression on \\(x_j\\) and \\(p_{jt}\\) with product-specific unobserved fixed effect. This can be estimated by a within-estimator. This specification is a good starting point: we better start with the simplest specification and use the estimate as the initial guess for the following specifications. 4.4.43 Unobserved Consumer Heterogeneity and Unobserved Fixed Effects in Mixed-logit Model So far we abstracted away from the unobserved consumer heterogeneity. Next, suppose that the indirect utility function of good \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[\\begin{equation} \\beta_i&#39; x_{jt} - \\alpha_i p_{jt} - \\xi_{jt} + \\epsilon_{ik}, \\end{equation}\\] where \\(\\epsilon_{ik}\\) is i.i.d. Type-I extreme value. The coefficient are drawn according to: \\[\\begin{equation} \\begin{split} &amp;\\beta_{it} = \\beta_0 + \\Sigma \\nu_{it},\\\\ &amp;\\alpha_{it} = \\alpha_0 + \\Omega \\upsilon_{it}, \\end{split} \\end{equation}\\] \\(\\nu_i\\) are i.i.d. standard normal random variables. Then the indirect utility of good \\(j\\) for consumer \\(i\\) in market \\(t\\) is written as: \\[\\begin{equation} \\underbrace{\\beta_0&#39; x_{jt} - \\alpha_0 p_{jt} + \\xi_{jt}}_{\\text{(conditional) mean}} + \\underbrace{\\nu_{it}&#39; \\Sigma x_{jt} - \\upsilon_{it}&#39; \\Omega p_{jt}}_{\\text{deviation from the mean}} \\end{equation}\\] We refer to \\(\\beta_0, \\alpha_0\\) as linear parameters and \\(\\Sigma, \\Omega\\) as non-linear parameters, because of the reason I explain in the subsequent section. Let \\(\\theta_1\\) be the linear parameters and \\(\\theta_2\\) the non-linear parameters and let \\(\\theta = (\\theta_1&#39;, \\theta_2&#39;)&#39;\\). 4.4.44 Unobserved Fixed Effects in Mixed-logit Model The choice share of good \\(j\\) in market \\(t\\) is: \\[\\begin{equation} \\begin{split} &amp;\\sigma_{j}(p_t, x_t, \\xi_t; \\theta)\\\\ &amp;= \\int \\frac{\\exp[\\beta_0&#39; x_{jt} - \\alpha_0 p_{jt} + \\xi_{jt} + \\nu_{it}&#39; \\Sigma x_{jt} - \\upsilon_{it}&#39; \\Omega p_{jt}]}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp[\\exp[\\beta_0&#39; x_{kt} - \\alpha_0 p_{kt} + \\xi_{kt} + \\nu_{it}&#39; \\Sigma x_{kt} - \\upsilon_{it}&#39; \\Omega p_{kt}]]} f(\\nu, \\upsilon) d \\nu d \\upsilon. \\end{split} \\end{equation}\\] How can we represent \\(\\xi_{jt}\\) as a function of parameters of interest to exploit the moment condition? 4.4.45 Representing \\(\\xi_{jt}\\) as a Function of Parameters of Interest Let \\(s_{jt}\\) be the share of product \\(j\\) in market \\(t\\). The following system of equations implicitly determines \\(\\xi_{jt}\\) as a function of parameters of interest: \\[\\begin{equation} s_{jt} = \\sigma_j(p_t, x_t, \\xi_t; \\theta). \\end{equation}\\] Let \\(\\xi_{jt}(\\theta)\\) is the solution to the system of equations above given parameter \\(\\theta\\). If it exists, it is the unobserved heterogeneity as a function of parameters and data. Does this solution exist? Is it unique? Is there efficient method to find the solution? 4.4.46 Summarizing the Conditional Mean Term Now, let \\(\\delta_{jt}\\) be the conditional mean term in the indirect utility: \\[\\begin{equation} \\delta_{jt} \\equiv \\beta_0&#39; x_{jt} - \\alpha_0 p_{jt} + \\xi_{jt}. \\end{equation}\\] I call it the average utility of the product in the market. Then, the choice share of product \\(j\\) in market \\(t\\) is written as: \\[\\begin{equation} \\begin{split} &amp;\\sigma_{jt}(\\delta_t, \\theta_2) \\\\ &amp;\\equiv \\int \\frac{\\exp\\Bigg(\\delta_{jt} + \\nu&#39; \\Sigma x_{jt} - \\upsilon&#39; \\Omega p_{jt}\\Bigg)}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp\\Bigg(\\delta_{kt} + \\nu&#39; \\Sigma x_{kt} - \\upsilon&#39; \\Omega p_{kt}\\Bigg)} f(\\nu, \\upsilon) d\\nu d\\upsilon, \\end{split} \\end{equation}\\] for \\(j = 1, \\cdots, J, t = 1, \\cdots, T\\). 4.4.47 Contraction Mapping for \\(\\delta_t\\). Now, fix \\(\\theta_2\\) and define an operator \\(T\\) such that: \\[\\begin{equation} T_t(\\delta_t) = \\delta_t + \\ln \\underbrace{s_{jt}}_{\\text{data}} - \\ln \\underbrace{\\sigma_{jt}(\\delta_t, \\theta_2)}_{\\text{model}}. \\end{equation}\\] Let \\(\\delta_t^{(0)} = (\\delta_{1t}^{(0)}, \\cdots, \\delta_{Jt}^{(0)})&#39;\\) be an arbitrary starting vector of average utility of products in a market. Using the operator above, we update \\(\\delta_{t}^{(r)}\\) by: \\[\\begin{equation} \\delta_{t}^{(r + 1)} = T_t(\\delta_{t}^{(r)}) = \\delta_t^{(r)} + \\ln s_{jt} - \\ln \\sigma_{jt}(\\delta_t^{(r)}, \\theta_2), \\end{equation}\\] for \\(r = 0, 1, \\cdots\\). S. Berry, Levinsohn, &amp; Pakes (1995) proved that \\(T_t\\) as specified above is a contraction mapping with modulus less than one. This means that: \\(T_t\\) has a unique fixed point; For arbitrary \\(\\delta_t^{(r)}\\), \\(\\lim_{r \\to \\infty} T_t^r(\\delta_t^{(0)})\\) is the unique fixed point. The fixed point of \\(T_t\\) is \\(\\delta_t^*\\) such that \\(\\delta_t^* = T_t(\\delta_t^*)\\), i.e., \\[\\begin{equation} \\begin{split} &amp;\\delta_t^* = \\delta_t^* + \\ln s_{jt} - \\ln \\sigma_{jt}(\\delta_t^*, \\theta_2),\\\\ &amp;\\Leftrightarrow s_{jt} = \\sigma_{jt}(\\delta_t^*, \\theta_2). \\end{split} \\end{equation}\\] So, the fixed point \\(\\delta_t^*\\) is the conditional mean indirect utility that solves the equality given non-linear parameter \\(\\theta_2\\). Moreover, the solution is unique. Moreover, it can be found by iterating the operator. Let \\(\\delta_t(\\theta_2)\\) be the solution to this equation, i.e., the limit of this operation. The above result is useful because it ensures the inversion and provides the algorithm to find the solution. The invertibility itself holds under more general settings (S. Berry, Gandhi, &amp; Haile, 2013). 4.4.48 Solving for \\(\\xi_{jt}(\\theta)\\) We defined the average utility as: \\[\\begin{equation} \\delta_{jt} = \\beta_0&#39; x_{jt} - \\alpha_0 p_{jt} + \\xi_{jt}. \\end{equation}\\] Hence, if we set: \\[\\begin{equation} \\xi_{jt}(\\theta) \\equiv \\delta_t(\\theta_2) - \\Bigg[\\sum_{l = 1}^L \\beta_{l} x_{jl} + \\alpha_0 p_{jt} \\Bigg], \\end{equation}\\] the \\(\\xi_{jt}(\\theta)\\) solves the equality: \\[\\begin{equation} s_{jt} = \\sigma_{j}(p_t, x, \\xi_t; \\theta). \\end{equation}\\] 4.4.49 Solving for \\(\\xi_{jt}(\\theta)\\): Summary In summary, \\(\\xi_{jt}\\) that solves the equality exists and unique, and can be computed by: Fix \\(\\theta = \\{\\theta_1, \\theta_2\\}\\). Fix arbitrary starting value \\(\\delta_t^{(0)}\\) for \\(t = 1, \\cdots, T\\). Let \\(\\delta_t(\\theta_2)\\) be the limit of \\(T_t^r(\\delta_t^{(0)})\\) for \\(r = 0, 1, \\cdots\\) for each \\(t = 1, \\cdots, T\\). Stop the iteration if \\(|\\delta_t(\\theta_2)^{(r + 1)} - \\delta_t(\\theta_2)^{(r)}|\\) is below a threshold. Let \\(\\xi_{jt}(\\theta)\\) be such that: \\[\\begin{equation} \\xi_{jt}(\\theta) = \\delta_{jt}(\\theta_2) - \\beta_0&#39; x_{jt} - \\alpha_0 p_{jt}. \\end{equation}\\] Then we can evaluate the moment at \\(\\theta\\) by: \\[\\begin{equation} \\mathbb{E}\\{\\xi_{jt}(\\theta)|w_{jt}\\} = 0. \\end{equation}\\] We run this algorithm every time we evaluate the moment condition at a parameter value. 4.4.50 GMM Objective Function Find \\(\\theta\\) that solves: \\[\\begin{equation} \\min_{\\theta} \\xi(\\theta)&#39; W \\Phi^{-1} W&#39; \\xi(\\theta), \\end{equation}\\] where \\(\\Phi\\) is a weight matrix, \\[\\begin{equation} \\xi(\\theta) = \\begin{pmatrix} \\xi_{11}(\\theta)\\\\ \\vdots\\\\ \\xi_{J_1 1}(\\theta)\\\\ \\vdots\\\\ \\xi_{1T} \\\\ \\vdots\\\\ \\xi_{J_T T} \\end{pmatrix}, W = \\begin{pmatrix} w_{11}&#39; \\\\ \\vdots \\\\ w_{J_11}&#39; \\\\ \\vdots \\\\ w_{1T}&#39; \\\\ \\vdots \\\\ w_{J_TT}&#39; \\\\ \\end{pmatrix}. \\end{equation}\\] There are \\(J \\to \\infty\\) and \\(T \\to \\infty\\) asymptotics. Either is fine to consistently estimate the parameters. \\(w_{jt} = (x_{jt}&#39;, w_{jt}^*)&#39;\\) where \\(w_{jt}^*\\) is an excluded instrument that is relevant to \\(p_{jt}\\). 4.4.51 Estimating Linear Parameters The first-order condition for \\(\\theta_1\\) is: \\[\\begin{equation} \\theta_1 = (X_1&#39;W \\Phi^{-1} W&#39;X_1)^{-1} X_1&#39; W \\Phi^{-1} W&#39; \\delta(\\theta_2), \\end{equation}\\] where \\[\\begin{equation} X_1 = \\begin{pmatrix} x_{11}&#39; &amp; - p_{11}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_1 1}&#39; &amp; - p_{J_1 1}\\\\ \\vdots &amp; \\vdots \\\\ x_{1T}&#39; &amp; - p_{1T}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_T T} &amp; - p_{J_T T} \\end{pmatrix}, \\delta(\\theta_2) = \\begin{pmatrix} \\delta_1(\\theta_2)\\\\ \\vdots\\\\ \\delta_T(\\theta_2) \\end{pmatrix} \\end{equation}\\] . If \\(\\theta_2\\) is given, the optimal \\(\\theta_1\\) is computed by the above formula. \\(\\rightarrow\\) We only have to search over \\(\\theta_2\\). This is the reason why we called \\(\\theta_1\\) linear parameters and \\(\\theta_2\\) non-linear parameters. 4.4.52 BLP Algorithm Find \\(\\theta_2\\) that maximizes the GMM objective function. To do so: Pick up \\(\\theta_2\\). Compute \\(\\delta(\\theta_2)\\) by the fixed-point algorithm. Compute associated \\(\\theta_1\\) by the formula: \\[\\begin{equation} \\theta_1 = (X_1&#39;W \\Phi^{-1} W&#39;X_1)^{-1} X_1&#39; W \\Phi^{-1} W&#39; \\delta(\\theta_2), \\end{equation}\\] Compute \\(\\xi(\\theta)\\) from the above \\(\\delta(\\theta_2)\\) and \\(\\theta_1\\). Evaluate the GMM objective function with the \\(\\xi(\\theta)\\). 4.4.53 Mathematical Program with Equilibrium Constraints (MPEC) In the BLP algorithm, for each parameter \\(\\theta\\), find \\(\\xi(\\theta)\\) that solve: \\[\\begin{equation} s = \\sigma(p, x, \\xi; \\theta) \\end{equation}\\] by the fixed-point algorithm and then evaluate the GMM objective function. This inner loop takes time if the stopping criterion is tight. If the stopping criterion is loose, the loop may stop earlier but the error may be unacceptably large. Dubé, Fox, &amp; Su (2012) suggest to minimize the GMM objective function with the above equation as the constraints. \\[\\begin{equation} \\min_{\\theta} \\xi(\\theta)&#39; W \\Phi^{-1} W&#39; \\xi(\\theta) \\text{ s.t. } s = \\sigma(p, x, \\xi; \\theta). \\end{equation}\\] To enjoy the benefit of this approach, we have to analytically derive the gradient and hessian of the objective function and the constraints, which are anyway needed if we estimate the standard error with the plug-in method. If the problem is of small scale, BLP algorithm will be fast enough and easier to implement. If the problem is of large scale, you may better use the MPEC approach. 4.4.54 Instrumental Variables The remaining problem is how to choose the excluded instrumental variable \\(w_{jt}^*\\) for each product/market. Cost shifters: Traditional instruments. Hausman-type IV (Hausman, Leonard, &amp; Zona, 1994): Assume that demand shocks are independent across markets, whereas the cost shocks are correlated. The latter will be true if the product is produced by the same manufacturer. Then, the price of the same product in the other markets \\(p_{j, -t}\\) will be valid instruments for the price of the product in a given market, \\(p_{jt}\\). BLP-type IV (S. Berry et al., 1995): In oligopoly, the price of a good in a market depends on the market structure, i.e., what kind of products are available in the market. For example, if there are similar products in the market, the price will tend to be lower. Then, the product characteristics of other products in the market , will be valid instrument for the price of goods in a given market, \\(p_{jt}\\). If there are multi-product firms, whether the other good is owned by the same company will also affect the price. Specifically, S. Berry et al. (1995) use: \\[\\begin{equation} \\sum_{k \\neq j \\in \\mathcal{J}_t \\cap \\mathcal{F}_{f}} x_{kt}, \\end{equation}\\] \\[\\begin{equation} \\sum_{k \\neq j \\in \\mathcal{J}_t \\setminus \\mathcal{F}_{f}} x_{kt}. \\end{equation}\\] \\(f\\) is the firm that owns product \\(j\\) and \\(\\mathcal{F}_{f}\\) is the set of products firm \\(f\\) owns. Differentiation IV (Gandhi &amp; Houde, 2015): Let \\(d_{jkt} = d(x_{jt}, x_{kt})\\) be some distance between product characteristics. They showed that under certain conditions the optimal BLP-type IV is a function \\(d_{-jt}\\{d_{jkt}\\}_{k \\neq j \\in \\mathcal{J}_t}\\). The suggest to use the moments of \\(d_{-jt}\\) as the excluded instrument variables. Weak instruments problem of BLP-type IV: Armstrong (2016) argued that estimates based on BLP-type IV may be inconsistent when \\(J \\times \\infty\\) asymptotics is considered, because then the market approaches the competitive market and the correlation between the markup and the product characteristics of the rivals disappear. Specifically, the estimator is inconsistent if all of the following conditions are met: \\(J \\to \\infty\\) but \\(T\\) is fixed; The demand/cost functions are such that the correlation between markups and characteristics of other products decreases quickly enough as \\(J \\to \\infty\\). There is no cost instruments or other sources of identification. References "],
["merger.html", "Chapter 5 Merger Analysis 5.1 Motivations 5.2 Identification of Conduct 5.3 Merger Simulation", " Chapter 5 Merger Analysis 5.1 Motivations Measuring the market power of firms and predicting the possible consequence of horizontal merger cases is one of the primary goal of empirical industrial organization. This is important for antitrust authority to review merger cases. To do so, we integrate the product/cost function estimation and demand function estimation techniques. We introduce the last piece of parameters that characterize the market competition, conduct parameter, and discuss its identification. Then, we conduct the first kind of counterfactual analysis, the merger simularion. In this exercise, we predict the market response when the ownership structure of product is changed due to a hypothetical merger. Every market institution needs its own model for merger simulation: Gowrisankaran, Nevo, &amp; Town (2015): In the U.S. hospitals and managed care organizations (MGO) negotiate the hospital prices and the coinsurance rates. What if hospitals are merged? How much do the hospital prices and the coinsurance rates increase? Smith (2004): Sometimes the same service is sold through multiple stores such as in the supermarket industry. How does this multi-store nature affect the merger effects? Ivaldi &amp; Verboven (2005) reviews the cases in the European Commission. 5.2 Identification of Conduct 5.2.1 Identification of Conduct So far we have been concerned with the two types of parameters: Production and/or cost function. Demand function. To identify the marginal cost by the revealed preference approach, we have assumed that firms are engaging in a price competition. The mode of competition is another parameter of interest. Can we infer the mode of competition instead of assuming it? 5.2.2 Marginal Revenue Function To be specific, consider firms producing homogeneous product. Under what conditions can we distinguish across Bertrand competition, Cournot competition, and collusion? (Timothy F. Bresnahan, 1982). Consider the following marginal revenue function: \\[\\begin{equation} MR(Q) \\equiv \\lambda Q P&#39;(Q) + P(Q), \\end{equation}\\] where \\(Q\\) is the aggregate quantity, \\(P(Q)\\) is the inverse demand function. This formula nests Bertrand, Cournot, and collusion: Bertrand: In Bertrand, a firm cannot change the market price. If a firm increases the production by one unit, whose revenue increases by \\(P(Q)\\). Therefore, \\(\\lambda = 0\\). Cournot: In Cournot, the marginal revenue of firm \\(f\\) is: \\[ q_f P&#39;(Q) + P(Q). \\] Therefore, \\(\\lambda = s_f\\), the quantity share of the firm \\(f\\). Collusion: Under collusion, firms behave like a single monopoly. Then, the marginal revenue is: \\[ QP&#39;(Q) + P(Q). \\] Therefore, \\(\\lambda = 1\\). The identification of the mode of competition in this context is equivalent to the identification of \\(\\lambda\\), the conduct parameter. 5.2.3 First-Order Condition Let \\(MC(q_f)\\) be the marginal cost of firm \\(f\\). Given the previous general marginal revenue function, the first-order condition for profit maximization for firm \\(f\\) is written as: \\[\\begin{equation} \\lambda Q P&#39;(Q) + P(Q) = MC(q_f). \\end{equation}\\] The system of equations for \\(f = 1, \\cdots, F\\) determine the market equilibrium. 5.2.4 Linear Model To be simple, consider a linear inverse demand function: \\[\\begin{equation} P^D(Q) = \\frac{\\alpha_0}{\\alpha_1} + \\frac{1}{\\alpha_1}Q + \\frac{\\alpha_2}{\\alpha_1} X + \\frac{1}{\\alpha_1}u^D, \\end{equation}\\] where \\(X_t\\) is a vector of observed demand sifters. Consider a linear marginal cost function: \\[\\begin{equation} MC(q_{f}) = \\beta_0 + \\beta_1 q_{f} + \\beta_2 W + u^S, \\end{equation}\\] where \\(W\\) is a vector of observed cost sifters. 5.2.5 Pricing Equation Inserting the inverse demand function and marginal cost function to the optimality condition: \\[\\begin{equation} \\frac{\\lambda}{\\alpha_1} Q + P^S(Q) = \\beta_0 + \\beta_1 q_f + \\beta_2 W + u^S \\end{equation}\\] Summing them up and dividing by the number of firms \\(N\\): \\[\\begin{equation} \\frac{\\lambda}{\\alpha_1} Q + P^S(Q) = \\beta_0 + \\frac{\\beta_1}{N} Q + \\beta_2 W_t + u^S, \\end{equation}\\] This determines the aggregate pricing equation: \\[\\begin{equation} \\begin{split} P^S(Q) &amp;= \\beta_0 + (\\frac{\\beta_1}{N} - \\frac{\\lambda}{\\alpha_1})Q + \\beta_2 W + u^S\\\\ &amp;= \\beta_0 + \\gamma Q + \\beta_2 W + u^S. \\end{split} \\end{equation}\\] The key parameter is: \\[\\begin{equation} \\gamma \\equiv \\frac{\\beta_1}{N} - \\frac{\\lambda}{\\alpha_1}. \\end{equation}\\] 5.2.6 Identification of Inverse Demand Function and Pricing Equation We have two systems of reduced-form equations: \\[\\begin{equation} \\begin{split} &amp;P^D(Q) = \\frac{\\alpha_0}{\\alpha_1} + \\frac{1}{\\alpha_1}Q + \\frac{\\alpha_2}{\\alpha_1} X + \\frac{1}{\\alpha_1}u^D,\\\\ &amp;P^S(Q) = \\beta_0 + \\gamma Q + \\beta_2 W + u^S. \\end{split} \\end{equation}\\] If we observe a demand shifter \\(X\\), then it can be used as an instrument for \\(Q\\) in the pricing equation to identify the parameters in the pricing equation. If we observe a cost shifter \\(W_t\\), then it can be used as an instrument for \\(Q\\) in the inverse demand function to identify the parameters in the pricing equation. Thus, we can identify the reduced-form parameters \\((\\alpha_0, \\alpha_1, \\alpha_2)\\) and \\((\\beta_0, \\gamma, \\beta_2)\\) if we observe a demand shifter \\(X\\) and a cost shifter \\(W\\). However, this is not enough to separately identify the structural-form parameters \\(\\beta_1\\) and \\(\\lambda\\) in \\(\\gamma\\). 5.2.7 The Conduct Parameter is Unidentified Even if the demand function and pricing equation (supply function) are identified, we still cannot identify the conduct parameter \\(\\lambda\\). The price at a quantity may be high either because of the high marginal cost or because of the high markup. Remember that the identification of \\(\\gamma\\) and \\(\\alpha_1\\) do not determine the value of \\(\\lambda\\) and \\(\\beta_1\\) in: \\[\\begin{equation} \\gamma = \\frac{\\beta_1}{N} - \\frac{\\lambda}{\\alpha_1}. \\end{equation}\\] \\(\\beta_1\\) is the derivative of the marginal cost. Figure 5.1: Figure 6.2 of Davis (2006) 5.2.8 Identification of the Conduct Parameter: When Cost Data is Available If there is reliable cost data, we can directly identify the marginal cost function: \\[\\begin{equation} MC(q_f) = \\beta_0 + \\beta_1 q_f + \\beta_2 W + u^S, \\end{equation}\\] and so \\(\\beta_1\\). Then, in a combination with the identification of inverse demand function and pricing equation, \\(\\lambda\\) is identified as: \\[\\begin{equation} \\lambda = \\alpha_1 \\left(\\frac{\\beta_1}{N} - \\gamma\\right). \\end{equation}\\] 5.2.9 Identification of the Conduct Parameter: When Cost Data is Not Available Remember the first-order condition: \\[\\begin{equation} \\lambda Q P^{D\\prime}(Q) + P^S(Q) = MC(q_f), \\end{equation}\\] where we can identify \\(P^D(Q)\\) and \\(P^S(Q)\\) if we have demand and cost shifters. It is clear from this expression that we need a variation in \\(P^{D\\prime}(Q)\\) with a fixed \\(Q\\) to identify \\(\\lambda\\), i.e., something that rotates the inverse demand function. Intuition: If demand becomes more elastic, prices will decrease and quantity will increase in a market with a high degree of market power. 5.2.10 Identification of the Conduct Parameter: Demand Rotater is Available Let’s formalize the idea. To identify the conduct parameter, we needed a demand rotater: \\[\\begin{equation} P^D(Q) = \\frac{\\alpha_0}{\\alpha_1} + \\frac{1}{\\alpha_1}Q + \\frac{\\alpha_2}{\\alpha_1} X + \\frac{\\alpha_3}{\\alpha_1} Q \\underbrace{Z}_{\\text{demand rotater}} + \\frac{1}{\\alpha_1}u^D. \\end{equation}\\] Inserting this into the first-order condition yields: \\[\\begin{equation} \\frac{\\lambda}{\\alpha} Q_t (1 + \\alpha Z_t)+ P^S(Q) = \\beta_0 + \\frac{\\beta_1}{N} Q + \\beta_2 W + u^s, \\end{equation}\\] This determines the pricing equation: \\[\\begin{equation} \\begin{split} P^S(Q) &amp;= \\beta_0 - \\frac{\\lambda}{\\alpha_1} Q(1 + \\alpha_3 Z_t) + \\frac{\\beta_1}{N} Q_t + \\beta_2 W + u^S\\\\ &amp;\\equiv \\beta_0 + \\gamma_1 Q + \\gamma_2 Z Q + \\beta_2 W + u^S, \\end{split} \\end{equation}\\] where: \\[\\begin{equation} \\gamma_1 \\equiv \\frac{\\beta_1}{N} - \\frac{\\lambda}{\\alpha_1}, \\gamma_2 \\equiv - \\frac{\\lambda \\alpha_3}{\\alpha_1}. \\end{equation}\\] 5.2.10 Identification of the Conduct Parameters: Demand Rotater is Available If we have cost shifters \\(W\\), it can be used as instruments for \\(Q\\) in the inverse demand function to identify the demand parameters \\((\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3)\\). If we have demand shifters \\(X\\), it can be used as instruments for \\(Q\\) in the pricing equation to identify the supply parameters \\((\\beta_0, \\gamma_1, \\gamma_2, \\beta_2)\\). Now we can identify the reduced-form parameters \\(\\alpha_1, \\alpha_3\\) and \\(\\gamma_2\\). Then, we can not identify the conduct parameter: \\[\\begin{equation} \\lambda = - \\frac{\\gamma_2 \\alpha_1}{\\alpha_3}. \\end{equation}\\] 5.2.11 Identification of Conduct in Differentiated Product Market Consider the identification of conduct when there are two differentiated substitutable products and companies compete in price (Nevo, 1998). Are the prices determined independently or jointly? The general first-order condition is: \\[\\begin{equation} \\begin{split} &amp;(p_1 - c_1) \\frac{\\partial Q_1(p)}{\\partial p_1} + Q_1^S(p) + \\Delta_{12}(p_2 - c_2) \\frac{\\partial Q_2 (p)}{\\partial p_1} = 0\\\\ &amp;\\Delta_{21}(p_1 - c_1)\\frac{\\partial Q_1(p)}{\\partial p_2} + Q_2^S(p) + (p_2 - c_2) \\frac{\\partial Q_2 (p)}{\\partial p_2} = 0, \\end{split} \\end{equation}\\] where \\(\\Delta_{12} = \\Delta_{21} = 0\\) if prices are determined independently. \\(\\Delta_{12} = \\Delta_{21} = 1\\) if price are determined jointly. Under what conditions can we identify \\(\\Delta_{12}\\) and \\(\\Delta_{21}\\)? We will have to rotate \\(\\frac{\\partial Q_1(p)}{\\partial p_2}\\) and \\(\\frac{\\partial Q_2 (p)}{\\partial p_1}\\) while keeping the other variables at the same values. Miller &amp; Weinberg (2017) infers \\(\\Delta\\) after the MillerCoors, a joint venture of SABMiller PLC and Molson Coors Brewing, is formed. The unobserved year-specific and region-specific cost shocks are identified from the outsiders and the unobserved product-specific cost shocks are assumed to be the same before and after the merger. 5.3 Merger Simulation 5.3.1 Unilateral and Coordinated Effects of a Horizontal Merger There are two effects associated with a merge episode: Unilateral effect: The new merged firm usually have a unilateral incentive to raise prices above their pre-merger level. This unilateral effect may lead to the other firms to have an unilateral incentive to raise price, and the reaction continues to reach the new equilibrium. The latter chain reaction is sometimes called the multi-lateral effect. In economic theory, it is the price change when the same mode of competition (say, the Bertrand-Nash equilibrium) is played. \\(\\Delta\\) is either 0 or 1: firms internalize the profits from owned product but do not internalize the other products. Coordinated effect: After the merger, the mode of competition may change. For example, the tacit collusion becomes easier and it can happen. This effect, caused by the change in the mode of competition, is called the coordinated effect of a merger. In this case, the conduct parameters \\(\\Delta\\) may take positive values for products owned by the rival firms. 5.3.2 Merger Simulation In merger simulation, we compute the equilibrium under different ownership structure. This amounts to run a counterfactual simulation hypothetically changing the conduct parameter \\(\\Delta\\). The idea stems back to Farrell &amp; Shapiro (1990), Werden &amp; Frobe (1993), Hausman et al. (1994). Before running the simulation, you have to be very careful about the model assumptions: In music industry, firms compete not in price but in advertisement. If technological diffusion is important, firms may set dynamic pricing. This is the first counterfactual analysis we study in this lecture. The results are valid only if the modeling assumptions are correct. 5.3.3 Quantifying the Unilateral Effect See Nevo (2000) and Nevo (2001). There are \\(J\\) products, \\(\\mathcal{J} = \\{1, \\cdots, J\\}\\). We can have multiple markets but we suppress the market index. Firm \\(f\\) produces a set of products which we denote \\(\\mathcal{J}_f \\subset \\mathcal{J}\\). Let \\(mc_j\\) be the constant marginal cost of producing good \\(j\\). Thus assuming a separable cost function. We can relax this assumption for the estimation. However, once you admit that the costs can be non-separable, you will start to wonder what happens to the cost function if two firms merged and started to produce the two products that were previously produced by separate firms. Let \\(D_j(p)\\) be the demand for product \\(j\\) when the price vector is \\(p\\). The problem for firm \\(f\\) given the price of other firms \\(p_{-f}\\) is: \\[\\begin{equation} \\max_{p_f} \\sum_{j \\in \\mathcal{J}_f} \\Pi_j(p_f, p_{-f}) = \\sum_{j \\in \\mathcal{J}_f} (p_j - mc_j) D_j(p_f, p_{-f}), \\end{equation}\\] where \\(p_f = \\{p_j\\}_{j \\in \\mathcal{J}_f}\\), \\(p_{-f} = \\{p_j\\}_{j \\in \\mathcal{J} \\setminus \\mathcal{J}_f}\\). 5.3.4 Pre-Merger Equilibrium The first-order condition for firm \\(f\\) is: \\[\\begin{equation} D_k(p) + \\sum_{j \\in \\mathcal{J}_f} (p_j - mc_j) \\frac{\\partial D_j(p)}{\\partial p_k} = 0, \\forall k \\in \\mathcal{J}_f. \\end{equation}\\] Let \\(\\Delta_{jk}^{pre}\\) takes 1 if same firm produces \\(j\\) and \\(k\\) and 0 otherwise before the merger. The first-order condition can be written as: \\[\\begin{equation} D_k(p) + \\sum_{j \\in \\mathcal{J}} \\Delta_{jk}^{pre}(p_j - mc_j) \\frac{\\partial D_j(p)}{\\partial p_k} = 0, \\forall k \\in \\mathcal{J}_f. \\end{equation}\\] In terms of the product share: \\[\\begin{equation} s_k(p) + \\sum_{j \\in \\mathcal{J}} \\Delta_{jk}^{pre}(p_j - mc_j) \\frac{\\partial s_j(p)}{\\partial p_k} = 0, \\forall k \\in \\mathcal{J}_f. \\end{equation}\\] Let \\(\\Delta^{pre}\\) be a \\(J \\times J\\) matrix whose \\((j, k)\\)-element is \\(\\Delta_{jk}\\). At the end of the day, performing a merger simulation is to recompute the equilibrium with different ownership structure encoded in \\(\\Delta\\). 5.3.5 Post-Merger Equilibrium Let \\(\\Omega^{pre}(p)\\) is a matrix whose \\((j, k)\\)-element is: \\[\\begin{equation} - \\frac{\\partial s_{j}(p)}{\\partial p_k} \\Delta_{jk}^{pre}. \\end{equation}\\] Then, by the first-order condition, the marginal cost should be: \\[\\begin{equation} mc = p - \\Omega^{pre}(p)^{-1} s(p). \\end{equation}\\] If the ownership structure \\(\\Delta^{pre}\\) is changed to \\(\\Delta^{post}\\), and \\(\\Omega^{pre}\\) changed to \\(\\Omega^{post}\\), the post-merger price is determined by solving the non-linear equation: \\[\\begin{equation} p^{post} = mc + \\Omega^{post}(p^{post})^{-1}s(p^{post}). \\end{equation}\\] The post-merger share is given by: \\[\\begin{equation} s^{post} = s(p^{post}). \\end{equation}\\] 5.3.6 Consumer Surplus Suppose that the demand function is based on the mixed-logit model such that the indirect utility is: \\[\\begin{equation} u_{ijt} = x_{jt} \\beta_i + \\alpha_i p_{jt} + \\xi_{j} + \\xi_t + \\Delta \\xi_{jt} + \\epsilon_{ijt}, \\end{equation}\\] with \\(\\epsilon_{ijt}\\) is drawn from i.i.d. Type-I extreme value distribution and the consumer-level heterogeneity: \\[\\begin{equation} \\begin{pmatrix} \\alpha_i \\\\ \\beta_i \\end{pmatrix} = \\begin{pmatrix} \\alpha\\\\ \\beta \\end{pmatrix} + \\Pi z_i + \\Sigma \\nu_i, \\nu_i \\sim N(0, I_{K + 1}). \\end{equation}\\] Then, the compensated variation due to the price change for consumer \\(i\\) is: \\[\\begin{equation} CV_{it} = \\frac{\\ln (\\sum_{j = 0}^J \\exp(V_{ijt}^{post}) ) - \\ln (\\sum_{j = 0}^J \\exp(V_{ijt}^{pre})) }{\\alpha_i}, \\end{equation}\\] where \\(V_{ijt}^{post}\\) and \\(V_{ijt}^{pre}\\) are indirect utility for consumer \\(i\\) to purchase good \\(j\\) at the prices after and before the merger. This formula holds only if the price enters linearly in the indirect utility (no income effect). For general case, see Small &amp; Rosen (1981) and D. Mcfadden et al. (1995). 5.3.7 Quantifying the Coordinated Effect The repeated-game theory suggests that a collusion is sustainable if and only if it it incentive compatible: the collusion profit is no less than the deviation profit for each member of the collusion. The theory provides a check list that affects the incentive compatibility such as the market share, cost asymmetry, and demand dynamics. But it is often hard to judge the coordinated effects from these qualitative information, because mergers simultaneously change many factors and the factors may encourage or hinder collusion. Miller &amp; Weinberg (2017) retrospectively studies the coordinated effect of a merger. Is prospective analysis of coordinated effects possible as well as the analysis of unilateral effects? If we can identify the demand and cost functions, we can calculate the collusion profits and deviation profits. If we specify the collusion strategy, we can write down the incentive compatibility. We can check how the incentive compatibility change when a hypothetical merger happens. The problem is the identification of conduct: to identify the cost function, we need to know the conduct. Thus, the stated strategy will work only if we have a data during which we are sure that there was no collusion, or there was a particular type of collusion. Igami et al. (2018) use the detailed information of vitamin C cartel case and apply this approach. References "],
["entryexit.html", "Chapter 6 Entry and Exit 6.1 Motivations 6.2 Monopoly Entry 6.3 Complete-Information Homogeneous Oligopoly Entry 6.4 Complete-Information Heterogeneous Oligopoly Entry 6.5 Multiple Prediction 6.6 Incompelete-Information Heterogenous Oligopoly Entry", " Chapter 6 Entry and Exit 6.1 Motivations By studying the entry decisions of firms, we can identify the profit function including the entry cost of firms. A profit function is the reduced-form parameter of the underlying demand function, cost function, and conduct parameters. The profit function can be identified without assuming a particular conduct. The parameter is informative enough to answer questions regarding the market structure and producer surplus. In the last chapter, we discussed the identification of conduct, in which we learned that the exogenous change in the number of firms in a market gives some information about the conduct. In the entry/exit analysis, we study the relationship between the market size, which exogenously changes the equilibrium number of firms, and the change in the market structure to infer the conduct. Entry and exit is not all about firms. The decision of launching a product is a sort of entry decision and the decision of abolishing a product is a sort of exit decision. The framework in this chapter can be applied to a wider class of problems. This chapter is mostly based on Berry &amp; Tamer (2006). 6.1.1 Entry Cost, Mode of Competition, and Market Structure Fixed and sunk entry costs and mode of competition are key determinants for market structure (Sutton, 2007). Figure 6.1: Figure 1 of Sutton (2007) The tougher the mode of competition, the less firms can earn enough profit to compensate the entry cost. Therefore, the tougher the mode of competition, the number of firms in the market in the equilibrium cannot grow when the market size increases. 6.1.2 Exogenous and Endogenous Entry Cost Exogenous fixed and sunk entry cost: The cost of entry is the same across modes of entry. Endogenous fixed and sunk entry cost: The cost differs across modes of entry. For example, firms decide the quality of the product upon entering the market and the the cost of entry is increasing in the quality choice. If endogenous fixed and sunk entry cost is relevant, the entry cost to compete with the incumbent and have a positive profit will increase as the the number of incumbent firms increases. Therefore, the equilibrium firm number will be small and firm size will be large when endogenous fixed and sunk entry cost is relevant. 6.2 Monopoly Entry 6.2.1 Variable Profits and Fixed Costs Consider a cross-section of markets, with one potential entrant in each market. Profits in market \\(i\\) are given by: \\[ \\pi(x_i, F_i) \\equiv v(x_i) - F_i. \\] \\(v(x)\\) is deterministic and \\(F\\) is random. Typically, \\(v(x)\\) is interpreted as the variable profits and \\(F\\) as the fixed costs. The potential entrant will enter market \\(i\\) if and only if: \\[ F_i \\le v(x_i). \\] The parameters of interest are \\(v\\) and the distribution of \\(F\\), \\(\\Phi\\). In general, \\(F\\) in a market can be correlated with \\(x\\). 6.2.2 Non-Identification Assume that \\(F\\) is independent of \\(x\\). Notice that a monotonic transformation of both sides of: \\[ F_i \\le v(x_i). \\] will not change the entry probability. Therefore, without further restrictions, \\(v\\) and \\(\\Psi\\) are at best identified only up to a monotonic transformation. 6.2.3 Restrictions for Identification. Keep assuming that \\(F\\) is independent of \\(x\\). Let \\(p(x)\\) is the observed entry probability at \\(x\\). \\(v\\) is known: Because \\(v\\) is the variable profit function, we can identify it directly from the demand function and cost function. Let \\(z = v(x)\\). Then \\(\\Psi\\) is identified at \\(z\\) by: \\[ \\Psi(z) = \\mathbb{P}\\{F \\le z\\} = \\mathbb{P}\\{F \\le v[v^{-1}(z)]\\} = p[v^{- 1}(z)]. \\] \\(\\Psi\\) is known: Normalize the distribution of \\(F\\). Then, \\(v\\) is identified at \\(x\\) by: \\[ v(x) = \\Psi^{-1}[p(x)]. \\] Impose shape restrictions on \\(v\\) (Matzkin, 1992): \\(v(x)\\) is homogeneous of degree 1 and there exists \\(x_0\\) such that \\(v(x_0) = 1\\). Then the functions \\(v\\) and \\(\\Psi\\) are identified. Homogeneous of degree 1: \\(v(z \\cdot x) = z v(x)\\). Let \\(p(z, x_0)\\) be the probability of entry at \\(z\\) and \\(x_0\\). Then, \\(\\Psi\\) is identified at \\(z\\) by: \\[ \\Psi(z) = \\mathbb{P}\\{F \\le z\\} = \\mathbb{P}\\{F \\le z v(x_0)\\} = p(z, x_0). \\] Then, \\(v\\) is identified by the second argument. 6.2.4 Homogeneous of Degree 1 Variable Profits Sufficient condition for the variable profit function to satisfy the stated condition: Demand is proportional to the population. Marginal cost is constant. Then, the variable profit is the market size \\(z\\) times the per-capita profit. We can normalize \\(v\\) at a market of size \\(z = 1\\) by choosing arbitrary \\(x_0\\). 6.3 Complete-Information Homogeneous Oligopoly Entry 6.3.1 Variable and Fixed Costs Timothy F. Bresnahan &amp; Reiss (1991) pioneered the analysis of oligopoly entry models. We observe a cross-section of markets in which we observe the number of homogeneous firms and other market-specific characteristics. Let \\(y_m\\) be the number of firms in market \\(m\\). Let \\(v_{y_m}(x_m)\\) is the variable profit per firm in a market with number of firms \\(y_m\\) and the market characteristics \\(x_m\\). Let \\(F_m\\) be the market-specific fixed costs that are i.i.d. across markets with unknown distribution \\(\\Psi\\). If there are \\(y_m\\) firms in market \\(m\\), the profit per firm in the market is: \\[ \\pi(y_m, x_m, F_m) \\equiv v_{y_m}(x_m) - F_m. \\] 6.3.2 Equilibrium Condition The unique Nash equilibrium in the number of firms in market \\(m\\) is determined by the following equilibrium condition: \\[ v_{y_m}(x_m) \\ge F_m. \\] \\[ v_{y_m + 1}(x_m) &lt; F_m. \\] Under this equilibrium, the probability of observing \\(y\\) firms in a market of type \\(x\\) is: \\[ \\mathbb{P}\\{y = 0|x\\} = 1 - \\Psi[v_1(x)]. \\] \\[ \\mathbb{P}\\{y = 1|x\\} = \\Psi[v_1(x)] - \\Psi[v_2(x)]. \\] \\[ \\mathbb{P}\\{y = 2|x\\} = \\Psi[v_2(x)] - \\Psi[v_3(x)]. \\] \\[ \\cdots \\] In other words, the probability of observing at least \\(y\\) firms in a market of type \\(x\\) is: \\[ \\mathbb{P}\\{y \\ge 1|x\\} = \\Psi[v_1(x)]. \\] \\[ \\mathbb{P}\\{y \\ge 2|x\\} = \\Psi[v_2(x)]. \\] \\[ \\mathbb{P}\\{y \\ge 3|x\\} = \\Psi[v_3(x)]. \\] \\[ \\cdots \\] 6.3.3 Identification under a Shape Restriction The identification argument for each \\(y\\) is the same as the monopoly entry case. For example, assume \\(v_y(x) = z v_y(\\tilde{x})\\) and \\(v_1(\\tilde{x}_0) = 1\\). Let \\(P_y(z, \\tilde{x})\\) be the observed probability that the number of firms is no less than \\(y\\) in market of type \\(z, \\tilde{x}\\). Then \\(\\Psi\\) is identified at \\(z\\) by: \\[ \\Psi(z) = \\mathbb{P}\\{F \\le z\\} = \\mathbb{P}\\{F \\le z v_1(\\tilde{x}_0)\\} = P_1(z, \\tilde{x}_0). \\] Then, the identification of \\(v_y\\) follows from: \\[ v_y(\\tilde{x}) = \\frac{\\Psi^{-1}[P_y(z, \\tilde{x})]}{z}. \\] 6.3.4 Log Likelihood Function The log likelihood of observing \\(\\{y_m\\}_{m = 1}^M\\) given \\(\\{x_m\\}_{m = 1}^M\\) is: \\[ l(v, \\Psi|\\{y_m\\}_{m = 1}^M, \\{x_m\\}_{m = 1}^M) = \\sum_{m = 1}^M \\log\\{\\Psi[v_{y_m}(x_m)] - \\Psi[v_{y_m + 1}(x_m)]\\}. \\] This is a ordered model in \\(y_m\\). If \\(\\Psi\\) is a normal distribution, it is called an ordered probit model. 6.4 Complete-Information Heterogeneous Oligopoly Entry 6.4.1 Bivartiate Game with Heterogeneous Profits We observe a cross-section of markets in which there are two potential entrants. Let the profit of firm \\(i\\) in market \\(m\\) be: \\[ \\begin{split} \\pi_{im}(x_{im}, y_{jm}, f_{im}) &amp; \\equiv v(y_{jm}, x_{im}) - f_{im}\\\\ &amp;=v_{0i}(x_{im}) + y_{jm} v_{1i}(x_{im}) - f_{im}. \\end{split} \\] \\(y_{im}\\) and \\(y_{jm}\\) are the indicators of entry of firm \\(i\\) and \\(j\\) in market \\(m\\). \\(x_{im}\\) and \\(x_{jm}\\) are firm \\(i\\) and \\(j\\)’s characteristics in market \\(m\\). \\(f_{im}\\) and \\(f_{jm}\\) are the fixed costs of firm \\(i\\) and \\(j\\) in market \\(m\\). The second equation is without loss of generality because \\(y_{jm}\\) is a binary variable. The competitive effect of firm \\(j\\) on \\(i\\) and the effect o firm \\(i\\) on \\(j\\) are asymmetric. The parameters of interest are \\(v_{0i}, v_{0j}, v_{1i}, v_{1j}\\) and the joint distribution of \\(f_{im}, f_{jm}\\) conditional on \\(x_{im}\\) and \\(x_{jm}\\). Firms observe both \\(f_{im}, f_{jm}\\) when they make decisions, but econometrician does not. 6.4.2 Sampling Assumption and Observations We have a random sample of observations on markets where every observation is an observable realization of an equilibrium game played between firm \\(i\\) and \\(j\\). Thus, we can observe: \\(\\mathbb{P}\\{0, 0|x\\}\\): the probability that a market of type \\(x\\) has no firm. \\(\\mathbb{P}\\{1, 0|x\\}\\): the probability that a market of type \\(x\\) has firm \\(i\\) but not firm \\(j\\). \\(\\mathbb{P}\\{0, 1|x\\}\\): the probability that a market of type \\(x\\) has firm \\(j\\) but not firm \\(i\\). \\(\\mathbb{P}\\{1, 1|x\\}\\): the probability that a market of type \\(x\\) has both firms. 6.4.3 Identification Assuming Pure-Strategy Equilibrium Tamer (2003) considers the identification when there are only two potential entrants and the pure-strategy Nash equilibrium is assumed. Assume that the data is from a pure-strategy equilibrium. Then, the probabilities \\(\\mathbb{P}\\{0, 0|x\\}\\) and \\(\\mathbb{P}\\{1, 1|x\\}\\) are written as: \\[ \\mathbb{P}\\{0, 0|x\\} = \\mathbb{P}\\{f_{im} \\ge v_{0i}(x_{im}), f_{jm} \\ge v_{0j}(x_{jm})|x_{im}, x_{jm}\\}. \\] \\[ \\mathbb{P}\\{1, 1|x\\} = \\mathbb{P}\\{f_{im} \\le v_{0i}(x_{im}) + v_{1i}(x_{im}), f_{jm} \\le v_{0j}(x_{jm}) + v_{1j}(x_{jm})|x_{im}, x_{jm}\\}. \\] Assume that \\((f_{im}, f_{jm})\\) are distributed independently of \\((x_{im}, x_{jm})\\) with a joint distribution \\(F\\). Assume that \\(v_{0i}(x_{im}) = z_{im} v_0(\\tilde{x}_{im})\\) and \\(v_{0j}(x_{jm}) = z_{jm} v_0(\\tilde{x}_{jm})\\). Assume that \\(v_{0i}(\\tilde{x}_0) = v_{0j}(\\tilde{x}_0) = 1\\). Assume that \\(v_{1i}\\) and \\(v_{1j}\\) are non-positive. Assume that \\(z_{im}| z_{jm}, \\tilde{x}_{im}, \\tilde{x}_{jm}\\) has a distribution with support on \\(\\mathbb{R}\\) and similar for \\(z_{jm}\\). Then, \\(F\\) is identified by: \\[ \\begin{split} \\mathbb{P}\\{f_{im} \\ge z_{im}, f_{jm} \\ge z_{jm}\\} &amp;= \\mathbb{P}\\{ f_{im} \\ge z_{im} v_0(\\tilde{x}_{0}), f_{jm} \\ge z_{jm} v_0(\\tilde{x}_{0})\\}\\\\ &amp;= \\mathbb{P}\\{0, 0|z_{im}, \\tilde{x}_0, z_{jm}, \\tilde{x}_0\\}. \\end{split} \\] Then, push \\(z_{jm} \\to - \\infty\\) to get: \\[ \\begin{split} \\mathbb{P}\\{f_{im} \\ge z_{im} v_0(\\tilde{x}_{im})\\} &amp;= \\lim_{z_{jm} \\to - \\infty} \\mathbb{P}\\{f_{im} \\ge z_{im} v_0(x_{im}), f_{jm} \\ge z_{jm} v_0(x_{jm})\\}\\\\ &amp;= \\lim_{z_{jm} \\to - \\infty} \\mathbb{P}\\{0, 0|z_{im}, \\tilde{x}_{im}, z_{jm}, \\tilde{x}_{jm}\\} \\end{split} \\] Hence, \\(v_{0}\\) is identified by: \\[ v_0(\\tilde{x}_{im}) = \\frac{F^{-1}[\\lim_{z_{jm} \\to - \\infty} \\mathbb{P}\\{0, 0|z_{im}, \\tilde{x}_{im}, z_{jm}, \\tilde{x}_{jm}\\}]}{z_{im}}. \\] The identification of \\(v_{1i}\\) and \\(v_{1j}\\) are similar. 6.4.4 Heterogeneous Independent Fixed Costs S. T. Berry (1992) considers several extensions to the homogeneous oligopoly models. Assume that the fixed costs are heterogeneous and independent across firms. c.f. In homogeneous oligopoly model, the fixed costs were perfectly correlated across firms in a market. Assume that the characteristics can be firm-specific (that can include market-specific characteristics). Assume that the variable profit function is homogeneous: \\[ \\pi_{y}(x_m, F_{im}) = v_y(x_{im}) - F_{im}. \\] Assume that \\(F\\) is independent of \\(x\\). Suppose that we observe the number of potential entrants in each market. For example, in the airline industry, market is a city pair. The potential entrants into an airline city pair were those with some service out of at least one of the endpoints of the city pair. The variation in the number of potential entrants can be used to identify the model. Define: \\[ \\mu(x) = \\mathbb{P}\\{F_{im} &lt; v_1(x)\\}. \\] \\[ \\delta(x) = \\mathbb{P}\\{F_{im} &lt; v_2(x)\\}. \\] Suppose that we know that there only two potential entrants into a market. Among such markets, we have: \\[ \\mathbb{P}\\{0, 0|x_1, x_2\\} = [1 - \\mu(x_1)] [1 - \\mu(x_2)]. \\] \\[ \\mathbb{P}\\{1, 1|x_1, x_2\\} = \\delta(x_1) \\delta(x_2). \\] If we set \\(x_1 = x_2 = x\\), then we have: \\[ \\mathbb{P}\\{0, 0|x, x\\} = [1 - \\mu(x)]^2. \\] \\[ \\mathbb{P}\\{1, 1|x, x\\} = \\delta(x)^2. \\] They identify \\(\\mu\\) and \\(\\delta\\) at \\(x\\). Under shape restrictions, we can identify \\(v\\) and \\(\\Psi\\) as well. 6.4.5 Heterogenous and Market-Level Correlated Fixed Costs Let \\(\\epsilon_m\\) be the market-specific shock that affects the entry into market \\(m\\). Conditional on \\(\\epsilon_m\\), \\(F_{im}\\) are still independent across firms in a market. Let \\(\\mu\\) be: \\[ \\mu(x, \\epsilon_m) = \\mathbb{P}\\{F_{im} &lt; v_1(x, \\epsilon_m)\\}. \\] When there is a market in which we know that there are only two potential entrants, the probability of observing \\((0, 0)\\) becomes: \\[ \\mathbb{P}\\{0, 0|x_1, x_2\\} = \\int [1 - \\mu(x_1, \\epsilon_m)] [1 - \\mu(x_2, \\epsilon_m)] d \\Gamma(\\epsilon_m). \\] Specifically, assume that: \\[ \\epsilon_m = \\begin{cases} 0 &amp; \\text{ with probability } \\lambda \\\\ 1 &amp; \\text{ with probability } 1 - \\lambda. \\end{cases} \\] Then, we in a market in which we know that there are only two potential entrants, the probability of observing \\((0, 0)\\) becomes: \\[ \\mathbb{P}\\{0, 0|x_1, x_2\\} = \\lambda [1 - \\mu(x_1, 0)] [1 - \\mu(x_2, 0)] + (1 - \\lambda) [1 - \\mu(x_1, 1)] [1 - \\mu(x_2, 1)]. \\] In general, in a market in which we know that there are only \\(K\\) potential entrants such that \\(x_1 = \\cdots x_K = x\\), the probability of observing \\((0, 0)\\) becomes: \\[ \\mathbb{P}\\{0, 0|x, \\cdots x\\} = \\lambda [1 - \\mu(x, 0)]^K + (1 - \\lambda) [1 - \\mu(x, 1)]^K. \\] If there markets with \\(K = 1, \\cdots, \\overline{K}\\), we can construct \\(\\overline{K}\\) equations with three unknowns \\(\\lambda\\), \\(\\mu(x, 0)\\), and \\(\\mu(x, 1)\\). Thus, the knowledge and the variation in the number of potential entrants help the identification. 6.4.6 Inference Based On a Unique Prediction S. T. Berry (1992) develops a more general model built on the ideas above. The key for his analysis is that he ensures that there is a unique number of equilibrium entrants. This enables him to calculate the likelihood of observing a sequence of number of entrants, but at the cost of the generality of the underlying model. 6.4.7 Entry in the Airline Industry: One-shot Game Based on S. T. Berry (1992). A market = a city pair market at a single point in time. Consider a one-shot entry game that yields a network structure. At the beginning of the period, each firm takes its overall network structure as given and decides whether to operate in a given city pair independently across markets. 6.4.8 Entry in the Airline Industry: Profit Function There are \\(K_m\\) potential entrants in market \\(m\\). Let \\(y_m\\) be a strategy profile. \\(y_m = (y_{1m}, \\cdots, y_{K_m m})&#39;\\), \\(y_{im} \\in \\{0, 1\\}\\). The profit function for firm \\(i\\) in market \\(m\\): \\[\\begin{equation} \\pi_{im}(y_m, f_{im}) = v_m\\left(N_{m}\\right) - f_{im}. \\end{equation}\\] \\(N_{m} = \\sum_{i = 1}^{K_m} y_{im}\\). \\(v_m\\) is strictly decreasing in \\(N_m\\). 6.4.9 Entry in the Airline Industry: Profit Function The common term is assumed to be: \\[ v_m(N) = x_m&#39; \\beta + h(\\delta, N_m) + \\rho u_{m}, \\] \\(x_m\\) is the observed market characteristics, \\(h(\\cdot)\\) is a function that is decreasing in \\(N_m\\), say, \\(- \\delta \\ln (N_m)\\). \\(u_m\\) is the market characteristics that is observed by firms but not by econometrician. The firm-specific term: \\[ f_{im} = z_{im}&#39; \\alpha + \\sigma u_{im}, \\] \\(z_{im}\\) is the observed firm characteristics. A scale normalization: \\(\\sigma = \\sqrt{1 - \\rho^2}\\) \\(\\Rightarrow var(\\rho u_m + \\sigma u_{im}) = 1\\). 6.4.10 Entry in the Airline Industry: Likelihood Function The observed part: \\[ r_{im}(N) = x_m&#39; \\beta - \\delta \\ln (N_m) + z_{im}&#39; \\alpha. \\] The unobserved part: \\[ \\epsilon_{im} = \\sqrt{1 - \\rho^2} u_{im} + \\rho u_{m}. \\] 6.4.11 Is the Equilibrium Number of Firms Unique? Either of the following conditions are sufficient: No firm-level unobserved heterogeneity: \\(\\rho = 1\\). No market-level unobserved heterogeneity: \\(\\rho = 0\\). The order of entry is predetermined, for example, the most profitable firms enter first. The incumbent firms enter first. Under either of the above assumptions, simulate the equilibrium number of firms in each market and match with the data. 6.5 Multiple Prediction If we further generalize the model, we will suffer from the problem of multiple prediction. First, even in S. T. Berry (1992)’s framework, the identity of entrants were not uniquely predicted. Second, if we allows for the asymmetric competitive effects, we will not have the unique number of equilibrium entrants. Third, the uniqueness of the equilibria may not hold once we allow for the mixed-strategy Nash equilibria. If we do not have the unique prediction on the endogenous variables, we cannot write down the likelihood function. 6.5.1 Multiple Equilibria in Bivariate Game with Heterogenous Profits Return to Tamer (2003)’s bivariate game with heterogeneous profits. Consider the pure-strategy Nash equilibrium when \\(f_{im}, f_{jm}\\) are realized. \\((0, 0)\\) is a pure-strategy Nash equilibrium if: \\[ f_{im} \\ge v_{0i}(x_{im}); \\] \\[ f_{jm} \\ge v_{0j}(x_{jm}). \\] \\((1, 1)\\) is a pure-strategy Nash equilibrium if: \\[ f_{im} \\le v_{0i}(x_{im}) + v_{1i}(x_{im}); \\] \\[ f_{jm} \\le v_{0j}(x_{jm}) + v_{1j}(x_{jm}). \\] \\((0, 1)\\) is a pure-strategy Nash equilibrium if: \\[ f_{im} \\ge v_{0i}(x_{im}) + v_{1i}(x_{im}); \\] \\[ f_{jm} \\le v_{0j}(x_{jm}). \\] \\((1, 0)\\) is a pure-strategy Nash equilibrium if: \\[ f_{im} \\le v_{0i}(x_{im}); \\] \\[ f_{jm} \\ge v_{0j}(x_{jm}) + v_{1j}(x_{jm}). \\] In a certain region of \\(f_{im}, f_{jm}\\), there are multiple equilibria. 6.5.2 Multiple Prediction in Bivariate Game with Heterogenous Profits In the orange region, we have multiple pure-strategy equilibria. If we allow for mixed-strategy equilibria, the region of \\(f_{im}, f_{jm}\\) on which there are multiple equilibria will increase. In this example, we can write the likelihood of the number of equilibrium entrants: \\(2\\): green area. \\(1\\): yellow, orange, and red areas. \\(0\\): blue area. However, we cannot write the likelihood of the strategy profile. \\((1, 1)\\): green area. \\((0, 1)\\): \\(\\ge\\) red area, \\(\\le\\) orange and red area. \\((1, 0)\\): \\(\\ge\\) yellow area, \\(\\le\\) orange and yellow area. \\((0, 0)\\): blue area. 6.5.3 Inference Based on Moment Inequalities According to the theory, we have: \\[ \\mathbb{P}\\{0, 0|x\\} = \\mathbb{P}\\{f_{i} \\ge v_{0i}(x_{i}), f_{j} \\ge v_{0j}(x_{j})\\} \\equiv H(0, 0|x). \\] \\[ \\mathbb{P}\\{1, 1|x\\} = \\mathbb{P}\\{f_{i} \\le v_{0i}(x_{i}) + v_{1i}(x_{i}), f_{j} \\le v_{0j}(x_{j}) + v_{1j}(x_{j}) \\} \\equiv H(1, 1|x). \\] \\[ \\mathbb{P}\\{0, 1|x\\} \\ge \\mathbb{P}\\{f_{i} \\ge v_{0i}, f_{j} \\le v_{0j}\\} + \\mathbb{P}\\{f_{i} \\ge v_{0i} + v_{1i}, f_{j} \\le v_{0j} + v_{1j}\\} \\equiv \\underline{H}(0, 1|x). \\] \\[ \\mathbb{P}\\{0, 1|x\\} \\le \\underline{H}(0, 1|x) + \\mathbb{P}\\{v_{0i} + v_{1i} \\le f_{i} \\le v_{0i}, v_{0j} + v_{1j} \\le f_{j} \\le f_{0j}\\} \\equiv \\overline{H}(0, 1|x). \\] \\[ \\mathbb{P}\\{1, 0|x\\} \\ge \\mathbb{P}\\{f_{j} \\ge v_{0j}, f_{i} \\le v_{0i}\\} + \\mathbb{P}\\{f_{j} \\ge v_{0j} + v_{1j}, f_{i} \\le v_{0i} + v_{1i}\\} \\equiv \\underline{H}(1, 0|x). \\] \\[ \\mathbb{P}\\{1, 0|x\\} \\le \\underline{H}(0, 1|x) + \\mathbb{P}\\{v_{0} + v_{1i} \\le f_{i} \\le v_{0i}, v_{0j} + v_{1j} \\le f_{j} \\le f_{0j}\\} \\equiv \\overline{H}(1, 0|x). \\] The parameters should satisfy the moment conditions: \\[ \\mathbb{P}\\{0, 0|x\\} - H(0, 0|x) = 0; \\] \\[ \\mathbb{P}\\{1, 1|x\\} - H(1, 1|x) = 0; \\] \\[ \\min\\left\\{\\mathbb{P}\\{0, 1|x\\} - \\underline{H}(0, 1|x), 0\\right\\} = 0; \\] \\[ \\max\\left\\{\\mathbb{P}\\{0, 1|x\\} - \\overline{H}(0, 1|x), 0\\right\\} = 0; \\] \\[ \\min\\left\\{\\mathbb{P}\\{1, 0|x\\} - \\underline{H}(1, 0|x), 0\\right\\} = 0; \\] \\[ \\max\\left\\{\\mathbb{P}\\{1, 0|x\\} - \\overline{H}(1, 0|x), 0\\right\\} = 0; \\] We can estimate the parameters with the GMM method using the above modified moment conditions. Inference based on the moment inequalities are found in D. W. K. Andrews &amp; Soares (2010). Ciliberto &amp; Tamer (2009) study the entry exit of airlines when there are heterogeneous competitive effects using the above approach. 6.6 Incompelete-Information Heterogenous Oligopoly Entry 6.6.1 Bivariate Case There are two potential entrants. Let the profit of firm \\(i\\) in market \\(m\\) be: \\[ \\begin{split} \\pi_{im}(x_{im}, y_{jm}, f_{im}) &amp; \\equiv v(y_{jm}, x_{im}) - f_{im}\\\\ &amp;=v_{0i}(x_{im}) + y_{jm} v_{1i}(x_{im}) - f_{im}. \\end{split} \\] \\(y_{im}\\) and \\(y_{jm}\\) are the indicators of entry of firm \\(i\\) and \\(j\\) in market \\(m\\). \\(x_{im}\\) and \\(x_{jm}\\) are firm \\(i\\) and \\(j\\)’s characteristics in market \\(m\\). \\(f_{im}\\) and \\(f_{jm}\\) are the fixed costs of firm \\(i\\) and \\(j\\) in market \\(m\\). The second equation is without loss of generality because \\(y_{jm}\\) is a binary variable. The competitive effect of firm \\(j\\) on \\(i\\) and the effect o firm \\(i\\) on \\(j\\) are asymmetric. The parameters of interest are \\(v_{0i}, v_{0j}, v_{1i}, v_{1j}\\) and the joint distribution of \\(f_{im}, f_{jm}\\) conditional on \\(x_{im}\\) and \\(x_{jm}\\). Firm \\(i\\) observe both \\(f_{im}\\) but not \\(f_{jm}\\) when it makes decision. Firm \\(j\\) observe both \\(f_{jm}\\) but not \\(f_{im}\\) when it makes decision. Econometrican does not observe either of them. The joint distribution of \\(f_{im}, f_{jm}\\) is \\(F\\) (independent of \\(x\\)). 6.6.2 The Equilibrium Strategy and Belief The equilibrium strategy becomes a step function that decreases in a threshold: \\[ y_{im} = 1\\{f_{im} \\le t_{im}\\}. \\] \\[ y_{jm} = 1\\{f_{jm} \\le t_{jm}\\}. \\] Suppose that firm \\(i\\) believes that \\(f_{jm}\\) has a distribution of \\(G_{jm}^{im}\\) and firm \\(j\\) believes that \\(f_{im}\\) has a distribution of \\(G_{im}^{jm}\\). Usually, we assume a common and objective prior: \\(G_{jm}^{im}(\\epsilon_{jm}) = F(\\epsilon_{jm}|\\epsilon_{im})\\) and \\(G_{im}^{jm}(\\epsilon_{im}) = F(\\epsilon_{im}|\\epsilon_{jm})\\). When firm \\(j\\) follows strategy \\(t_{jm}\\), the expected payoff for firm \\(i\\) to enter is: \\[ [v_{0i}(x_{im}) - f_{im}][1 - G_{jm}^{im}(t_{jm})] + [v_{0i}(x_{im}) + v_{1i}(x_{im} - f_{im})] G_{jm}^{im}(t_{jm}). \\] Thus, the threshold \\(t_{im}\\) is determined by: \\[ [v_{0i}(x_{im}) - t_{im}][1 - G_{jm}^{im}(t_{jm})] + [v_{0i}(x_{im}) + v_{1i}(x_{im} - t_{im})] G_{jm}^{im}(t_{jm}) = 0. \\] In the same way, the threshold \\(t_{jm}\\) is determined by: \\[ [v_{0j}(x_{jm}) - t_{jm}][1 - G_{im}^{jm}(t_{im})] + [v_{0j}(x_{jm}) + v_{1j}(x_{jm} - t_{jm})] G_{im}^{jm}(t_{im}) = 0. \\] This system of equations can have multiple solution. 6.6.3 Equilibrium Selection and Likelihood If we specify the equilibrium selection rule \\(\\mathbb{P}(t_{im}, t_{jm}|x_{im}, x_{jm}, f_{im}, f_{jm})\\), then we can specify the likelihood of observing \\((0, 0)\\), \\((0, 1)\\), \\((1, 0)\\), and \\((1, 1)\\). If we do not, we will only have bounds on the likelihood of observing \\((0, 0)\\), \\((0, 1)\\), \\((1, 0)\\), and \\((1, 1)\\). Another way is to assume that the same equilibrium \\(t_{im}^*(x_{im}, x_{jm}), t_{jm}^*(x_{im}, x_{jm})\\) holds across markets. Then, the across market relative frequency of entries conditional on \\(x_{im}, x_{jm}\\) gives the estimates of the entry probabilities: \\[ \\widehat{G}_{im}(x_{im}, x_{jm}) \\approx G_{im}^{jm}[t^*_{im}(x_{im}, x_{jm})], \\] \\[ \\widehat{G}_{jm}(x_{im}, x_{jm}) \\approx G_{jm}^{im}[t_{jm}^*(x_{im}, x_{jm})], \\] for each \\(x_{im}, x_{jm}\\). The estimated distribution has to solve: \\[ \\widehat{G}_{im}(x_{im}, x_{jm}) = \\mathbb{P}\\{[v_{0i}(x_{im}) - f_{im}][1 - \\widehat{G}_{jm}(x_{im}, x_{jm})] + [v_{0i}(x_{im}) + v_{1i}(x_{im}) - f_{im}] \\widehat{G}_{jm}(x_{im}, x_{jm}) &gt; 0\\}. \\] \\[ \\widehat{G}_{jm}(x_{im}, x_{jm}) = \\mathbb{P}\\{[v_{0j}(x_{jm}) - f_{jm}][1 - \\widehat{G}_{im}(x_{im}, x_{jm})] + [v_{0j}(x_{jm}) + v_{1j}(x_{jm}) - f_{jm}] \\widehat{G}_{im}(x_{im}, x_{jm}) &gt; 0\\}. \\] We find parameters \\(v_{0i}, v_{0j}, v_{1i}, v_{1j}, F\\) that solve this system of equations. The “same equilibrium” assumption is hardly justified, but is often used in the empirical studies. References "],
["assignment1.html", "Chapter 7 Assignment 1: Basic Programming in R 7.1 Simulate data 7.2 Estimate the parameter", " Chapter 7 Assignment 1: Basic Programming in R The deadline is February 18 1:30pm. Report the following results in html format using R markdown. In other words, replicate this document. You write functions in a separate R file and put in R folder in the project folder. Build the project as a package and load it from the R markdown file. The execution code sholuld be written in R markdown file. You submit: R file containing functions. R markdown file containing your answers and executing codes. HTML report generated from the R markdown. 7.1 Simulate data Consider to simulate data from the following model and estimate the parameters from the simulated data. \\[ y_{ij} = 1\\{j = \\text{argmax}_{k = 1, 2} \\beta x_k + \\epsilon_{ik} \\}, \\] where \\(\\epsilon_{ik}\\) follows i.i.d. type-I extreme value distribution, \\(\\beta = 0.2\\), \\(x_1 = 0\\) and \\(x_2 = 1\\). To simulate data, first make a data frame as follows: ## # A tibble: 2,000 x 3 ## i k x ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 1 2 1 ## 3 2 1 0 ## 4 2 2 1 ## 5 3 1 0 ## 6 3 2 1 ## 7 4 1 0 ## 8 4 2 1 ## 9 5 1 0 ## 10 5 2 1 ## # … with 1,990 more rows Second, draw type-I extreme value random variables. Set the seed at 1. You can use evd package to draw the variables. You should get exactly the same realization if the seed is correctly set. ## # A tibble: 2,000 x 4 ## i k x e ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 ## 2 1 2 1 -0.167 ## 3 2 1 0 1.93 ## 4 2 2 1 1.97 ## 5 3 1 0 0.830 ## 6 3 2 1 -1.06 ## 7 4 1 0 -0.207 ## 8 4 2 1 0.617 ## 9 5 1 0 0.0444 ## 10 5 2 1 1.92 ## # … with 1,990 more rows Third, compute the latent value of each option to obtain the following data frame: ## # A tibble: 2,000 x 5 ## i k x e latent ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 0.281 ## 2 1 2 1 -0.167 0.0331 ## 3 2 1 0 1.93 1.93 ## 4 2 2 1 1.97 2.17 ## 5 3 1 0 0.830 0.830 ## 6 3 2 1 -1.06 -0.863 ## 7 4 1 0 -0.207 -0.207 ## 8 4 2 1 0.617 0.817 ## 9 5 1 0 0.0444 0.0444 ## 10 5 2 1 1.92 2.12 ## # … with 1,990 more rows Finally, compute \\(y\\) by comparing the latent values of \\(k = 1, 2\\) for each \\(i\\) to obtain the following result: ## # A tibble: 2,000 x 6 ## i k x e latent y ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0.281 0.281 1 ## 2 1 2 1 -0.167 0.0331 0 ## 3 2 1 0 1.93 1.93 0 ## 4 2 2 1 1.97 2.17 1 ## 5 3 1 0 0.830 0.830 1 ## 6 3 2 1 -1.06 -0.863 0 ## 7 4 1 0 -0.207 -0.207 0 ## 8 4 2 1 0.617 0.817 1 ## 9 5 1 0 0.0444 0.0444 0 ## 10 5 2 1 1.92 2.12 1 ## # … with 1,990 more rows 7.2 Estimate the parameter Now you generated simulated data. Suppose you observe \\(x_k\\) and \\(y_{ik}\\) for each \\(i\\) and \\(k\\) and estimate \\(\\beta\\) by a maximum likelihood estimator. The likelihood for \\(i\\) to choose \\(k\\) (\\(y_{ik} = 1\\)) can be shown to be: \\[ p_{ik}(\\beta) = \\frac{\\exp(\\beta x_k)}{\\exp(\\beta x_1) + \\exp(\\beta x_2)}. \\] Then, the likelihood of observing \\(\\{y_{ik}\\}_{i, k}\\) is: \\[ L(\\beta) = \\prod_{i = 1}^{1000} p_{i1}(\\beta)^{y_{i1}} [1 - p_{i1}(\\beta)]^{1 - y_{i1}}, \\] and the log likelihood is: \\[ l(\\beta) = \\sum_{i = 1}^{1000}\\{y_{i1}\\log p_{i1}(\\beta) + (1 - y_{i1})\\log [1 - p_{i1}(\\beta)\\}. \\] Write a function to compute the livelihood for a given \\(\\beta\\) and data and name the function loglikelihood_A1. Compute the value of log likelihood for \\(\\beta = 0, 0.1, \\cdots, 1\\) and plot the result using ggplot2 packages. You can use latex2exp package to use LaTeX math symbol in the label: Find and report \\(\\beta\\) that maximizes the log likelihood for the simulated data. You can use optim function to achieve this. You will use Brent method and set the lower bound at -1 and upper bound at 1 for the parameter search. ## $par ## [1] 0.2371046 ## ## $value ## [1] -0.6861689 ## ## $counts ## function gradient ## NA NA ## ## $convergence ## [1] 0 ## ## $message ## NULL "],
["assignment2.html", "Chapter 8 Assignment 2: Production Function Estimation 8.1 Simulate data 8.2 Estimate the parameters", " Chapter 8 Assignment 2: Production Function Estimation The deadline is February 28 1:30pm. 8.1 Simulate data Consider the following production and investment process for \\(j = 1, \\cdots, 1000\\) firms across \\(t = 1, \\cdots, 10\\) periods. The log production function is of the form: \\[ y_{jt} = \\beta_0 + \\beta_l l_{jt} + \\beta_k k_{jt} + \\omega_{jt} + \\eta_{jt}, \\] where \\(\\omega_{jt}\\) is an anticipated shock and \\(\\eta_{jt}\\) is an ex post shock. The anticipated shocks evolve as: \\[ \\omega_{jt} = \\alpha \\omega_{j, t - 1} + \\nu_{jt}, \\] where \\(\\nu_{jt}\\) is an i.i.d. normal random variable with mean 0 and standard deviation \\(\\sigma_\\nu\\). The ex post shock is an i.i.d. normal random variable with mean 0 and standard deviation \\(\\sigma_{\\eta}\\). The product price the same across firms and normalized at 1. The price is normalized at 1. The wage \\(w_t\\) is constant at 0.5. Finally, the capital accumulate according to: \\[ K_{j, t + 1} = (1 - \\delta) K_{jt} + I_{jt}. \\] We set the parameters as follows: parameter variable value \\(\\beta_0\\) beta_0 1 \\(\\beta_l\\) beta_l 0.2 \\(\\beta_k\\) beta_k 0.7 \\(\\alpha\\) alpha 0.7 \\(\\sigma_{\\eta}\\) sigma_eta 0.2 \\(\\sigma_{\\nu}\\) sigma_nu 0.5 \\(\\sigma_{w}\\) sigma_w 0.1 \\(\\delta\\) delta 0.05 Define the parameter variables as above. Write a function that returns the log output given \\(l_{jt}\\), \\(k_{jt}\\), \\(\\omega_{jt}\\), and \\(\\eta_{jt}\\) under the given parameter values according to the above production function and name it log_production(l, k, omega, eta, beta_0, beta_l, beta_k). Suppose that the labor is determined after \\(\\omega_{jt}\\) is observed, but before \\(\\eta_{jt}\\) is observed, given the log capital level \\(k_{jt}\\). Derive the optimal log labor as a function of \\(\\omega_{jt}\\), \\(\\eta_{jt}\\), \\(k_{jt}\\), and wage. Write a function to return the optimal log labor given the variables and parameters and name it log_labor_choice(k, wage, omega, beta_0, beta_l, beta_k, sigma_eta). As discussed in the class, if there is no additional variation in labor, the coefficient on the labor \\(\\beta_l\\) is not identified. Thus, if we generate labor choice from the previous function, \\(\\beta_l\\) will not be identified from the simulated data. To see this, we write a modified version of the previous function in which \\(\\omega_{jt}\\) is replaced with \\(\\omega_{jt} + \\iota_{jt}\\), where \\(\\iota_{jt}\\) is an optimization error that follows an i.i.d. normal distribution with mean 0 and standard deviation 0.05. That is, the manager of the firm perceives as if the shock is \\(\\omega_{jt} + \\iota_{jt}\\), even though the true shock is \\(\\omega_{jt}\\). Modify the previous function by including \\(\\iota_{jt}\\) as an additional input and name it log_labor_choice_error(k, wage, omega, beta_0, beta_l, beta_k, iota, sigma_eta). Consider an investment process such that: \\[ I_{jt} = (\\delta + \\gamma \\omega_{jt}) K_{jt}, \\] where \\(I_{jt}\\) and \\(K_{jt}\\) are investment and capital in level. Set \\(\\gamma = 0.1\\), i.e., the investment is strictly increasing in \\(\\omega_{jt}\\). The investment function should be derived by solving the dynamic problem of a firm. But here, we just specify it in a reduced-form. Define variable \\(\\gamma\\) and assign it the value. Write a function that returns the investment given \\(K_{jt}\\), \\(\\omega_{jt}\\), and parameter values, according to the previous equation, and name it investment_choice(k, omega, gamma, delta). Simulate the data first using the labor choice without optimization error and second using the labor choice with optimization error. To do so, we specify the initial values for the state variables \\(k_{jt}\\) and \\(\\omega_{jt}\\) as follows. Draw \\(k_{j1}\\) from an i.i.d. normal distribution with mean 1 and standard deviation 0.5. Draw \\(\\omega_{j1}\\) from its stationary distribution (check the stationary distribution of AR(1) process). Draw a wage. Before simulating the rest of the data, set the seed at 1. ## # A tibble: 1,000 x 5 ## j t k omega wage ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 ## 2 2 1 1.09 0.779 0.5 ## 3 3 1 0.582 -0.610 0.5 ## 4 4 1 1.80 0.148 0.5 ## 5 5 1 1.16 0.0486 0.5 ## 6 6 1 0.590 -1.16 0.5 ## 7 7 1 1.24 0.568 0.5 ## 8 8 1 1.37 -1.34 0.5 ## 9 9 1 1.29 -0.873 0.5 ## 10 10 1 0.847 0.699 0.5 ## # … with 990 more rows Draw optimization error \\(\\iota_{jt}\\) and compute the labor and investment choice of period 1. For labor choice, compute both types of labor choices. ## # A tibble: 1,000 x 9 ## j t k omega wage iota l l_error I ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 ## # … with 990 more rows Draw ex post shock and compute the output according to the production function for both labor without optimization error and with optimization error. Name the output without optimization error y and the one with optimization error y_error. ## # A tibble: 1,000 x 12 ## j t k omega wage iota l l_error I eta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 0.148 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 0.0773 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 0.259 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 -0.161 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 -0.321 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 0.187 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 0.361 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 -0.0113 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 0.377 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 0.316 ## # … with 990 more rows, and 2 more variables: y &lt;dbl&gt;, y_error &lt;dbl&gt; Repeat this procedure for \\(t = 1, \\cdots 10\\) by updating the capital and anticipated shocks, and name the resulting data frame df_T. ## # A tibble: 10,000 x 13 ## j t k omega wage iota l l_error I eta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.687 0.795 0.5 -0.0443 1.72 1.67 0.257 0.148 ## 2 2 1 1.09 0.779 0.5 -0.0961 2.06 1.94 0.381 0.0773 ## 3 3 1 0.582 -0.610 0.5 0.0810 -0.123 -0.0218 -0.0196 0.259 ## 4 4 1 1.80 0.148 0.5 0.0260 1.89 1.92 0.391 -0.161 ## 5 5 1 1.16 0.0486 0.5 -0.00279 1.21 1.21 0.176 -0.321 ## 6 6 1 0.590 -1.16 0.5 0.0348 -0.809 -0.766 -0.120 0.187 ## 7 7 1 1.24 0.568 0.5 0.00268 1.93 1.93 0.370 0.361 ## 8 8 1 1.37 -1.34 0.5 -0.0655 -0.346 -0.428 -0.330 -0.0113 ## 9 9 1 1.29 -0.873 0.5 -0.106 0.165 0.0327 -0.135 0.377 ## 10 10 1 0.847 0.699 0.5 -0.0104 1.74 1.73 0.280 0.316 ## # … with 9,990 more rows, and 3 more variables: y &lt;dbl&gt;, y_error &lt;dbl&gt;, ## # nu &lt;dbl&gt; Check the simulated data by making summary table. N Mean Sd Min Max j 10000 500.5000000 288.6894251 1.0000000 1000.0000000 t 10000 5.5000000 2.8724249 1.0000000 10.0000000 k 10000 0.9797900 0.5838949 -1.2822534 3.2332312 omega 10000 -0.0055826 0.7025102 -2.5894171 2.6281307 wage 10000 0.5000000 0.0000000 0.5000000 0.5000000 iota 10000 -0.0000696 0.0502883 -0.1841453 0.1715419 l 10000 0.9799746 1.0965108 -3.3281023 4.9679634 l_error 10000 0.9798876 1.0971595 -3.3765433 4.9520674 I 10000 0.1793502 0.3006526 -1.2722627 3.2975332 eta 10000 0.0015825 0.2001539 -0.7650371 0.7455922 y 10000 1.8778479 1.1171035 -2.4680251 6.1228291 y_error 10000 1.8778305 1.1169266 -2.4777133 6.1196499 nu 10000 -0.0021155 0.4984324 -2.1513907 1.8253882 8.2 Estimate the parameters For now, use the labor choice with optimization error. First, simply regress \\(y_{jt}\\) on \\(l_{jt}\\) and \\(k_{jt}\\) using the least square method. This is likely to give an upwardly biased estimates on \\(\\beta_l\\) and \\(\\beta_k\\). Why is it? ## ## Call: ## lm(formula = y_error ~ l_error + k, data = df_T) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73002 -0.14117 -0.00071 0.13743 0.87983 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.892542 0.004058 219.966 &lt;2e-16 *** ## l_error 0.997913 0.002396 416.454 &lt;2e-16 *** ## k 0.007599 0.004503 1.688 0.0915 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2068 on 9997 degrees of freedom ## Multiple R-squared: 0.9657, Adjusted R-squared: 0.9657 ## F-statistic: 1.408e+05 on 2 and 9997 DF, p-value: &lt; 2.2e-16 Second, take within-transformation on \\(y_{jt}\\), \\(l_{jt}\\), and \\(k_{jt}\\) and let \\(\\Delta y_{jt}\\), \\(\\Delta l_{jt}\\), and \\(\\Delta k_{jt}\\) denote them. Then, regress \\(\\Delta y_{jt}\\) on \\(\\Delta l_{jt}\\), and \\(\\Delta k_{jt}\\) by the least squares method. ## ## Call: ## lm(formula = dy_error ~ -1 + dl_error + dk, data = df_T_within) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72450 -0.13285 -0.00244 0.12931 0.77657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## dl_error 0.9910916 0.0029548 335.413 &lt;2e-16 *** ## dk -0.0009029 0.0127539 -0.071 0.944 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1961 on 9998 degrees of freedom ## Multiple R-squared: 0.9184, Adjusted R-squared: 0.9184 ## F-statistic: 5.629e+04 on 2 and 9998 DF, p-value: &lt; 2.2e-16 Next, we attempt to estimate the parameters using Olley-Pakes method. Estimate the first-step model of Olley-Pakes method: \\[ y_{jt} = \\beta_0 + \\beta_1 l_{jt} + \\phi(k_{jt}, I_{jt}) + \\eta_{jt}, \\] by approximating \\(\\phi_t\\) by a kernel function. Remark that \\(\\phi\\) in general depends on observed and unobserved state variables. For this reason, in theory, \\(\\phi\\) should be estimated for each period. In this exercise, we assume \\(\\phi\\) is common across periods because we know that there is no unobserved state variables in the true data generating process. Moreover, we do not include \\(w_t\\) because we know that it is time -invariant. Do not forget to consider them in the actual data analysis. You can use npplreg function of np package to estimate a partially linear model with a multivariate kernel. You first use npplregbw to obtain the optimal band width and then use npplreg to estimate the model under the optimal bandwidth. The computation of the optimal bandwidth is time consuming. Return the summary of the first stage estimation and plot the fitted values against the data points. ## ## Partially Linear Model ## Regression data: 10000 training points, in 5 variable(s) ## With 3 linear parametric regressor(s), 2 nonparametric regressor(s) ## ## y(z) ## Bandwidth(s): 0.07355058 0.01435558 ## ## x(z) ## Bandwidth(s): 0.03908594 0.01191551 ## 0.01397428 3.74952954 ## 0.83529394 0.00398329 ## ## l_error k I ## Coefficient(s): 0.2485295 2.355522 5.345144 ## ## Kernel Regression Estimator: Local-Constant ## Bandwidth Type: Fixed ## ## Residual standard error: 0.1934585 ## R-squared: 0.970064 ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 2 Check that \\(\\beta_l\\) is not identified with the data without optimization error. Estimate the first stage model of Olley-Pakes with the labor choice without optimization error and report the result. ## ## Partially Linear Model ## Regression data: 10000 training points, in 5 variable(s) ## With 3 linear parametric regressor(s), 2 nonparametric regressor(s) ## ## y(z) ## Bandwidth(s): 0.07347226 0.01437256 ## ## x(z) ## Bandwidth(s): 0.02960021 0.009986945 ## 0.01397428 3.749529542 ## 0.83529394 0.003983290 ## ## l k I ## Coefficient(s): 1.180628 2.034182 0.7805077 ## ## Kernel Regression Estimator: Local-Constant ## Bandwidth Type: Fixed ## ## Residual standard error: 0.1932285 ## R-squared: 0.970116 ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 2 Then, we estimate the second stage model of Olley-Pakes method: \\[ y_{jt} - \\hat{\\beta_l} l_{jt} = \\beta_0 + \\beta_k k_{jt} + \\alpha[\\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}) - \\beta_0 - \\beta_k k_{jt}] + \\nu_{jt} + \\eta_{jt}. \\] In this model, we do not have to non-parametetrically estimate the conditional expectation of \\(\\omega_{jt}\\) on \\(\\omega_{j, t - 1}\\), because we know that the anticipated shock follows an AR(1) process. Remark that we in general have to non-parametrically estimate it. The model is non-linear in parameters, because of the term \\(\\alpha \\beta_0\\) and \\(\\alpha \\beta_k\\). We estimate \\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_k\\) by a GMM estimator. The moment is: \\[ g_{JT}(\\alpha, \\beta_0, \\beta_k) \\equiv \\frac{1}{JT}\\sum_{j = 1}^J \\sum_{t = 1}^T \\{y_{jt} - \\hat{\\beta_l} l_{jt} - \\beta_0 - \\beta_k k_{jt} - \\alpha[\\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}) - \\beta_0 - \\beta_k k_{jt}]\\} \\begin{bmatrix} k_{jt} \\\\ k_{j, t - 1} \\\\ I_{j, t - 1} \\end{bmatrix}. \\] Using the estimates in the first step, compute: \\[ y_{jt} - \\hat{\\beta_l} l_{jt}, \\] and: \\[ \\hat{\\phi}(k_{j, t - 1}, I_{j, t - 1}), \\] for each \\(j\\) and \\(t\\) and save it as a data frame names df_T_1st. ## # A tibble: 10,000 x 4 ## j t y_error_tilde phi_t_1 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2.34 NA ## 2 1 2 1.37 2.21 ## 3 1 3 0.621 1.49 ## 4 1 4 0.447 0.882 ## 5 1 5 0.878 0.611 ## 6 1 6 1.62 0.926 ## 7 1 7 0.558 1.40 ## 8 1 8 0.684 0.439 ## 9 1 9 0.939 0.520 ## 10 1 10 1.49 0.836 ## # … with 9,990 more rows Compute a function that returns the value of \\(g_{JT}(\\alpha, \\beta_0, \\beta_k)\\) given parameter values, data, and df_T_1st, and name it moment_OP_2nd. Show the values of the moments evaluated at the true parameters. ## [1] -0.018507303 -0.019038229 -0.003867714 Based on the moment, we can define the objective function of a generalized method of moments estimator with a weighting matrix \\(W\\) as: \\[ Q_{JT}(\\alpha, \\beta_0, \\beta_k) \\equiv g_{JT}(\\alpha, \\beta_0, \\beta_k)&#39; W g_{JT}(\\alpha, \\beta_0, \\beta_k). \\] Write a function that returns the value of \\(Q_{JT}(\\alpha, \\beta_0, \\beta_k)\\) given the vector of parameter values, data, and df_T_1st, and name it objective_OP_2nd. Setting \\(W\\) at the identity matrix, show the value of the objective function evaluated at the true parameters. ## [,1] ## [1,] 0.0007199336 Draw the graph of the objective function when one of \\(\\alpha\\), \\(\\beta_0\\), and \\(\\beta_k\\) are changed from 0 to 1 by 0.1 while the others are set at the true value. Is the objective function minimized at around the true value? Find the parameters that minimize the objective function using optim. You may use L-BFGS-B method to solve it. ## $par ## [1] 0.7020260 0.9766308 0.6693945 ## ## $value ## [1] 1.994601e-07 ## ## $counts ## function gradient ## 10 10 ## ## $convergence ## [1] 0 ## ## $message ## [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; "],
["assignment3.html", "Chapter 9 Assignment 3: Demand Function Estimation I 9.1 Simulate data 9.2 Estimate the parameters", " Chapter 9 Assignment 3: Demand Function Estimation I The deadline is March 11 1:30pm. 9.1 Simulate data We simulate data from a discrete choice model. There are \\(T\\) markets and each market has \\(N\\) consumers. There are \\(J\\) products and the indirect utility of consumer \\(i\\) in market \\(t\\) for product \\(j\\) is: \\[ u_{itj} = \\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt} + \\epsilon_{ijt}, \\] where \\(\\epsilon_{ijt}\\) is an i.i.d. type-I extreme random variable. \\(x_j\\) is \\(K\\)-dimensional observed characteristics of the product. \\(p_{jt}\\) is the retail price of the product in the market. \\(\\xi_{jt}\\) is product-market specific fixed effect. \\(p_{jt}\\) can be correlated with \\(\\xi_{jt}\\) but \\(x_{jt}\\)s are independent of \\(\\xi_{jt}\\). \\(j = 0\\) is an outside option whose indirect utility is: \\[ u_{it0} = \\epsilon_{i0t}, \\] where \\(\\epsilon_{i0t}\\) is an i.i.d. type-I extreme random variable. \\(\\beta_{it}\\) and \\(\\alpha_{it}\\) are different across consumers, and they are distributed as: \\[ \\beta_{itk} = \\beta_{0k} + \\sigma_k \\nu_{itk}, \\] \\[ \\alpha_{it} = - \\exp(\\mu + \\omega \\upsilon_{it}) = - \\exp(\\mu + \\frac{\\omega^2}{2}) + [- \\exp(\\mu + \\omega \\upsilon_{it}) + \\exp(\\mu + \\frac{\\omega^2}{2})] \\equiv \\alpha_0 + \\tilde{\\alpha}_{it}, \\] where \\(\\nu_{itk}\\) for \\(k = 1, \\cdots, K\\) and \\(\\upsilon_{it}\\) are i.i.d. standard normal random variables. \\(\\alpha_0\\) is the mean of \\(\\alpha_i\\) and \\(\\tilde{\\alpha}_i\\) is the deviation from the mean. Given a choice set in the market, \\(\\mathcal{J}_t \\cup \\{0\\}\\), a consumer chooses the alternative that maximizes her utility: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] The choice probability of product \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[ \\sigma_{jt}(p_t, x_t, \\xi_t) = \\mathbb{P}\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] Suppose that we only observe the share data: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N q_{ijt}, \\] along with the product-market characteristics \\(x_{jt}\\) and the retail prices \\(p_{jt}\\) for \\(j \\in \\mathcal{J}_t \\cup \\{0\\}\\) for \\(t = 1, \\cdots, T\\). We do not observe the choice data \\(q_{ijt}\\) nor shocks \\(\\xi_{jt}, \\nu_{it}, \\upsilon_{it}, \\epsilon_{ijt}\\). In this assignment, we consider a model with \\(\\xi_{jt} = 0\\), i.e., the model without the unobserved fixed effects. However, the code to simulate data should be written for general \\(\\xi_{jt}\\), so that we can use the same code in the next assignment in which we consider a model with the unobserved fixed effects. Set the seed, constants, and parameters of interest as follows. # set the seed set.seed(1) # number of products J &lt;- 10 # dimension of product characteristics including the intercept K &lt;- 3 # number of markets T &lt;- 100 # number of consumers per market N &lt;- 500 # number of Monte Carlo L &lt;- 500 # set parameters of interests beta &lt;- rnorm(K); beta[1] &lt;- 4 beta ## [1] 4.0000000 0.1836433 -0.8356286 sigma &lt;- abs(rnorm(K)); sigma ## [1] 1.5952808 0.3295078 0.8204684 mu &lt;- 0.5 omega &lt;- 1 Generate the covariates as follows. The product-market characteristics: \\[ x_{j1} = 1, x_{jk} \\sim N(0, \\sigma_x), k = 2, \\cdots, K, \\] where \\(\\sigma_x\\) is referred to as sd_x in the code. The product-market-specific unobserved fixed effect: \\[ \\xi_{jt} = 0. \\] The marginal cost of product \\(j\\) in market \\(t\\): \\[ c_{jt} \\sim \\text{logNormal}(0, \\sigma_c), \\] where \\(\\sigma_c\\) is referred to as sd_c in the code. The retail price: \\[ p_{jt} - c_{jt} \\sim \\text{logNorm}(\\gamma \\xi_{jt}, \\sigma_p), \\] where \\(\\gamma\\) is referred to as price_xi and \\(\\sigma_p\\) as sd_p in the code. This price is not the equilibrium price. We will revisit this point in a subsequent assignment. The value of the auxiliary parameters are set as follows: # set auxiliary parameters price_xi &lt;- 1 prop_jt &lt;- 0.6 sd_x &lt;- 0.5 sd_c &lt;- 0.05 sd_p &lt;- 0.05 X is the data frame such that a row contains the characteristics vector \\(x_{j}\\) of a product and columns are product index and observed product characteristics. The dimension of the characteristics \\(K\\) is specified above. Add the row of the outside option whose index is \\(0\\) and all the characteristics are zero. X ## # A tibble: 11 x 4 ## j x_1 x_2 x_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 1 1 0.244 -0.00810 ## 3 2 1 0.369 0.472 ## 4 3 1 0.288 0.411 ## 5 4 1 -0.153 0.297 ## 6 5 1 0.756 0.459 ## 7 6 1 0.195 0.391 ## 8 7 1 -0.311 0.0373 ## 9 8 1 -1.11 -0.995 ## 10 9 1 0.562 0.310 ## 11 10 1 -0.0225 -0.0281 M is the data frame such that a row contains the price \\(\\xi_{jt}\\), marginal cost \\(c_{jt}\\), and price \\(p_{jt}\\). After generating the variables, drop 1 - prop_jt products from each market using dplyr::sample_frac function. The variation in the available products is important for the identification of the distribution of consumer-level unobserved heterogeneity. Add the row of the outside option to each market whose index is \\(0\\) and all the variables take value zero. M ## # A tibble: 700 x 5 ## j t xi c p ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 1 1 0 0.951 1.93 ## 3 5 1 0 0.974 1.94 ## 4 6 1 0 0.980 1.96 ## 5 7 1 0 0.961 1.94 ## 6 8 1 0 0.989 1.99 ## 7 10 1 0 1.02 2.09 ## 8 0 2 0 0 0 ## 9 1 2 0 0.988 2.09 ## 10 2 2 0 1.04 1.96 ## # … with 690 more rows Generate the consumer-level heterogeneity. V is the data frame such that a row contains the vector of shocks to consumer-level heterogeneity, \\((\\nu_{i}&#39;, \\upsilon_i)\\). They are all i.i.d. standard normal random variables. V ## # A tibble: 50,000 x 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.02 0.731 -0.169 -1.40 ## 2 2 1 0.375 0.418 -0.243 -0.899 ## 3 3 1 -1.14 0.257 -2.56 1.44 ## 4 4 1 -0.752 0.449 0.718 0.497 ## 5 5 1 3.06 0.355 0.652 2.02 ## 6 6 1 1.44 -0.0302 0.585 0.406 ## 7 7 1 0.323 -0.363 -0.441 0.618 ## 8 8 1 -0.107 0.392 0.823 1.56 ## 9 9 1 -0.0515 0.733 -0.454 1.30 ## 10 10 1 0.790 0.468 1.10 -0.241 ## # … with 49,990 more rows Join X, M, V using dplyr::left_join and name it df. df is the data frame such that a row contains variables for a consumer about a product that is available in a market. df ## # A tibble: 350,000 x 13 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.02 0.731 -0.169 -1.40 0 0 0 0 ## 2 1 1 1 1.02 0.731 -0.169 -1.40 1 0.244 -0.00810 0 ## 3 1 1 5 1.02 0.731 -0.169 -1.40 1 0.756 0.459 0 ## 4 1 1 6 1.02 0.731 -0.169 -1.40 1 0.195 0.391 0 ## 5 1 1 7 1.02 0.731 -0.169 -1.40 1 -0.311 0.0373 0 ## 6 1 1 8 1.02 0.731 -0.169 -1.40 1 -1.11 -0.995 0 ## 7 1 1 10 1.02 0.731 -0.169 -1.40 1 -0.0225 -0.0281 0 ## 8 1 2 0 0.375 0.418 -0.243 -0.899 0 0 0 0 ## 9 1 2 1 0.375 0.418 -0.243 -0.899 1 0.244 -0.00810 0 ## 10 1 2 5 0.375 0.418 -0.243 -0.899 1 0.756 0.459 0 ## # … with 349,990 more rows, and 2 more variables: c &lt;dbl&gt;, p &lt;dbl&gt; Draw a vector of preference shocks e whose length is the same as the number of rows of df. head(e) ## [1] -0.01971328 -0.44401874 0.15952459 0.17658106 -0.55495888 -0.12854864 Write a function compute_indirect_utility(df, beta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). # compute indirect utility u &lt;- compute_indirect_utility( df, beta, sigma, mu, omega) head(u) ## u ## [1,] 0.000000 ## [2,] 4.957950 ## [3,] 4.716943 ## [4,] 4.537668 ## [5,] 4.672690 ## [6,] 5.322723 Write a function compute_choice(X, M, V, e, beta, sigma, mu, omega) that first construct df from X, M, V, second call compute_indirect_utility to obtain the vector of mean indirect utilities u, third compute the choice vector q based on the vector of mean indirect utilities and e, and finally return the data frame to which u and q are added as columns. # compute choice df_choice &lt;- compute_choice(X, M, V, e, beta, sigma, mu, omega) df_choice ## # A tibble: 350,000 x 16 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.02 0.731 -0.169 -1.40 0 0 0 0 ## 2 1 1 1 1.02 0.731 -0.169 -1.40 1 0.244 -0.00810 0 ## 3 1 1 5 1.02 0.731 -0.169 -1.40 1 0.756 0.459 0 ## 4 1 1 6 1.02 0.731 -0.169 -1.40 1 0.195 0.391 0 ## 5 1 1 7 1.02 0.731 -0.169 -1.40 1 -0.311 0.0373 0 ## 6 1 1 8 1.02 0.731 -0.169 -1.40 1 -1.11 -0.995 0 ## 7 1 1 10 1.02 0.731 -0.169 -1.40 1 -0.0225 -0.0281 0 ## 8 1 2 0 0.375 0.418 -0.243 -0.899 0 0 0 0 ## 9 1 2 1 0.375 0.418 -0.243 -0.899 1 0.244 -0.00810 0 ## 10 1 2 5 0.375 0.418 -0.243 -0.899 1 0.756 0.459 0 ## # … with 349,990 more rows, and 5 more variables: c &lt;dbl&gt;, p &lt;dbl&gt;, ## # u &lt;dbl&gt;, e &lt;dbl&gt;, q &lt;dbl&gt; summary(df_choice) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.000 Min. :-4.302781 ## 1st Qu.: 25.75 1st Qu.:125.8 1st Qu.: 2.000 1st Qu.:-0.685716 ## Median : 50.50 Median :250.5 Median : 5.000 Median : 0.000103 ## Mean : 50.50 Mean :250.5 Mean : 4.639 Mean :-0.004312 ## 3rd Qu.: 75.25 3rd Qu.:375.2 3rd Qu.: 7.000 3rd Qu.: 0.668186 ## Max. :100.00 Max. :500.0 Max. :10.000 Max. : 3.809895 ## v_x_2 v_x_3 v_p ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 ## 1st Qu.:-0.678436 1st Qu.:-0.674487 1st Qu.:-0.670251 ## Median : 0.000444 Median : 0.005891 Median : 0.002309 ## Mean :-0.001340 Mean : 0.003736 Mean :-0.001305 ## 3rd Qu.: 0.670840 3rd Qu.: 0.678349 3rd Qu.: 0.671041 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.074300 ## x_1 x_2 x_3 xi ## Min. :0.0000 Min. :-1.10735 Min. :-0.9947 Min. :0 ## 1st Qu.:1.0000 1st Qu.:-0.15269 1st Qu.: 0.0000 1st Qu.:0 ## Median :1.0000 Median : 0.19492 Median : 0.2970 Median :0 ## Mean :0.8571 Mean : 0.06936 Mean : 0.1186 Mean :0 ## 3rd Qu.:1.0000 3rd Qu.: 0.36916 3rd Qu.: 0.4106 3rd Qu.:0 ## Max. :1.0000 Max. : 0.75589 Max. : 0.4719 Max. :0 ## c p u e ## Min. :0.0000 Min. :0.000 Min. :-200.871 Min. :-2.6364 ## 1st Qu.:0.9417 1st Qu.:1.921 1st Qu.: -2.202 1st Qu.:-0.3302 ## Median :0.9887 Median :1.986 Median : 0.000 Median : 0.3634 ## Mean :0.8583 Mean :1.718 Mean : -1.316 Mean : 0.5760 ## 3rd Qu.:1.0278 3rd Qu.:2.046 3rd Qu.: 1.961 3rd Qu.: 1.2415 ## Max. :1.1996 Max. :2.192 Max. : 10.731 Max. :14.0966 ## q ## Min. :0.0000 ## 1st Qu.:0.0000 ## Median :0.0000 ## Mean :0.1429 ## 3rd Qu.:0.0000 ## Max. :1.0000 Write a function compute_share(X, M, V, e, beta, sigma, mu, omega) that first construct df from X, M, V, second call compute_choice to obtain a data frame with u and q, third compute the share of each product at each market s and the log difference in the share from the outside option, \\(\\ln(s_{jt}/s_{0t})\\), denoted by y, and finally return the data frame that is summarized at the product-market level, dropped consumer-level variables, and added s and y. # compute share df_share &lt;- compute_share(X, M, V, e, beta, sigma, mu, omega) df_share ## # A tibble: 700 x 11 ## t j x_1 x_2 x_3 xi c p q s y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 153 0.306 0 ## 2 1 1 1 0.244 -0.00810 0 0.951 1.93 49 0.098 -1.14 ## 3 1 5 1 0.756 0.459 0 0.974 1.94 38 0.076 -1.39 ## 4 1 6 1 0.195 0.391 0 0.980 1.96 41 0.082 -1.32 ## 5 1 7 1 -0.311 0.0373 0 0.961 1.94 45 0.09 -1.22 ## 6 1 8 1 -1.11 -0.995 0 0.989 1.99 131 0.262 -0.155 ## 7 1 10 1 -0.0225 -0.0281 0 1.02 2.09 43 0.086 -1.27 ## 8 2 0 0 0 0 0 0 0 170 0.34 0 ## 9 2 1 1 0.244 -0.00810 0 0.988 2.09 50 0.1 -1.22 ## 10 2 2 1 0.369 0.472 0 1.04 1.96 37 0.074 -1.52 ## # … with 690 more rows summary(df_share) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.000 Min. :0.0000 Min. :-1.10735 ## 1st Qu.: 25.75 1st Qu.: 2.000 1st Qu.:1.0000 1st Qu.:-0.15269 ## Median : 50.50 Median : 5.000 Median :1.0000 Median : 0.19492 ## Mean : 50.50 Mean : 4.639 Mean :0.8571 Mean : 0.06936 ## 3rd Qu.: 75.25 3rd Qu.: 7.000 3rd Qu.:1.0000 3rd Qu.: 0.36916 ## Max. :100.00 Max. :10.000 Max. :1.0000 Max. : 0.75589 ## x_3 xi c p ## Min. :-0.9947 Min. :0 Min. :0.0000 Min. :0.000 ## 1st Qu.: 0.0000 1st Qu.:0 1st Qu.:0.9417 1st Qu.:1.921 ## Median : 0.2970 Median :0 Median :0.9887 Median :1.986 ## Mean : 0.1186 Mean :0 Mean :0.8583 Mean :1.718 ## 3rd Qu.: 0.4106 3rd Qu.:0 3rd Qu.:1.0278 3rd Qu.:2.046 ## Max. : 0.4719 Max. :0 Max. :1.1996 Max. :2.192 ## q s y ## Min. : 25.00 Min. :0.0500 Min. :-1.9459 ## 1st Qu.: 43.00 1st Qu.:0.0860 1st Qu.:-1.3636 ## Median : 51.00 Median :0.1020 Median :-1.1579 ## Mean : 71.43 Mean :0.1429 Mean :-0.9968 ## 3rd Qu.: 73.00 3rd Qu.:0.1460 3rd Qu.:-0.8316 ## Max. :191.00 Max. :0.3820 Max. : 0.0000 9.2 Estimate the parameters Estimate the parameters assuming there is no consumer-level heterogeneity, i.e., by assuming: \\[ \\ln \\frac{s_{jt}}{s_{0t}} = \\beta&#39; x_{jt} + \\alpha p_{jt}. \\] This can be implemented using lm function. Print out the estimate results. ## ## Call: ## lm(formula = y ~ -1 + x_1 + x_2 + x_3 + p, data = df_share) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5777 -0.1051 0.0000 0.1042 0.4913 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_1 0.97770 0.19287 5.069 5.13e-07 *** ## x_2 0.17795 0.02945 6.043 2.46e-09 *** ## x_3 -0.87591 0.03482 -25.159 &lt; 2e-16 *** ## p -1.01500 0.09613 -10.559 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1731 on 696 degrees of freedom ## Multiple R-squared: 0.9765, Adjusted R-squared: 0.9764 ## F-statistic: 7237 on 4 and 696 DF, p-value: &lt; 2.2e-16 We estimate the model using simulated share. When optimizing an objective function that uses the Monte Carlo simulation, it is important to keep the realizations of the shocks the same across the evaluations of the objective function. If the realization of the shocks differ across the objective function evaluations, the optimization algorithm will not converge because it cannot distinguish the change in the value of the objective function due to the difference in the parameters and the difference in the realized shocks. The best practice to avoid this problem is to generate the shocks outside the optimization algorithm as in the current case. If the size of the shocks can be too large to store in the memory, the second best practice is to make sure to set the seed inside the optimization algorithm so that the realized shocks are the same across function evaluations. For this reason, we first draw Monte Carlo consumer-level heterogeneity V_mcmc and Monte Carlo preference shocks e_mcmc. The number of simulations is L. This does not have to be the same with the actual number of consumers N. V_mcmc ## # A tibble: 50,000 x 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 -1.07 -1.30 2.32 0.110 ## 2 2 1 -0.730 0.684 1.07 -0.802 ## 3 3 1 -0.437 -0.243 0.383 -0.318 ## 4 4 1 -0.979 0.520 1.02 0.637 ## 5 5 1 0.487 -0.991 0.0422 0.613 ## 6 6 1 -0.805 1.15 1.08 -0.473 ## 7 7 1 0.761 0.353 -2.05 -0.989 ## 8 8 1 0.965 1.76 -1.34 -0.686 ## 9 9 1 0.702 -0.583 0.144 -0.0259 ## 10 10 1 0.213 1.60 -1.32 1.72 ## # … with 49,990 more rows head(e_mcmc) ## [1] 0.8830453 1.1151824 3.2225788 -0.9125983 0.8022472 5.1145476 Use compute_share to check the simulated share at the true parameter using the Monte Carlo shocks. Remember that the number of consumers should be set at L instead of N. df_share_mcmc ## # A tibble: 700 x 11 ## t j x_1 x_2 x_3 xi c p q s y ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 153 0.306 0 ## 2 1 1 1 0.244 -0.00810 0 0.951 1.93 59 0.118 -0.953 ## 3 1 5 1 0.756 0.459 0 0.974 1.94 46 0.092 -1.20 ## 4 1 6 1 0.195 0.391 0 0.980 1.96 39 0.078 -1.37 ## 5 1 7 1 -0.311 0.0373 0 0.961 1.94 51 0.102 -1.10 ## 6 1 8 1 -1.11 -0.995 0 0.989 1.99 107 0.214 -0.358 ## 7 1 10 1 -0.0225 -0.0281 0 1.02 2.09 45 0.09 -1.22 ## 8 2 0 0 0 0 0 0 0 164 0.328 0 ## 9 2 1 1 0.244 -0.00810 0 0.988 2.09 51 0.102 -1.17 ## 10 2 2 1 0.369 0.472 0 1.04 1.96 33 0.066 -1.60 ## # … with 690 more rows Vectorize the parameters to a vector theta because optim requires the maximiand to be a vector. # set parameters theta &lt;- c(beta, sigma, mu, omega) theta ## [1] 4.0000000 0.1836433 -0.8356286 1.5952808 0.3295078 0.8204684 ## [7] 0.5000000 1.0000000 Write a function NLLS_objective_A3(theta, df_share, X, M, V_mcmc, e_mcmc) that first computes the simulated share and then compute the mean-squared error between the share data. NLLS_objective ## [1] 0.0004878743 Draw a graph of the objective function that varies each parameter from 0.5, 0.6, \\(\\cdots\\), 1.5 of the true value. First try with the actual shocks V and e and then try with the Monte Carlo shocks V_mcmc and e_mcmc. You will some of the graph does not look good with the Monte Carlo shocks. It will cause the approximation error. Because this takes time, you may want to parallelize the computation using %dopar functionality of foreach loop. To do so, first install doParallel package and then load it and register the workers as follows: registerDoParallel() This automatically detect the number of cores available at your computer and registers them as the workers. Then, you only have to change %do% to %dopar in the foreach loop as follows: foreach (i = 1:4) %dopar% { # this part is parallelized y &lt;- 2 * i return(y) } ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 In windows, you may have to explicitly pass packages, functions, and data to the worker by using .export and .packages options as follows: temp_func &lt;- function(x) { y &lt;- 2 * x return(y) } foreach (i = 1:4, .export = &quot;temp_func&quot;, .packages = &quot;magrittr&quot;) %dopar% { # this part is parallelized y &lt;- temp_func(i) return(y) } ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 If you have called a function in a package in this way dplyr::mutate, then you will not have to pass dplyr by .packages option. This is one of the reasons why I prefer to explicitly call the every time I call a function. If you have compiled your functions in a package, you will just have to pass the package as follows: # this function is compiled in the package EmpiricalIO # temp_func &lt;- function(x) { # y &lt;- 2 * x # return(y) # } foreach (i = 1:4, .packages = c( &quot;EmpiricalIO&quot;, &quot;magrittr&quot;)) %dopar% { # this part is parallelized y &lt;- temp_func(i) return(y) } ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 6 ## ## [[4]] ## [1] 8 The graphs with the true shocks: ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] ## ## [[7]] ## ## [[8]] The graphs with the Monte Carlo shocks: ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] ## ## [[7]] ## ## [[8]] Use optim to find the minimizer of the objective function using Nelder-Mead method. You can start from the true parameter values. Compare the estimates with the true parameters. ## $par ## [1] 4.0425713 0.1841677 -0.8230489 1.6348574 0.3148886 0.7831262 ## [7] 0.4998710 1.0138327 ## ## $value ## [1] 0.0004760686 ## ## $counts ## function gradient ## 263 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimates ## 1 4.0000000 4.0425713 ## 2 0.1836433 0.1841677 ## 3 -0.8356286 -0.8230489 ## 4 1.5952808 1.6348574 ## 5 0.3295078 0.3148886 ## 6 0.8204684 0.7831262 ## 7 0.5000000 0.4998710 ## 8 1.0000000 1.0138327 "],
["assignment4.html", "Chapter 10 Assignment 4: Demand Function Estimation II 10.1 Simulate data 10.2 Estimate the parameters", " Chapter 10 Assignment 4: Demand Function Estimation II The deadline is March 25 1:30pm. 10.1 Simulate data Be carefull that some parameters are changed from assignment 3. We simulate data from a discrete choice model that is the same with in assignment 3 except for the existence of unobserved product-specific fixed effects. There are \\(T\\) markets and each market has \\(N\\) consumers. There are \\(J\\) products and the indirect utility of consumer \\(i\\) in market \\(t\\) for product \\(j\\) is: \\[ u_{itj} = \\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt} + \\epsilon_{ijt}, \\] where \\(\\epsilon_{ijt}\\) is an i.i.d. type-I extreme random variable. \\(x_j\\) is \\(K\\)-dimensional observed characteristics of the product. \\(p_{jt}\\) is the retail price of the product in the market. \\(\\xi_{jt}\\) is product-market specific fixed effect. \\(p_{jt}\\) can be correlated with \\(\\xi_{jt}\\) but \\(x_{jt}\\)s are independent of \\(\\xi_{jt}\\). \\(j = 0\\) is an outside option whose indirect utility is: \\[ u_{it0} = \\epsilon_{i0t}, \\] where \\(\\epsilon_{i0t}\\) is an i.i.d. type-I extreme random variable. \\(\\beta_{it}\\) and \\(\\alpha_{it}\\) are different across consumers, and they are distributed as: \\[ \\beta_{itk} = \\beta_{0k} + \\sigma_k \\nu_{itk}, \\] \\[ \\alpha_{it} = - \\exp(\\mu + \\omega \\upsilon_{it}) = - \\exp(\\mu + \\frac{\\omega^2}{2}) + [- \\exp(\\mu + \\omega \\upsilon_{it}) + \\exp(\\mu + \\frac{\\omega^2}{2})] \\equiv \\alpha_0 + \\tilde{\\alpha}_{it}, \\] where \\(\\nu_{itk}\\) for \\(k = 1, \\cdots, K\\) and \\(\\upsilon_{it}\\) are i.i.d. standard normal random variables. \\(\\alpha_0\\) is the mean of \\(\\alpha_i\\) and \\(\\tilde{\\alpha}_i\\) is the deviation from the mean. Given a choice set in the market, \\(\\mathcal{J}_t \\cup \\{0\\}\\), a consumer chooses the alternative that maximizes her utility: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] The choice probability of product \\(j\\) for consumer \\(i\\) in market \\(t\\) is: \\[ \\sigma_{jt}(p_t, x_t, \\xi_t) = \\mathbb{P}\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}. \\] Suppose that we only observe the share data: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N q_{ijt}, \\] along with the product-market characteristics \\(x_{jt}\\) and the retail prices \\(p_{jt}\\) for \\(j \\in \\mathcal{J}_t \\cup \\{0\\}\\) for \\(t = 1, \\cdots, T\\). We do not observe the choice data \\(q_{ijt}\\) nor shocks \\(\\xi_{jt}, \\nu_{it}, \\upsilon_{it}, \\epsilon_{ijt}\\). We draw \\(\\xi_{jt}\\) from i.i.d. normal distribution with mean 0 and standard deviation \\(\\sigma_{\\xi}\\). Set the seed, constants, and parameters of interest as follows. # set the seed set.seed(1) # number of products J &lt;- 10 # dimension of product characteristics including the intercept K &lt;- 3 # number of markets T &lt;- 100 # number of consumers per market N &lt;- 500 # number of Monte Carlo L &lt;- 500 # set parameters of interests beta &lt;- rnorm(K); beta[1] &lt;- 4 beta ## [1] 4.0000000 0.1836433 -0.8356286 sigma &lt;- abs(rnorm(K)); sigma ## [1] 1.5952808 0.3295078 0.8204684 mu &lt;- 0.5 omega &lt;- 1 Generate the covariates as follows. The product-market characteristics: \\[ x_{j1} = 1, x_{jk} \\sim N(0, \\sigma_x), k = 2, \\cdots, K, \\] where \\(\\sigma_x\\) is referred to as sd_x in the code. The product-market-specific unobserved fixed effect: \\[ \\xi_{jt} \\sim N(0, \\sigma_\\xi), \\] where \\(\\sigma_xi\\) is referred to as sd_xi in the code. The marginal cost of product \\(j\\) in market \\(t\\): \\[ c_{jt} \\sim \\text{logNormal}(0, \\sigma_c), \\] where \\(\\sigma_c\\) is referred to as sd_c in the code. The retail price: \\[ p_{jt} - c_{jt} \\sim \\text{logNorm}(\\gamma \\xi_{jt}, \\sigma_p), \\] where \\(\\gamma\\) is referred to as price_xi and \\(\\sigma_p\\) as sd_p in the code. This price is not the equilibrium price. We will revisit this point in a subsequent assignment. The value of the auxiliary parameters are set as follows: # set auxiliary parameters price_xi &lt;- 1 sd_x &lt;- 2 sd_xi &lt;- 0.5 sd_c &lt;- 0.05 sd_p &lt;- 0.05 X is the data frame such that a row contains the characteristics vector \\(x_{j}\\) of a product and columns are product index and observed product characteristics. The dimension of the characteristics \\(K\\) is specified above. Add the row of the outside option whose index is \\(0\\) and all the characteristics are zero. X ## # A tibble: 11 x 4 ## j x_1 x_2 x_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 ## 2 1 1 0.975 -0.0324 ## 3 2 1 1.48 1.89 ## 4 3 1 1.15 1.64 ## 5 4 1 -0.611 1.19 ## 6 5 1 3.02 1.84 ## 7 6 1 0.780 1.56 ## 8 7 1 -1.24 0.149 ## 9 8 1 -4.43 -3.98 ## 10 9 1 2.25 1.24 ## 11 10 1 -0.0899 -0.112 M is the data frame such that a row contains the price \\(\\xi_{jt}\\), marginal cost \\(c_{jt}\\), and price \\(p_{jt}\\). After generating the variables, drop some products in each market. In this assignment, we drop products in a different way from the last assignment. In order to change the number of available products in each market, for each market, first draw \\(J_t\\) from a discrete uniform distribution between \\(1\\) and \\(J\\). Then, drop products from each market using dplyr::sample_frac function with the realized number of available products. The variation in the available products is important for the identification of the distribution of consumer-level unobserved heterogeneity. Add the row of the outside option to each market whose index is \\(0\\) and all the variables take value zero. M ## # A tibble: 746 x 5 ## j t xi c p ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 6 1 -0.0514 0.980 1.91 ## 3 0 2 0 0 0 ## 4 1 2 -0.197 0.988 1.90 ## 5 10 2 -0.354 1.05 1.82 ## 6 0 3 0 0 0 ## 7 1 3 0.182 1.04 2.22 ## 8 2 3 0.384 1.01 2.61 ## 9 3 3 -0.0562 1.02 2.01 ## 10 4 3 0.441 1.01 2.62 ## # … with 736 more rows Generate the consumer-level heterogeneity. V is the data frame such that a row contains the vector of shocks to consumer-level heterogeneity, \\((\\nu_{i}&#39;, \\upsilon_i)\\). They are all i.i.d. standard normal random variables. V ## # A tibble: 50,000 x 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.16 -1.40 0.0786 -1.15 ## 2 2 1 -1.05 -0.149 1.08 0.623 ## 3 3 1 -0.426 -4.21 0.625 -1.14 ## 4 4 1 -0.235 0.463 0.470 0.241 ## 5 5 1 1.19 -0.342 0.169 0.160 ## 6 6 1 0.541 0.525 0.305 1.72 ## 7 7 1 -0.0893 -0.434 2.18 -0.432 ## 8 8 1 -0.712 0.747 -0.306 -0.527 ## 9 9 1 0.504 1.11 1.70 -1.75 ## 10 10 1 -0.107 1.83 -0.841 0.693 ## # … with 49,990 more rows Join X, M, V using dplyr::left_join and name it df. df is the data frame such that a row contains variables for a consumer about a product that is available in a market. df ## # A tibble: 373,000 x 13 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.16 -1.40 0.0786 -1.15 0 0 0 0 ## 2 1 1 6 1.16 -1.40 0.0786 -1.15 1 0.780 1.56 -0.0514 ## 3 1 2 0 -1.05 -0.149 1.08 0.623 0 0 0 0 ## 4 1 2 6 -1.05 -0.149 1.08 0.623 1 0.780 1.56 -0.0514 ## 5 1 3 0 -0.426 -4.21 0.625 -1.14 0 0 0 0 ## 6 1 3 6 -0.426 -4.21 0.625 -1.14 1 0.780 1.56 -0.0514 ## 7 1 4 0 -0.235 0.463 0.470 0.241 0 0 0 0 ## 8 1 4 6 -0.235 0.463 0.470 0.241 1 0.780 1.56 -0.0514 ## 9 1 5 0 1.19 -0.342 0.169 0.160 0 0 0 0 ## 10 1 5 6 1.19 -0.342 0.169 0.160 1 0.780 1.56 -0.0514 ## # … with 372,990 more rows, and 2 more variables: c &lt;dbl&gt;, p &lt;dbl&gt; Draw a vector of preference shocks e whose length is the same as the number of rows of df. head(e) ## [1] 0.2262454 1.3417639 -0.1693913 0.8906905 0.5558130 1.9909058 Write a function compute_indirect_utility(df, beta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). (This function is the same with assignment 3. You can use the function.) # compute indirect utility u &lt;- compute_indirect_utility( df, beta, sigma, mu, omega) head(u) ## u ## [1,] 0.0000000 ## [2,] 3.3750542 ## [3,] 0.0000000 ## [4,] -3.3983588 ## [5,] 0.0000000 ## [6,] 0.8235142 In the previous assingment, we computed predicted share by simulating choice and taking their average. Instead, we compute the actual share by: \\[ s_{jt} = \\frac{1}{N} \\sum_{i = 1}^N \\frac{\\exp[\\beta_{it}&#39; x_j + \\alpha_{it} p_{jt} + \\xi_{jt}]}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp[\\beta_{it}&#39; x_k + \\alpha_{it} p_{kt} + \\xi_{jt}]} \\] and the predicted share by: \\[ \\widehat{\\sigma}_{j}(x, p_t, \\xi_t) = \\frac{1}{L} \\sum_{l = 1}^L \\frac{\\exp[\\beta_{t}^{(l)\\prime} x_j + \\alpha_{t}^{(l)} p_{jt} + \\xi_{jt}]}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp[\\beta_{t}^{(l)\\prime} x_k + \\alpha_{t}^{(l)} p_{kt} + \\xi_{jt}]}. \\] To do so, write a function compute_choice_smooth(X, M, V, beta, sigma, mu, omega) in which the choice of each consumer is not: \\[ q_{ijt} = 1\\{u_{ijt} = \\max_{k \\in \\mathcal{J}_t \\cup \\{0\\}} u_{ikt}\\}, \\] but \\[ \\tilde{q}_{ijt} = \\frac{\\exp(u_{ijt})}{1 + \\sum_{k \\in \\mathcal{J}_t} \\exp(u_{ikt})}. \\] df_choice_smooth &lt;- compute_choice_smooth(X, M, V, beta, sigma, mu, omega) summary(df_choice_smooth) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.000 Min. :-4.302781 ## 1st Qu.: 26.00 1st Qu.:125.8 1st Qu.: 2.000 1st Qu.:-0.685447 ## Median : 51.50 Median :250.5 Median : 5.000 Median :-0.000461 ## Mean : 51.26 Mean :250.5 Mean : 4.807 Mean :-0.005284 ## 3rd Qu.: 77.00 3rd Qu.:375.2 3rd Qu.: 8.000 3rd Qu.: 0.665219 ## Max. :100.00 Max. :500.0 Max. :10.000 Max. : 3.809895 ## v_x_2 v_x_3 v_p ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 ## 1st Qu.:-0.678377 1st Qu.:-0.676638 1st Qu.:-0.672011 ## Median : 0.001435 Median : 0.006281 Median : 0.002166 ## Mean :-0.001076 Mean : 0.003433 Mean :-0.001402 ## 3rd Qu.: 0.671827 3rd Qu.: 0.679273 3rd Qu.: 0.674681 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.074300 ## x_1 x_2 x_3 xi ## Min. :0.000 Min. :-4.4294 Min. :-3.97870 Min. :-1.444460 ## 1st Qu.:1.000 1st Qu.:-0.6108 1st Qu.:-0.03238 1st Qu.:-0.286633 ## Median :1.000 Median : 0.7797 Median : 1.18780 Median : 0.000000 ## Mean :0.866 Mean : 0.2713 Mean : 0.44050 Mean : 0.009578 ## 3rd Qu.:1.000 3rd Qu.: 1.4766 3rd Qu.: 1.64244 3rd Qu.: 0.317352 ## Max. :1.000 Max. : 3.0236 Max. : 1.88767 Max. : 1.905138 ## c p u q ## Min. :0.0000 Min. :0.000 Min. :-284.515 Min. :0.0000000 ## 1st Qu.:0.9425 1st Qu.:1.562 1st Qu.: -3.157 1st Qu.:0.0009063 ## Median :0.9886 Median :1.902 Median : 0.000 Median :0.0225375 ## Mean :0.8670 Mean :1.871 Mean : -1.927 Mean :0.1340483 ## 3rd Qu.:1.0322 3rd Qu.:2.394 3rd Qu.: 1.623 3rd Qu.:0.1203147 ## Max. :1.1996 Max. :8.211 Max. : 19.907 Max. :1.0000000 Next, write a function compute_share_smooth(X, M, V, beta, sigma, mu, omega) that calls compute_choice_smooth and then returns the share based on above \\(\\tilde{q}_{ijt}\\). If we use these functions with the Monte Carlo shocks, it gives us the predicted share of the products. df_share_smooth &lt;- compute_share_smooth(X, M, V, beta, sigma, mu, omega) summary(df_share_smooth) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.000 Min. :0.000 Min. :-4.4294 ## 1st Qu.: 26.00 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:-0.6108 ## Median : 51.50 Median : 5.000 Median :1.000 Median : 0.7797 ## Mean : 51.26 Mean : 4.807 Mean :0.866 Mean : 0.2713 ## 3rd Qu.: 77.00 3rd Qu.: 8.000 3rd Qu.:1.000 3rd Qu.: 1.4766 ## Max. :100.00 Max. :10.000 Max. :1.000 Max. : 3.0236 ## x_3 xi c p ## Min. :-3.97870 Min. :-1.444460 Min. :0.0000 Min. :0.000 ## 1st Qu.:-0.03238 1st Qu.:-0.286542 1st Qu.:0.9427 1st Qu.:1.564 ## Median : 1.18780 Median : 0.000000 Median :0.9886 Median :1.902 ## Mean : 0.44050 Mean : 0.009578 Mean :0.8670 Mean :1.871 ## 3rd Qu.: 1.62290 3rd Qu.: 0.316516 3rd Qu.:1.0322 3rd Qu.:2.392 ## Max. : 1.88767 Max. : 1.905138 Max. :1.1996 Max. :8.211 ## q s y ## Min. : 5.912 Min. :0.01182 Min. :-2.9837 ## 1st Qu.: 17.628 1st Qu.:0.03526 1st Qu.:-1.9109 ## Median : 28.025 Median :0.05605 Median :-1.5011 ## Mean : 67.024 Mean :0.13405 Mean :-1.2219 ## 3rd Qu.:100.446 3rd Qu.:0.20089 3rd Qu.:-0.5556 ## Max. :337.118 Max. :0.67424 Max. : 1.1167 Use this df_share_smooth as the data to estimate the parameters in the following section. 10.2 Estimate the parameters First draw Monte Carlo consumer-level heterogeneity V_mcmc and Monte Carlo preference shocks e_mcmc. The number of simulations is L. This does not have to be the same with the actual number of consumers N. V_mcmc ## # A tibble: 50,000 x 6 ## i t v_x_1 v_x_2 v_x_3 v_p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.865 -0.142 -0.667 1.17 ## 2 2 1 0.316 1.29 -1.56 -0.691 ## 3 3 1 0.673 1.35 -0.203 0.388 ## 4 4 1 0.295 0.613 1.31 0.698 ## 5 5 1 0.214 -0.0878 -0.343 -0.0642 ## 6 6 1 -1.06 -0.240 0.373 -0.631 ## 7 7 1 -0.556 1.05 -1.21 -2.22 ## 8 8 1 0.376 -2.47 1.77 -0.333 ## 9 9 1 -0.872 1.36 0.508 -0.834 ## 10 10 1 -0.895 -1.26 -3.04 0.821 ## # … with 49,990 more rows head(e_mcmc) ## [1] 1.4664583 0.9890441 1.2502808 0.7103677 0.7433964 1.9116964 Vectorize the parameters to a vector theta because optim requires the maximiand to be a vector. # set parameters theta &lt;- c(beta, sigma, mu, omega) theta ## [1] 4.0000000 0.1836433 -0.8356286 1.5952808 0.3295078 0.8204684 ## [7] 0.5000000 1.0000000 Estimate the parameters assuming there is no product-specific unobserved fixed effects \\(\\xi_{jt}\\), i.e., using the functions in assignment 3. To do so, first modify M to M_no in which xi is replaced with 0 and estimate the model with M_no. Otherwise, your function will compute the share with the true xi. M_no &lt;- M %&gt;% dplyr::mutate(xi = 0) ## $par ## [1] 3.1292612 0.1834495 -0.9357865 1.5678279 0.3768438 1.1698887 ## [7] -0.1108147 1.9156776 ## ## $value ## [1] 0.0004291768 ## ## $counts ## function gradient ## 297 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimates ## 1 4.0000000 3.1292612 ## 2 0.1836433 0.1834495 ## 3 -0.8356286 -0.9357865 ## 4 1.5952808 1.5678279 ## 5 0.3295078 0.3768438 ## 6 0.8204684 1.1698887 ## 7 0.5000000 -0.1108147 ## 8 1.0000000 1.9156776 Next, we estimate the model allowing for the product-market-specific unobserved fixed effect \\(\\xi_{jt}\\) using the BLP algorithm. To do so, we slightly modify the compute_indirect_utility, compute_choice_smooth, and compute_share_smooth functions so that they receive \\(\\delta_{jt}\\) to compute the indirect utilities, choices, and shares. Be careful that the treatment of \\(\\alpha_i\\) is slightly different from the lecture note, because we assumed that \\(\\alpha_i\\)s are log-normal random variables. Compute and print out \\(\\delta_{jt}\\) at the true parameters, i.e.: \\[ \\delta_{jt} = \\beta_0&#39; x_j + \\alpha_0&#39; p_{jt} + \\xi_{jt}. \\] delta ## # A tibble: 746 x 3 ## t j delta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 6 -2.40 ## 3 2 0 0 ## 4 2 1 -1.15 ## 5 2 10 -1.22 ## 6 3 0 0 ## 7 3 1 -1.65 ## 8 3 2 -4.03 ## 9 3 3 -2.69 ## 10 3 4 -3.78 ## # … with 736 more rows Write a function compute_indirect_utility_delta(df, delta, sigma, mu, omega) that returns a vector whose element is the mean indirect utility of a product for a consumer in a market. The output should have the same length with \\(e\\). Print out the output with \\(\\delta_{jt}\\) evaluated at the true parameters. Check if the output is close to the true indirect utilities. # compute indirect utility from delta u_delta &lt;- compute_indirect_utility_delta(df, delta, sigma, mu, omega) head(u_delta) ## u ## [1,] 0.0000000 ## [2,] 3.3750542 ## [3,] 0.0000000 ## [4,] -3.3983588 ## [5,] 0.0000000 ## [6,] 0.8235142 summary(u - u_delta) ## u ## Min. :-5.684e-14 ## 1st Qu.:-4.441e-16 ## Median : 0.000e+00 ## Mean :-1.279e-17 ## 3rd Qu.: 3.331e-16 ## Max. : 5.684e-14 Write a function compute_choice_smooth_delta(X, M, V, delta, sigma, mu, omega) that first construct df from X, M, V, second call compute_indirect_utility_delta to obtain the vector of mean indirect utilities u, third compute the (smooth) choice vector q based on the vector of mean indirect utilities, and finally return the data frame to which u and q are added as columns. Print out the output with \\(\\delta_{jt}\\) evaluated at the true parameters. Check if the output is close to the true (smooth) choice vector. # compute choice df_choice_smooth_delta &lt;- compute_choice_smooth_delta(X, M, V, delta, sigma, mu, omega) df_choice_smooth_delta ## # A tibble: 373,000 x 15 ## t i j v_x_1 v_x_2 v_x_3 v_p x_1 x_2 x_3 xi ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 1.16 -1.40 0.0786 -1.15 0 0 0 0 ## 2 1 1 6 1.16 -1.40 0.0786 -1.15 1 0.780 1.56 -0.0514 ## 3 1 2 0 -1.05 -0.149 1.08 0.623 0 0 0 0 ## 4 1 2 6 -1.05 -0.149 1.08 0.623 1 0.780 1.56 -0.0514 ## 5 1 3 0 -0.426 -4.21 0.625 -1.14 0 0 0 0 ## 6 1 3 6 -0.426 -4.21 0.625 -1.14 1 0.780 1.56 -0.0514 ## 7 1 4 0 -0.235 0.463 0.470 0.241 0 0 0 0 ## 8 1 4 6 -0.235 0.463 0.470 0.241 1 0.780 1.56 -0.0514 ## 9 1 5 0 1.19 -0.342 0.169 0.160 0 0 0 0 ## 10 1 5 6 1.19 -0.342 0.169 0.160 1 0.780 1.56 -0.0514 ## # … with 372,990 more rows, and 4 more variables: c &lt;dbl&gt;, p &lt;dbl&gt;, ## # u &lt;dbl&gt;, q &lt;dbl&gt; summary(df_choice_smooth_delta) ## t i j v_x_1 ## Min. : 1.00 Min. : 1.0 Min. : 0.000 Min. :-4.302781 ## 1st Qu.: 26.00 1st Qu.:125.8 1st Qu.: 2.000 1st Qu.:-0.685447 ## Median : 51.50 Median :250.5 Median : 5.000 Median :-0.000461 ## Mean : 51.26 Mean :250.5 Mean : 4.807 Mean :-0.005284 ## 3rd Qu.: 77.00 3rd Qu.:375.2 3rd Qu.: 8.000 3rd Qu.: 0.665219 ## Max. :100.00 Max. :500.0 Max. :10.000 Max. : 3.809895 ## v_x_2 v_x_3 v_p ## Min. :-4.542122 Min. :-3.957618 Min. :-4.218131 ## 1st Qu.:-0.678377 1st Qu.:-0.676638 1st Qu.:-0.672011 ## Median : 0.001435 Median : 0.006281 Median : 0.002166 ## Mean :-0.001076 Mean : 0.003433 Mean :-0.001402 ## 3rd Qu.: 0.671827 3rd Qu.: 0.679273 3rd Qu.: 0.674681 ## Max. : 4.313621 Max. : 4.244194 Max. : 4.074300 ## x_1 x_2 x_3 xi ## Min. :0.000 Min. :-4.4294 Min. :-3.97870 Min. :-1.444460 ## 1st Qu.:1.000 1st Qu.:-0.6108 1st Qu.:-0.03238 1st Qu.:-0.286633 ## Median :1.000 Median : 0.7797 Median : 1.18780 Median : 0.000000 ## Mean :0.866 Mean : 0.2713 Mean : 0.44050 Mean : 0.009578 ## 3rd Qu.:1.000 3rd Qu.: 1.4766 3rd Qu.: 1.64244 3rd Qu.: 0.317352 ## Max. :1.000 Max. : 3.0236 Max. : 1.88767 Max. : 1.905138 ## c p u q ## Min. :0.0000 Min. :0.000 Min. :-284.515 Min. :0.0000000 ## 1st Qu.:0.9425 1st Qu.:1.562 1st Qu.: -3.157 1st Qu.:0.0009063 ## Median :0.9886 Median :1.902 Median : 0.000 Median :0.0225375 ## Mean :0.8670 Mean :1.871 Mean : -1.927 Mean :0.1340483 ## 3rd Qu.:1.0322 3rd Qu.:2.394 3rd Qu.: 1.623 3rd Qu.:0.1203147 ## Max. :1.1996 Max. :8.211 Max. : 19.907 Max. :1.0000000 summary(df_choice_smooth$q - df_choice_smooth_delta$q) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.166e-15 -6.939e-18 0.000e+00 4.540e-20 1.084e-18 1.221e-15 Write a function compute_share_delta(X, M, V, delta, sigma, mu, omega) that first construct df from X, M, V, second call compute_choice_delta to obtain a data frame with u and q, third compute the share of each product at each market s and the log difference in the share from the outside option, \\(\\ln(s_{jt}/s_{0t})\\), denoted by y, and finally return the data frame that is summarized at the product-market level, dropped consumer-level variables, and added s and y. # compute share df_share_smooth_delta &lt;- compute_share_smooth_delta(X, M, V, delta, sigma, mu, omega) df_share_smooth_delta ## # A tibble: 746 x 11 ## t j x_1 x_2 x_3 xi c p q s ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 0 0 307. 0.614 ## 2 1 6 1 0.780 1.56 -0.0514 0.980 1.91 193. 0.386 ## 3 2 0 0 0 0 0 0 0 208. 0.415 ## 4 2 1 1 0.975 -0.0324 -0.197 0.988 1.90 158. 0.317 ## 5 2 10 1 -0.0899 -0.112 -0.354 1.05 1.82 134. 0.268 ## 6 3 0 0 0 0 0 0 0 113. 0.225 ## 7 3 1 1 0.975 -0.0324 0.182 1.04 2.22 25.6 0.0513 ## 8 3 2 1 1.48 1.89 0.384 1.01 2.61 14.4 0.0288 ## 9 3 3 1 1.15 1.64 -0.0562 1.02 2.01 18.3 0.0366 ## 10 3 4 1 -0.611 1.19 0.441 1.01 2.62 9.46 0.0189 ## # … with 736 more rows, and 1 more variable: y &lt;dbl&gt; summary(df_share_smooth_delta) ## t j x_1 x_2 ## Min. : 1.00 Min. : 0.000 Min. :0.000 Min. :-4.4294 ## 1st Qu.: 26.00 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:-0.6108 ## Median : 51.50 Median : 5.000 Median :1.000 Median : 0.7797 ## Mean : 51.26 Mean : 4.807 Mean :0.866 Mean : 0.2713 ## 3rd Qu.: 77.00 3rd Qu.: 8.000 3rd Qu.:1.000 3rd Qu.: 1.4766 ## Max. :100.00 Max. :10.000 Max. :1.000 Max. : 3.0236 ## x_3 xi c p ## Min. :-3.97870 Min. :-1.444460 Min. :0.0000 Min. :0.000 ## 1st Qu.:-0.03238 1st Qu.:-0.286542 1st Qu.:0.9427 1st Qu.:1.564 ## Median : 1.18780 Median : 0.000000 Median :0.9886 Median :1.902 ## Mean : 0.44050 Mean : 0.009578 Mean :0.8670 Mean :1.871 ## 3rd Qu.: 1.62290 3rd Qu.: 0.316516 3rd Qu.:1.0322 3rd Qu.:2.392 ## Max. : 1.88767 Max. : 1.905138 Max. :1.1996 Max. :8.211 ## q s y ## Min. : 5.912 Min. :0.01182 Min. :-2.9837 ## 1st Qu.: 17.628 1st Qu.:0.03526 1st Qu.:-1.9109 ## Median : 28.025 Median :0.05605 Median :-1.5011 ## Mean : 67.024 Mean :0.13405 Mean :-1.2219 ## 3rd Qu.:100.446 3rd Qu.:0.20089 3rd Qu.:-0.5556 ## Max. :337.118 Max. :0.67424 Max. : 1.1167 summary(df_share_smooth$s - df_share_smooth_delta$s) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.388e-16 -1.388e-17 0.000e+00 7.046e-19 6.939e-18 2.220e-16 Write a function solve_delta(df_share_smooth, X, M, V, delta, sigma, mu, omega) that finds \\(\\delta_{jt}\\) that equates the actua share and the predicted share based on compute_share_smooth_delta by the fixed-point algorithm with an operator: \\[ T(\\delta_{jt}^{(r)}) = \\delta_{jt}^{(r)} + \\kappa \\cdot \\log\\left(\\frac{s_{jt}}{\\sigma_{jt}[\\delta^{(r)}]}\\right), \\] where \\(s_{jt}\\) is the actual share of product \\(j\\) in market \\(t\\) and \\(\\sigma_{jt}[\\delta^{(r)}]\\) is the predicted share of product \\(j\\) in market \\(t\\) given \\(\\delta^{(r)}\\). Multiplying \\(\\kappa\\) is for the numerical stability. I set the value at \\(\\kappa = 1\\). Adjust it if the algorithm did not work. Set the stopping criterion at \\(\\max_{jt}|\\delta_{jt}^{(r + 1)} - \\delta_{jt}^{(r)}| &lt; \\lambda\\). Set \\(\\lambda\\) at \\(10^{-3}\\). Make sure that \\(\\delta_{i0t}\\) is always set at zero while the iteration. Start the algorithm with the true \\(\\delta_{jt}\\) and check if the algorithm returns (almost) the same \\(\\delta_{jt}\\) when the actual and predicted smooth share are equated. kappa &lt;- 1 lambda &lt;- 1e-3 delta_new &lt;- solve_delta(df_share_smooth, X, M, V, delta, sigma, mu, omega, kappa, lambda) head(delta_new) ## # A tibble: 6 x 3 ## t j delta ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 6 -2.40 ## 3 2 0 0 ## 4 2 1 -1.15 ## 5 2 10 -1.22 ## 6 3 0 0 summary(delta_new$delta - delta$delta) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.553e-15 -4.441e-16 0.000e+00 -5.745e-17 0.000e+00 1.776e-15 Check how long it takes to compute the limit \\(\\delta\\) under the Monte Carlo shocks starting from the true \\(\\delta\\) to match with df_share_smooth. This is approximately the time to evaluate the objective function. delta_new &lt;- solve_delta(df_share_smooth, X, M, V_mcmc, delta, sigma, mu, omega, kappa, lambda) save(delta_new, file = &quot;data/A4_delta_new.RData&quot;) delta_new &lt;- get(load(file = &quot;data/A4_delta_new.RData&quot;)) summary(delta_new$delta - delta$delta) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.30836 -0.30499 0.00000 -0.06422 0.16800 1.36169 We use the marginal cost \\(c_{jt}\\) as the excluded instrumental variable for \\(p_{jt}\\). Let \\(\\Psi\\) be the weighing matrix for the GMM estimator. For now, let it be the identity matrix. Write a function compute_theta_linear(df_share_smooth, delta, mu, omega, Psi) that returns the optimal linear parameters associated with the data and \\(\\delta\\). Notice that we only obtain \\(\\beta_0\\) in this way because \\(\\alpha_0\\) is directly computed from the non-linear parameters by \\(-\\exp(\\mu + \\omega^2/2)\\). The first order condition for \\(\\beta_0\\) is: \\[\\begin{equation} \\beta_0 = (X&#39;W \\Phi^{-1} W&#39;X)^{-1} X&#39; W \\Phi^{-1} W&#39; [\\delta - \\alpha_0 p], \\end{equation}\\] where \\[\\begin{equation} X = \\begin{pmatrix} x_{11}&#39;\\\\ \\vdots \\\\ x_{J_1 1}&#39;\\\\ \\vdots \\\\ x_{1T}&#39; \\\\ \\vdots \\\\ x_{J_T T} \\end{pmatrix} \\end{equation}\\] \\[\\begin{equation} W = \\begin{pmatrix} x_{11}&#39; &amp; c_{11}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_1 1}&#39; &amp; c_{J_1 1}\\\\ \\vdots &amp; \\vdots \\\\ x_{1T}&#39; &amp; c_{1T}\\\\ \\vdots &amp; \\vdots \\\\ x_{J_T T} &amp; c_{J_T T} \\end{pmatrix}, \\end{equation}\\] \\[\\begin{equation} \\delta = \\begin{pmatrix} \\delta_11\\\\ \\vdots\\\\ \\delta_{J_1 1}\\\\ \\vdots\\\\ \\delta_1T\\\\ \\vdots\\\\ \\delta_{J_T T}\\\\ \\end{pmatrix} \\end{equation}\\] , where \\(\\alpha_0 = - \\exp(\\mu + \\omega^2/2)\\). Notice that \\(X\\) and \\(W\\) does not include rows for the outwide option. Psi &lt;- diag(length(beta) + 1) theta_linear &lt;- compute_theta_linear(df_share_smooth, delta, mu, omega, Psi) cbind(theta_linear, beta) ## delta beta ## x_1 3.9935855 4.0000000 ## x_2 0.1552469 0.1836433 ## x_3 -0.7838712 -0.8356286 Write a function solve_xi(df_share_smooth, delta, beta, mu, omega) that computes the values of \\(\\xi\\) that are implied from the data, \\(\\delta\\), and the linear parameters. Check that the (almost) true values are returned when true \\(\\delta\\) and the true linear parmaeters are passed to the function. Notice that the returend \\(\\xi\\) should not include rows for the outside option. xi_new &lt;- solve_xi(df_share_smooth, delta, beta, mu, omega) head(xi_new) ## xi ## [1,] -0.05139386 ## [2,] -0.19714498 ## [3,] -0.35374758 ## [4,] 0.18229098 ## [5,] 0.38426646 ## [6,] -0.05617311 xi_true &lt;- df_share_smooth %&gt;% dplyr::filter(j != 0) %&gt;% dplyr::select(xi) summary(xi_true - xi_new) ## xi ## Min. :-4.441e-16 ## 1st Qu.:-8.327e-17 ## Median : 0.000e+00 ## Mean :-4.134e-18 ## 3rd Qu.: 6.765e-17 ## Max. : 6.661e-16 Write a function GMM_objective_A4(theta_nonlinear, delta, df_share_smooth, Psi, X, M, V_mcmc, kappa, lambda) that returns the value of the GMM objective function as a function of non-linear parameters mu, omega, and sigma: \\[ \\min_{\\theta} \\xi(\\theta)&#39; W \\Phi^{-1} W&#39; \\xi(\\theta), \\] where \\(\\xi(\\theta)\\) is the values of \\(\\xi\\) that solves: \\[ s = \\sigma(p, x, \\xi), \\] given parameters \\(\\theta\\). Note that the row of \\(\\xi(\\theta)\\) and \\(W\\) do not include the rows for the outside options. # non-linear parmaeters theta_nonlinear &lt;- c(mu, omega, sigma) # compute GMM objective function objective &lt;- GMM_objective_A4(theta_nonlinear, delta, df_share_smooth, Psi, X, M, V_mcmc, kappa, lambda) save(objective, file = &quot;data/A4_objective.RData&quot;) objective &lt;- get(load(file = &quot;data/A4_objective.RData&quot;)) objective ## xi ## xi 0.1368324 Draw a graph of the objective function that varies each non-linear parameter from 0, 0.2, \\(\\cdots\\), 2.0 of the true value. Try with the actual shocks V. ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] Find non-linear parameters that minimize the GMM objective function. Because standard deviations of the same absolute value with positive and negative values have almost the same implication for the data, you can take the absolute value if the estimates of the standard deviations happened to be negative (Another way is to set the non-negativity constraints on the standard deviations). ## $par ## [1] 0.3635051 0.7502512 1.5852184 0.4223104 0.8757237 ## ## $value ## [1] 2.064906e-18 ## ## $counts ## function gradient ## 54 11 ## ## $convergence ## [1] 0 ## ## $message ## NULL ## true estimate ## [1,] 0.5000000 0.3635051 ## [2,] 1.0000000 0.7502512 ## [3,] 1.5952808 1.5852184 ## [4,] 0.3295078 0.4223104 ## [5,] 0.8204684 0.8757237 "],
["rcpp.html", "Chapter 11 Integrating with C++ using Rcpp and RcppEigen 11.1 Prerequisite 11.2 Workflow 11.3 Passing R Objects as Rcpp Objects 11.4 Passing R Objects as Eigen Objects 11.5 Passing R Objects in Other Objects in C/C++ 11.6 Manipulating Objects in a C++ Function", " Chapter 11 Integrating with C++ using Rcpp and RcppEigen 11.1 Prerequisite In this chapter, we learn how to integrate C++ using Rcpp and RcppEigen. RcppEigen is a package to use a linear algebra library Eigen with R. The original Eigen library and its documentation is found in their website. Instead of RcppEigen, you may want to use RcppArmadillo. Armadillo is another libear algebra library in C++. We presume that: C/C++ environment is installed to the computer. For OSX, you can install Apple Developer Tools. For Windows, you can try Rtools. Rcpp and RcppEigen are installed. The project is created in RStudio from File &gt; New Project &gt; New Directory &gt; R Package using RcppEigen. In the following, I use the project name EmpiricalIO but the name can be as you like. We presume the you have the following folder and file structure from the root directory: main. main.R: all executable statements are written in this file. R. functions.R: all function definitions in R are written in this file. src. functions.cpp: all function definitions in C++ are written in this file. It includes: &quot;src/functions.cpp&quot; ------------------- #include &lt;Rcpp.h&gt; #include &lt;RcppEigen.h&gt; Inside functions.cpp, avoid using name spaces. Makevars: compilation flags for osx/Linux should be written here. Makevars.win: compilation flags for Windows should be written here. inst. include: header files for external libraries in C/C++ are stored here. Makevars/Makevars.win should be: Makevars -------- PKG_CPPFLAGS = -w -std=c++11 -I../inst/include/ -O3 Makevars.win ------------ PKG_CPPFLAGS = -w -std=c++11 -I../inst/include/ -O3 -w is for supprssing some warnings in Eigen. If you want to keep warnings shown, this can be removed. -std=c++11 is for using the latest functionalities of C++ (optional). -I../inst/include/ is for setting the header path to ../inst/include/ (optional). 11.2 Workflow To minimize the likelihood of bugs and the time to edit and debug the code, I recommend you to follw the following workflow. This workflow is based on my experience. If you find better workflow, you can overwrite by your own way. Write a procedure in main/main.R. # main/main.R x &lt;- 1:4 y1 &lt;- x^2 Rewrite the procedure to a function in main/main.R. # main/main.R # compute coefficient-wise square compute_square &lt;- function(x) { y &lt;- x^2 return(y) } Execute the function in main/main.R and check that the output is the same with the output of the original # main/main.R y2 &lt;- compute_square(x) max(abs(y1 - y2)) ## [1] 0 Cut and paste the function to R/functions.R. Build the package and load the function from the library. Check if the function can be executed as in step 3. Copy the function to src/functions.cpp and comment them out. // src/functions.cpp // # compute coefficient-wise square // compute_square &lt;- // function(x) { // y &lt;- x^2 // return(y) // } Write function in C++ by translating the copied and pasted R code. The function name should be consistent with the function name in R. I often put _rcpp to the end of the name. Put // [[Rcpp::export]] above the name of the function if you want to call this function directly from R. Otherwise, the wrapper function to call from R is not created. The class of the inputs and output should be consistent with Rcpp objects. This will be explained later. // src/functions.cpp // # compute coefficient-wise square // compute_square &lt;- // [[Rcpp::export]] Eigen::VectorXd compute_square_rcpp(Eigen::VectorXd x) { // function(x) { Eigen::VectorXd y = x.array().square(); // y &lt;- x^2 // return(y) // } return(y); } Check if clean and rebuild work. If there is a compilation error happens, debug until the compilation succeeds. Sometimes, deleting R/RcppExports.R and R/RcppExports.cpp may be need when re-compile the functions. Clean up the code by eliminating the copied and pasted R code. // src/functions.cpp // compute coefficient-wise square // [[Rcpp::export]] Eigen::VectorXd compute_square_rcpp(Eigen::VectorXd x) { Eigen::VectorXd y = x.array().square(); return(y); } Now by calling the library, you should be able to use the function written in C++ in R. Check if it returns a valid value and if the output is the same as the output of the R function. # main/main.R y3 &lt;- compute_square_rcpp(x) max(abs(y2 - y3)) ## [1] 0 If there is a run-time bug in the C++ function, you may have to use some debugger for C++ to debug the function. In osx, you can debug the C++ function called from R function in the following way. Open the terminal and run R with the debugger lldb by typing the following command in the terminal: # terminal R -d lldb Run main/main.R by typing the following command in the terminal: # terminal run -f main/main.R This should execute the R source code. After library(EmpiricalIO) is read, stop the process by Ctrl + C before the function in question is called. If there is no time gap between them, set Sys.sleep() in R to buy some time. As you stop the process, set the break point at the function in question by typing in the terminal as: # terminal br s -n compute_square_rcpp and then continue the process by typing c in the terminal. For the rest, refer to the documentation of lldb. There is no such an easy way in Windows. You will have to establish an environment in which you can run a cpp file with executable statemtns and call the debugging functions from the file. Then, you can use some debuggers such as gdb to debug the functions inside the cpp file. If you need to modify the function, first rewrite the R function and then follow the same step to rewrite the C++ function. Never start the debugging from C++ functions. 11.3 Passing R Objects as Rcpp Objects R class corresponds to the following Rcpp class: R Rcpp logical Logical integer Integer numeric Numeric complex Complex character String Date Date POSIXct Datetime R data structure corresponds to the following Rcpp data structure: R Rcpp vector Vector matrix Matrix data.frame DataFrame list List For example, a numeric vector in R is passed to Rcpp::NumericVector in Rcpp, an integer matrix is to Rcpp::IntegerMatrix, and so on. # main/main.R # numeric vector x &lt;- rnorm(5) x ## [1] -0.9159760 0.2750165 -1.4173615 1.5778831 -2.1781476 # numeric matrix Y &lt;- matrix(rnorm(2*5), nrow = 2) Y ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.0509051 0.9240679 0.01461874 -0.1573930 -0.1157289 ## [2,] -0.4767702 0.4508252 -0.25871885 -0.4637887 0.8536339 Let’s write a C++ function that just receives a numeric vector and returns the numeric vector, and receives a numeric matrix and returns the numeric matrix. // src/functions.cpp // [[Rcpp::export]] Rcpp::NumericVector pass_numeric_vector_to_rcpp(Rcpp::NumericVector x) { return(x); } // [[Rcpp::export]] Rcpp::NumericMatrix pass_numeric_matrix_to_rcpp(Rcpp::NumericMatrix Y) { return(Y); } Check if you can pass R objects and get the right result: x_rcpp &lt;- pass_numeric_vector_to_rcpp(x) max(abs(x - x_rcpp)) ## [1] 0 Y_rcpp &lt;- pass_numeric_matrix_to_rcpp(Y) max(abs(Y - Y_rcpp)) ## [1] 0 Exercise: Write functions pass_integer_vector_to_rcpp, pass_integer_matrix_to_rcpp, pass_list_to_rcpp, pass_data_frame_to_rcpp that receive an integer vector, list, and data frame and just return themselves. # main/main.r # integer vector z &lt;- 1:5 z ## [1] 1 2 3 4 5 # integer matrix W &lt;- matrix(rep(1, 4), nrow = 2) W ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 # list L &lt;- list(x = x, y = y, z = z) L ## $x ## [1] -0.9159760 0.2750165 -1.4173615 1.5778831 -2.1781476 ## ## $y ## [1] 1 ## ## $z ## [1] 1 2 3 4 5 # data frame D &lt;- data.frame(x1 = rnorm(5), x2 = rnorm(5)) D ## x1 x2 ## 1 -0.86280066 -1.03184291 ## 2 -1.15758053 0.77688232 ## 3 -0.89268670 0.21788910 ## 4 0.04043654 1.33717072 ## 5 2.25861844 -0.07430342 z_rcpp &lt;- pass_integer_vector_to_rcpp(z) max(abs(z - z_rcpp)) ## [1] 0 W_rcpp &lt;- pass_integer_matrix_to_rcpp(W) max(abs(W - W_rcpp)) ## [1] 0 L_rcpp &lt;- pass_list_to_rcpp(L) max(abs(unlist(L) - unlist(L_rcpp))) ## [1] 0 D_rcpp &lt;- pass_data_frame_to_rcpp(D) max(abs(D - D_rcpp)) ## [1] 0 11.4 Passing R Objects as Eigen Objects R data structure corresponds to the following Eigen data structure: R Eigen vector Eigen::VectorX matrix Eigen::MatrixX If you pass an integer vector and matrix, the corresponding Eigen objects are Eigen::VectorXi and Eigen::MatrixXi. If you pass an numeric vector and matrix, the corresponding Eigen objects are Eigen::VectorXd and Eigen::MatrixXd. The class of the output can be Eigen class. If you return Eigen::VectorXd, Eigen::MatrixXd, then they are automatically converted to the corresponding R objects. Check if you can pass x, Y, and z as follows: // src/functions.cpp // [[Rcpp::export]] Eigen::VectorXd pass_numeric_vector_to_eigen(Eigen::VectorXd x) { return(x); } // [[Rcpp::export]] Eigen::MatrixXd pass_numeric_matrix_to_eigen(Eigen::MatrixXd Y) { return(Y); } # main/main.r x_eigen &lt;- pass_numeric_vector_to_eigen(x) max(abs(x - x_eigen)) ## [1] 0 Y_eigen &lt;- pass_numeric_matrix_to_eigen(Y) max(abs(Y - Y_eigen)) ## [1] 0 Exercise: Write functions pass_integer_vector_to_rcpp and pass_integer_matrix_to_rcpp that receive an integer vector and integer matrix and just return themselves. # main/main.r z_eigen &lt;- pass_integer_vector_to_eigen(z) max(abs(z - z_eigen)) ## [1] 0 W_eigen &lt;- pass_integer_matrix_to_eigen(W) max(abs(W - W_eigen)) ## [1] 0 If you pass a vector and matrix by Eigen::VectorX and Eigen::MatrixX, the objects are copied to the new objects. This means that the new memory is allocated. If you are going to modify the passed object inside the C++ function, the objects have to be copied. Otherwise, you can just map the objects in the following way so that the new memory is not allocated, whereas you cannot modify the objects in the C++ function. // src/functions.cpp // [[Rcpp::export]] Eigen::VectorXd map_numeric_vector_to_eigen(Eigen::Map&lt;Eigen::VectorXd&gt; x) { return(x); } // [[Rcpp::export]] Eigen::MatrixXd map_numeric_matrix_to_eigen(Eigen::Map&lt;Eigen::MatrixXd&gt; Y) { return(Y); } # main/main.r x_eigen_map &lt;- map_numeric_vector_to_eigen(x) max(abs(x - x_eigen_map)) ## [1] 0 Y_eigen_map &lt;- map_numeric_matrix_to_eigen(Y) max(abs(Y - Y_eigen_map)) ## [1] 0 I recommend to directly pass R vectors and matrices to Eigen::VectorX and Eigen::MatrixX rather than to Vector and Matrix in Rcpp, because Eigen::VectorX and Eigen::MatrixX have richer methods for linear algebra. R list cannot be directly translated to Eigen objects, but the list of vectors and matrices, and the list of list of R objects can be passed to Eigen in the following way. // src/functions.cpp // [[Rcpp::export]] Rcpp::List pass_list_to_eigen(Rcpp::List L) { // double vector Eigen::VectorXd x(Rcpp::as&lt;Eigen::VectorXd&gt;(L.at(0))); // double matrix Eigen::MatrixXd Y(Rcpp::as&lt;Eigen::MatrixXd&gt;(L.at(1))); // integer vector Eigen::VectorXi z(Rcpp::as&lt;Eigen::VectorXi&gt;(L.at(2))); // integer matrix Eigen::MatrixXi W(Rcpp::as&lt;Eigen::MatrixXi&gt;(L.at(3))); // return Rcpp::List output = Rcpp::List::create(x, Y, z, W); return(output); } # main/main.r list_1 &lt;- list() list_1[[1]] &lt;- x list_1[[2]] &lt;- Y list_1[[3]] &lt;- z list_1[[4]] &lt;- W list_1 ## [[1]] ## [1] -0.9159760 0.2750165 -1.4173615 1.5778831 -2.1781476 ## ## [[2]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.0509051 0.9240679 0.01461874 -0.1573930 -0.1157289 ## [2,] -0.4767702 0.4508252 -0.25871885 -0.4637887 0.8536339 ## ## [[3]] ## [1] 1 2 3 4 5 ## ## [[4]] ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 list_1_eigen &lt;- pass_list_to_eigen(list_1) max(abs(unlist(list_1) - unlist(list_1_eigen))) ## [1] 0 You can also pass a list with named arguments and return a named list as follows: // src/funtions.cpp // [[Rcpp::export]] Rcpp::List pass_named_list_to_eigen(Rcpp::List L) { // double vector Eigen::VectorXd x(Rcpp::as&lt;Eigen::VectorXd&gt;(L[&quot;x&quot;])); // double matrix Eigen::MatrixXd Y(Rcpp::as&lt;Eigen::MatrixXd&gt;(L[&quot;Y&quot;])); // integer vector Eigen::VectorXi z(Rcpp::as&lt;Eigen::VectorXi&gt;(L[&quot;z&quot;])); // integer matrix Eigen::MatrixXi W(Rcpp::as&lt;Eigen::MatrixXi&gt;(L[&quot;W&quot;])); // return Rcpp::List output = Rcpp::List::create( Rcpp::Named(&quot;x&quot;) = x, Rcpp::Named(&quot;Y&quot;) = Y, Rcpp::Named(&quot;z&quot;) = z, Rcpp::Named(&quot;W&quot;) = W); return(output); } # main/main.r list_2 &lt;- list() list_2$x &lt;- x list_2$Y &lt;- Y list_2$z &lt;- z list_2$W &lt;- W list_2 ## $x ## [1] -0.9159760 0.2750165 -1.4173615 1.5778831 -2.1781476 ## ## $Y ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.0509051 0.9240679 0.01461874 -0.1573930 -0.1157289 ## [2,] -0.4767702 0.4508252 -0.25871885 -0.4637887 0.8536339 ## ## $z ## [1] 1 2 3 4 5 ## ## $W ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 list_2_eigen &lt;- pass_named_list_to_eigen(list_2) max(abs(unlist(list_2) - unlist(list_2_eigen))) ## [1] 0 You can also access to the column of Rcpp::DataFrame in the similar way. // src/functions.cpp // [[Rcpp::export]] Eigen::VectorXd extract_column_from_data_frame(Rcpp::DataFrame D) { // pass column x1 of D to Eigen::VectorXd named x1 Eigen::VectorXd x1(Rcpp::as&lt;Eigen::VectorXd&gt;(D[&quot;x1&quot;])); return(x1); } # main/main.R x1 &lt;- extract_column_from_data_frame(D) max(abs(D$x1 - x1)) ## [1] 0 This allows us to pass whatever objects in R to a C++ function. If you are planning to translate R functions to C/C++, from the beginning, you should write the R functions in the way inputs and outpus can be passed to C/C++ as above. 11.5 Passing R Objects in Other Objects in C/C++ integer and numeric scalars in R can be simply passed to int and double in C/C++. Vectors in R can be passed to std::vector&lt;int&gt; or std::vector&lt;double&gt; objects. This may be helpful if you want to use the methods for std::vector. 11.6 Manipulating Objects in a C++ Function As mentioned above, the best practice is to pass vectors and matrices to Eigen::VectorXd/Eigen::VectorXi and Eigen::MatrixXd/Eigen::MatrixXi rather than Rcpp::NumericVector/Rcpp::IntegerVector and Rcpp::NumericMatrix/Rcpp::IntegerMatrix. The other objects will be passed as Rcpp::DataFrame or Rcpp::List and at the end converted to Eigen::VectorXd/Eigen::VectorXi and Eigen::MatrixXd/Eigen::MatrixXi usin Rcpp::as as explained above. The rest of manipulation will be done using the methods in Eigen. Here I introduce basic operations. For the detail, refer to the online document of Eigen. 11.6.1 Matrix and Vector Arithmetic Addition and subtraction: Binary operator \\(+\\) as in a + b. Binary operator \\(-\\) as in a - b. Unary operator \\(-\\) as in - a. Scalar multiplication and division: Binary operator \\(\\times\\) as in matrix * scalar. Binary operator \\(\\times\\) as in scalar * matrix. Binary operator \\(/\\) as in matrix / scalar. Transposition: Transposition as in matrix.transpose(). Matrix-matrix and matrix-vector multiplication: This part is different from R. Matrix multiplication is as in A * B. Coefficientwise multiplication is explained later but is as in A.array() * B.array(). Following the best practice, first write R function for these operations in R/functions.R and run in main/main.R: # R/functions.R # matrix and vector arithmetic matrix_vector_arithmetic &lt;- function(A, B, v, c) { # addition and subtraction X1 &lt;- A + B X2 &lt;- A - B X3 &lt;- -A # scalar multiplication and division X4 &lt;- A * c X5 &lt;- c * A X6 &lt;- A / c # transpose X7 &lt;- t(A) # matrix-matrix and matrix-vector multiplication X8 &lt;- A %*% t(B) X9 &lt;- A %*% v # return return(list(X1 = X1, X2 = X2, X3 = X3, X4 = X4, X5 = X5, X6 = X6, X7 = X7, X8 = X8, X9 = X9)) } # main/main.R set.seed(1) # addition subtraction A &lt;- matrix(rnorm(2*4), nrow = 2) B &lt;- matrix(rnorm(2*4), nrow = 2) c &lt;- 3 v &lt;- matrix(rnorm(4), nrow = 4) output &lt;- matrix_vector_arithmetic(A, B, v, c) Next, write these operations in C++ function: // src/functions.cpp // [[Rcpp::export]] Rcpp::List matrix_vector_arithmetic_rcpp( Eigen::MatrixXd A, Eigen::MatrixXd B, Eigen::VectorXd v, double c) { // addition and subtraction Eigen::MatrixXd X1 = A + B; Eigen::MatrixXd X2 = A - B; Eigen::MatrixXd X3 = - A; // scalar multiplication and division Eigen::MatrixXd X4 = A * c; Eigen::MatrixXd X5 = c * A; Eigen::MatrixXd X6 = A / c; // transpose Eigen::MatrixXd X7 = A.transpose(); // matrix-matrix and matrix-vector multiplication Eigen::MatrixXd X8 = A * B.transpose(); Eigen::VectorXd X9 = A * v; // return Rcpp::List output = Rcpp::List::create( Rcpp::Named(&quot;X1&quot;) = X1, Rcpp::Named(&quot;X2&quot;) = X2, Rcpp::Named(&quot;X3&quot;) = X3, Rcpp::Named(&quot;X4&quot;) = X4, Rcpp::Named(&quot;X5&quot;) = X5, Rcpp::Named(&quot;X6&quot;) = X6, Rcpp::Named(&quot;X7&quot;) = X7, Rcpp::Named(&quot;X8&quot;) = X8, Rcpp::Named(&quot;X9&quot;) = X9); return(output); } Then, we can check if this yields (almost) the same result with the R function: # main/main.R output_rcpp &lt;- matrix_vector_arithmetic_rcpp(A, B, v, c) max(abs(unlist(output) - unlist(output_rcpp))) ## [1] 5.551115e-17 The output looks like: # main/main.R output_rcpp ## $X1 ## [,1] [,2] [,3] [,4] ## [1,] -0.05067246 0.6761526 -0.2917328 1.6123600 ## [2,] -0.12174506 1.9851240 -3.0351683 0.6933911 ## ## $X2 ## [,1] [,2] [,3] [,4] ## [1,] -1.2022352 -2.347410 0.9507484 -0.6375019 ## [2,] 0.4890317 1.205438 1.3942315 0.7832583 ## ## $X3 ## [,1] [,2] [,3] [,4] ## [1,] 0.6264538 0.8356286 -0.3295078 -0.4874291 ## [2,] -0.1836433 -1.5952808 0.8204684 -0.7383247 ## ## $X4 ## [,1] [,2] [,3] [,4] ## [1,] -1.879361 -2.506886 0.9885233 1.462287 ## [2,] 0.550930 4.785842 -2.4614052 2.214974 ## ## $X5 ## [,1] [,2] [,3] [,4] ## [1,] -1.879361 -2.506886 0.9885233 1.462287 ## [2,] 0.550930 4.785842 -2.4614052 2.214974 ## ## $X6 ## [,1] [,2] [,3] [,4] ## [1,] -0.20881794 -0.2785429 0.1098359 0.1624764 ## [2,] 0.06121444 0.5317603 -0.2734895 0.2461082 ## ## $X7 ## [,1] [,2] ## [1,] -0.6264538 0.1836433 ## [2,] -0.8356286 1.5952808 ## [3,] 0.3295078 -0.8204684 ## [4,] 0.4874291 0.7383247 ## ## $X8 ## [,1] [,2] ## [1,] -1.280368 -0.8861152 ## [2,] 3.857726 2.3497425 ## ## $X9 ## [1] -0.2184706 1.2674165 11.6.2 The Array Class and Coefficient-wise Operations When you want to have coefficient-wise operations, you first use .array() method to convert the object to an array and then apply methods for arrays. The resulting array can be assigned to a matrix object implicitly or explicitly by using .matrix() method. Coefficient-wise multiplication: A * B in R is A.array() * B. array() in Eigen. Other coefficient-wise math functions: Absolute value: A.array().abs(). Exponential: A.array().exp(). Logarithm: A.array().log(). Power: A.array().power(r). For the other methods, refer to the online document. Write functions in R in R/functions.R and run in main/main.R: # R/functions.R coefficientwise_operation &lt;- function(A, B, r) { # coefficient-wise multiplication X1 &lt;- A * B # other coefficient-wise math functions X2 &lt;- abs(A) X3 &lt;- exp(A) X4 &lt;- log(abs(A)) X5 &lt;- A^r # return return(list( X1 = X1, X2 = X2, X3 = X3, X4 = X4, X5 = X5 )) } # main/main.R # coefficient-wise operations r &lt;- 2 output &lt;- coefficientwise_operation(A, B, r) Write C++ function in src/functions.cpp and call from main/main.R: // src/functions.cpp // [[Rcpp::export]] Rcpp::List coefficientwise_operation_rcpp( Eigen::MatrixXd A, Eigen::MatrixXd B, int r ) { // coefficient-wise multiplication Eigen::MatrixXd X1 = A.array() * B.array(); // other coefficient-wise math functions Eigen::MatrixXd X2 = A.array().abs(); Eigen::MatrixXd X3 = A.array().exp(); Eigen::MatrixXd X4 = A.array().abs().log(); Eigen::MatrixXd X5 = A.array().pow(r); // return Rcpp::List output = Rcpp::List::create( Rcpp::Named(&quot;X1&quot;) = X1, Rcpp::Named(&quot;X2&quot;) = X2, Rcpp::Named(&quot;X3&quot;) = X3, Rcpp::Named(&quot;X4&quot;) = X4, Rcpp::Named(&quot;X5&quot;) = X5 ); return(output); } output_rcpp &lt;- coefficientwise_operation_rcpp(A, B, r) max(abs(unlist(output) - unlist(output_rcpp))) ## [1] 2.220446e-16 output_rcpp ## $X1 ## [,1] [,2] [,3] [,4] ## [1,] -0.36070042 -1.2632876 -0.2047036 0.54832401 ## [2,] -0.05608254 0.6219094 1.8170912 -0.03317559 ## ## $X2 ## [,1] [,2] [,3] [,4] ## [1,] 0.6264538 0.8356286 0.3295078 0.4874291 ## [2,] 0.1836433 1.5952808 0.8204684 0.7383247 ## ## $X3 ## [,1] [,2] [,3] [,4] ## [1,] 0.5344838 0.4336018 1.3902836 1.628125 ## [2,] 1.2015872 4.9297132 0.4402254 2.092427 ## ## $X4 ## [,1] [,2] [,3] [,4] ## [1,] -0.4676802 -0.1795710 -1.1101553 -0.7186105 ## [2,] -1.6947599 0.4670498 -0.1978799 -0.3033716 ## ## $X5 ## [,1] [,2] [,3] [,4] ## [1,] 0.39244438 0.6982752 0.1085754 0.2375871 ## [2,] 0.03372487 2.5449208 0.6731684 0.5451234 11.6.3 Solving Linear Least Squares Systems It is often required to solve a linear least squares system \\(A \\cdot x = b\\). Solving using SVD decomposition: A.bdcSvd(Eigen::ComputeThinU | Eigen::ComputeThinV).solve(b) Solving using QR decomposition: A.colPivHouseholderQr().solve(b) Write R function in R/functions.R and run in main/main.R: # R/functions.R solve_least_squares &lt;- function(A, B) { x &lt;- solve(A, B) return(x) } # main/main.R set.seed(1) A &lt;- matrix(rnorm(4 * 4), nrow = 4) B &lt;- matrix(rnorm(4 * 4), nrow = 4) output &lt;- solve_least_squares(A, B) Write C++ function in src/functions.cpp and call in main/main.R: // src/functions.cpp // solve least squares using SVD decomposition // [[Rcpp::export]] Eigen::MatrixXd solve_least_squares_svd( Eigen::MatrixXd A, Eigen::MatrixXd B ) { Eigen::MatrixXd x = A.bdcSvd(Eigen::ComputeThinU | Eigen::ComputeThinV).solve(B); return(x); } // solve least squares using QR decomposition // [[Rcpp::export]] Eigen::MatrixXd solve_least_squares_qr( Eigen::MatrixXd A, Eigen::MatrixXd B ) { Eigen::MatrixXd x = A.colPivHouseholderQr().solve(B); return(x); } # main/main.R output_svd &lt;- solve_least_squares_svd(A, B) output_qr &lt;- solve_least_squares_qr(A, B) max(abs(output - output_svd)) ## [1] 8.881784e-16 max(abs(output - output_qr)) ## [1] 1.554312e-15 output_svd ## [,1] [,2] [,3] [,4] ## [1,] 0.65530863 -1.2441297 -1.0799835 0.5926519 ## [2,] -1.34801870 0.1453720 0.7313845 -2.2353988 ## [3,] 1.38750763 -0.3405502 -0.7649107 1.5987206 ## [4,] -0.06376266 -0.4632165 -0.2296862 0.4681169 output_qr ## [,1] [,2] [,3] [,4] ## [1,] 0.65530863 -1.2441297 -1.0799835 0.5926519 ## [2,] -1.34801870 0.1453720 0.7313845 -2.2353988 ## [3,] 1.38750763 -0.3405502 -0.7649107 1.5987206 ## [4,] -0.06376266 -0.4632165 -0.2296862 0.4681169 11.6.4 Accessing to Elements of a Matrix and Vector You can access to the size information and elements of a matrix and vector as follows: Sizes: A.rows() for row numbers and A.cols() for column numbers. Element: A(i, j) for (\\(i + 1\\), \\(j + 1\\))-th element. Rows and columns: A.row(i) for \\(i + 1\\)-th row and A.col(j) for \\(j + 1\\)-th column. Write R function in R/functions.R and run in main/main.R: # R/functions.R access &lt;- function(A, i, j) { I &lt;- nrow(A) J &lt;- ncol(A) a_ij &lt;- A[i, j] a_i &lt;- A[i, ] a_j &lt;- A[, j] return( list( I = I, J = J, a_ij = a_ij, a_i = a_i, a_j = a_j ) ) } # main/main.R output &lt;- access(A, 1, 2) Write C++ function in src/functions.cpp and call in main/main.R: // src/functions.cpp // [[Rcpp::export]] Rcpp::List access_rcpp( Eigen::MatrixXd A, int i, int j ) { int I = A.rows(); int J = A.cols(); double a_ij = A(i - 1, j - 1); Eigen::VectorXd a_i = A.row(i - 1); Eigen::VectorXd a_j = A.col(j - 1); Rcpp::List output = Rcpp::List::create( Rcpp::Named(&quot;I&quot;) = I, Rcpp::Named(&quot;J&quot;) = J, Rcpp::Named(&quot;a_ij&quot;) = a_ij, Rcpp::Named(&quot;a_i&quot;) = a_i, Rcpp::Named(&quot;a_j&quot;) = a_j ); return(output); } # main/main.R output_rcpp &lt;- access_rcpp(A, 1, 2) max(abs(unlist(output) - unlist(output_rcpp))) ## [1] 0 output_rcpp ## $I ## [1] 4 ## ## $J ## [1] 4 ## ## $a_ij ## [1] 0.3295078 ## ## $a_i ## [1] -0.6264538 0.3295078 0.5757814 -0.6212406 ## ## $a_j ## [1] 0.3295078 -0.8204684 0.4874291 0.7383247 "],
["references.html", "Chapter 12 References", " Chapter 12 References "]
]
