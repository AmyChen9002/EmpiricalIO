[
["dynamics.html", "Chapter 7 Dynamic Decision Model 7.1 Motivations 7.2 Single-Agent Model 7.3 Identification 7.4 Estimation by Nested Fixed-point Algorithm 7.5 Estimation by Conditional Choice Probability (CCP) Approach", " Chapter 7 Dynamic Decision Model 7.1 Motivations The model is dynamic if there is an endogenous state variable, a state variable that is affected by an action of a player in the past. There are many cases where the decision makers have to take into account the dynamic effects of their actions. Payoff linkages: Storable good: Next week’s demand for a detergent depends on how many bottles consumers purchase and consume this week. The latter depends on this week’s price and the price schedule of the detergent. The stock of the detergent consumers hold is an endogenous state variable. Learning by doing: The productivity of a firm next year can be higher if the firm produced more this year because the firm can learn from the experience. The cummulative production level is an endogenous state variable. Information linkages: Uncertainty about the quality: If a consumer is uncertain about the quality of a product but can learn from experiencing the product, next seaons’s demand for the product depends on how many consumers purchace and experience the product this season. The latter depends on this season’s price and the price schedule of the product. Strategic linkages: Tacit collusion: If a firm deviates from the collusive price, the price war will start. Then, the history of the prices is the endogenous state variable. 7.2 Single-Agent Model 7.2.1 Setting This model originates at Rust (1987), while the setting and the notation follows Pesendorfer &amp; Schmidt-Dengler (2008). We start from a simple set-up: Single agent. Infinite-horizon discrete time. Time is \\(t = 1, 2, \\cdots, \\infty\\). Finitely many choices. There are \\(K + 1\\) actions \\(A = \\{0, 1, \\cdots, K\\}\\). Finite state space. There are \\(L\\) states \\(S = \\{1, \\cdots, L\\}\\). Markovian state transition. 7.2.2 Timing of the Model At period \\(t\\): State \\(s_t \\in S\\) is publicly observed. Choice-specific profitability shocks \\(\\epsilon_t \\in \\mathbb{R}^{K + 1}\\) are realized according to \\(F(\\cdot|s_t)\\) and privately observed. Choice \\(a_t \\in A\\) is made. State evolves according to a transition probability: \\[\\begin{equation} g(a, s, s&#39;) \\equiv \\mathbb{P}\\{s_{t + 1} = s&#39;|s_t = s, a_t = a\\}, \\end{equation}\\] Thus, the transition law only depends on today’s state and action, but not on the past history. \\[\\begin{equation} G \\equiv \\begin{pmatrix} g(0, 1, 1) &amp; \\cdots &amp; g(0, 1, L)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(0, L, 1) &amp; \\cdots &amp; g(0, L, L)\\\\ &amp; \\vdots &amp; \\\\ g(K, 1, 1) &amp; \\cdots &amp; g(K, 1, L)\\\\ \\vdots &amp; &amp; \\vdots \\\\ g(K, L, 1) &amp; \\cdots &amp; g(K, L, L)\\\\ \\end{pmatrix}. \\end{equation}\\] 7.2.3 Period Payoff When the state is \\(s_t\\), action is \\(a_t\\), and the profitability shocks are \\(\\epsilon_t\\), the period payoff is: \\[\\begin{equation} \\pi(a_t, s_t) + \\sum_{k = 1}^K \\epsilon_{tk} 1\\{a_t = k\\}, \\end{equation}\\] \\(\\pi(a_t, s_t)\\) is the mean choice-specific period payoff. \\(\\epsilon_t\\) is assuemd to be i.i.d. across times and is drawn from \\(F\\). Let: \\[ \\epsilon_{t a_t} \\equiv \\sum_{k = 1}^K \\epsilon_{tk} 1\\{a_t = k\\}, \\] be the choice-specific profitability shock. Let \\(\\Pi\\) summarize the choice-specific period payoffs at each state: \\[\\begin{equation} \\Pi = \\begin{pmatrix} \\pi(0, 1)\\\\ \\vdots \\\\ \\pi(0, L)\\\\ \\vdots \\\\ \\pi(K, 1)\\\\ \\vdots \\\\ \\pi(K, L)\\\\ \\end{pmatrix}. \\end{equation}\\] The payoff is the discounted sum of future payoffs with discount factor \\(\\beta &lt; 1\\). \\(\\Pi\\) is one of the parameters of interest. 7.2.4 Markovian Framework The strategy is in general a mapping from the entire history to the action set. We restrict the set of possible strategies to Markovian strategies \\(a(\\epsilon_t, s_t)\\) that only depends on the latest realization of states \\(\\epsilon_t, s_t\\), i.e., the behavior does not depend on the past states, conditional on today’s states. The Markovian strategy is introduced by Maskin &amp; Tirole (1988). But their model is slightly different from the current mode. They considered an oligopoly model in which only one firm can move at one time and his/her action depends only on the rival’s latest move. The current single-agent model can be regarded as a version of Maskin &amp; Tirole (1988)’s model such that the rival is replaced with the nature. 7.2.5 Belief When a play makes a decision, s/he should have some belief about the future \\(\\epsilon_t, s_t\\), and \\(a_t\\). We usually assume the rational expectation: the play knows the equilibrium distribution of these future variables and use it as his/her belief. The distribution of \\(\\epsilon_t\\) and \\(s_t\\) is believed to follow \\(F\\) and \\(G\\). Let \\(\\sigma(a|s)\\) be the player’s belief about the possibility of taking \\(a\\) when the realized state is \\(s\\), which may or may not coincide with the equilibrium probability. Let \\(\\sigma\\) stack them up as: \\[\\begin{equation} \\sigma = \\begin{pmatrix} \\sigma(0|1)\\\\ \\vdots\\\\ \\sigma(0|L)\\\\ \\vdots\\\\ \\sigma(K|1)\\\\ \\vdots\\\\ \\sigma(K|L) \\end{pmatrix}. \\end{equation}\\] 7.2.6 Decision Problem The agent chooses strategy \\(a(\\cdot, \\cdot)\\) such that: \\[\\begin{equation} \\begin{split} \\max_{a(\\cdot, \\cdot)} &amp; \\pi[a(\\epsilon_0, s_0), s_0] + \\epsilon_{0 a(\\epsilon_0, s_0)}\\\\ &amp;+ \\mathbb{E}\\Bigg\\{ \\sum_{t = 1}^\\infty \\beta^t \\Bigg[\\pi(a_t, s_t) + \\epsilon_{t a(\\epsilon_t, s_t)}\\Bigg]\\Bigg|s_0, a(\\epsilon_0, s_0)\\Bigg\\} \\end{split} \\end{equation}\\] The expectation is taken with respect to his/her belief. 7.2.7 Value Function and Ex-ante Value Function When the belief about the future behavior is \\(\\sigma\\), then the value function associated with the belief is defined as: \\[\\begin{equation} \\begin{split} &amp;V(\\sigma, s_0, \\epsilon_0)\\\\ &amp;= \\sum_{a \\in A} \\sigma(a|s_0) \\Bigg\\{\\pi(a, s_0) + \\epsilon_{0a} + \\mathbb{E}\\Bigg[ \\sum_{t = 1}^\\infty \\beta^t \\sum_{a \\in A}\\sigma(a|s_t)\\Bigg(\\pi(a, s_t) + \\epsilon_{ta}\\Bigg)\\Bigg|s_0, a\\Bigg] \\Bigg\\}. \\end{split} \\end{equation}\\] It has the recursive structure as: \\[\\begin{equation} \\begin{split} &amp;V(\\sigma, s_0, \\epsilon_0)\\\\ &amp; = \\sum_{a \\in A} \\sigma(a|s_0) \\Bigg\\{\\pi(a, s_0) + \\epsilon_{0a} + \\beta \\mathbb{E}\\Bigg[V(\\sigma, s_1, \\epsilon_1)\\Bigg|s_0, a\\Bigg]\\Bigg\\}\\\\ &amp; = \\sum_{a \\in A} \\sigma(a|s_0) \\Bigg\\{\\pi(a, s_0) + \\epsilon_{0a} + \\beta \\sum_{s_1 \\in S} V(\\sigma, s_1, \\epsilon_1)g(a, s_0, s_1)\\Bigg\\}. \\end{split} \\end{equation}\\] \\(V(\\sigma, s, \\epsilon)\\) is the value function after the profitablity shock \\(\\epsilon\\) is realized. On the other hand, we define the ex-ante value function under belief \\(\\sigma\\) as: \\[\\begin{equation} V(\\sigma, s) = \\mathbb{E}\\{V(\\sigma, s, \\epsilon)|s\\}. \\end{equation}\\] 7.2.8 Choice-specific Value Function When the current state and profitability shocks are \\(s\\) and \\(\\epsilon\\) and the belief about the future behavior is \\(\\sigma\\), we define the choice-specific value function for an agent in a period as follows: \\[\\begin{equation} \\begin{split} V(\\sigma, a, s, \\epsilon) &amp;= \\pi(a , s) + \\epsilon_a + \\beta \\sum_{s&#39; \\in S} V(\\sigma, s&#39;) g(a, s, s&#39;)\\\\ &amp;= \\underbrace{\\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(\\sigma, s&#39;) g(a, s, s&#39;)}_{v(\\sigma, a, s)} + \\epsilon_a. \\end{split} \\end{equation}\\] We call \\(v(\\sigma, a, s)\\) be the choice-specific mean value function with belief \\(\\sigma\\). \\(V(\\sigma, s, \\epsilon)\\), \\(V(\\sigma, s)\\), \\(V(\\sigma, a, s, \\epsilon)\\), and \\(v(\\sigma, a, s)\\) are all different, abusing the notation. 7.2.9 Optimality Condition When the state and profitability shocks are \\(s\\) and \\(\\epsilon\\), \\(a\\) is the optimal choice if and only if: \\[\\begin{equation} v(\\sigma, a, s) + \\epsilon^{a} \\ge v(\\sigma, a&#39;, s) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A. \\end{equation}\\] This condition looks similar to the optimality condition in the static discrete choice model. The only difference from the static discrete choice model is that the mean indirect utility is the sum of the choice-specific mean profit and the discounted continuation value. Thus, as long as the mean choice-specific value function for given parameters can be computed, the following simulation and estimation procedure will be similar to the static discrete chocie model. 7.2.10 Optimal Conditional Choice Probability From the previous optimality condition, we can define the optimal conditional choice probability with belief \\(\\sigma\\) as: \\[\\begin{equation} \\begin{split} p(a|s, \\sigma) &amp;\\equiv \\mathbb{P}\\{v(\\sigma, a, s) + \\epsilon^{a} \\ge v(\\sigma, a&#39;, s) + \\epsilon^{a&#39;}, \\forall a&#39; \\in A\\}\\\\ &amp;= \\int \\prod_{a&#39; \\neq a} 1\\{v(\\sigma, a, s) + \\epsilon^{a} \\ge v(\\sigma, a&#39;, s) + \\epsilon^{a&#39;}\\} dF\\\\ &amp;\\equiv \\Psi(\\sigma, a, s). \\end{split} \\end{equation}\\] \\(\\Psi(\\sigma, a, s)\\) maps the tuple of action, state and belief to the optimal conditional choice probability of the action given the state and the belief. 7.2.11 Optimality Condition Let \\(p\\) and \\(\\Psi\\) be: \\[\\begin{equation} p = \\begin{pmatrix} p(0|1)\\\\ \\vdots\\\\ p(0|L)\\\\ \\vdots\\\\ p(K|1)\\\\ \\vdots\\\\ p(K|L) \\end{pmatrix}, \\end{equation}\\] \\[\\begin{equation} \\Psi(\\sigma) = \\begin{pmatrix} \\Psi(\\sigma, 0, 1)\\\\ \\vdots\\\\ \\Psi(\\sigma, 0, L)\\\\ \\vdots\\\\ \\Psi(\\sigma, K, 1)\\\\ \\vdots\\\\ \\Psi(\\sigma, K, L) \\end{pmatrix}. \\end{equation}\\] The optimality condition with respect to the conditional choice probabilities given the belief is written as: \\[\\begin{equation} p = \\Psi(\\sigma). \\end{equation}\\] The rational expectation hypothesis requires that the belief about the future behavior coincides with the optimal conditional choice probability, i.e.: \\[\\begin{equation} p = \\Psi(p). \\end{equation}\\] The optimal conditional choice probability under the rational expectation hypothesis is characterized as a fixed point of mapping \\(\\Psi\\). 7.2.12 Mapping from a Conditional Choice Probability to an Ex-ante Value Function Inserting \\(\\sigma = p\\), we obtain a mapping from an optimal conditional choice probability to an ex-ante value function such as: \\[\\begin{equation} \\begin{split} V(s) &amp;= \\mathbb{E}\\{V(s, \\epsilon)|s\\}\\\\ &amp;= \\mathbb{E}\\left[ \\sum_{t = 0}^\\infty \\beta^t \\sum_{a \\in A}p(a|s_t)\\left[\\pi(a, s_t) + \\epsilon_{ta}\\right]\\Bigg|s^0, a\\right]\\\\ &amp;\\equiv \\varphi(p, s). \\end{split} \\end{equation}\\] 7.2.13 Mapping from an Ex-ante Value Function to an Optimal Conditional Choice Probability On the other hand, we can derive a mapping from an ex-ante value function to an optimal conditional choice probability such as: \\[\\begin{equation} \\begin{split} p(a|s) = \\mathbb{P}\\Bigg\\{&amp;\\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a, s, s&#39;) + \\epsilon_a \\ge\\\\ &amp;\\pi(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a&#39;, s, s&#39;) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A \\Bigg\\}\\\\ &amp;\\equiv \\Lambda(V, s). \\end{split} \\end{equation}\\] 7.2.14 The Optimality Conditions Composing these two mappings, we can write down the optimality condition as the fixed-point for ex-ante value functions: \\[\\begin{equation} V = \\varphi(p) = \\varphi[\\Lambda(V)] \\equiv \\Phi(V). \\end{equation}\\] Or as the fixed-point for the optimal conditional choice probabilities: \\[ p = \\Lambda(V) = \\Lambda[\\varphi(p)] \\equiv \\Psi(p). \\] 7.2.15 Fixed-point Algorithm If \\(\\epsilon\\) is drawn from an i.i.d. type-I extreme value distribution, we can derive the mapping from the ex-ante value function to the conditional choice probability in the closed form: \\[\\begin{equation} \\begin{split} \\Lambda(V, s) &amp;= \\mathbb{P}\\{\\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a, s, s&#39;) + \\epsilon_{a} \\ge\\\\ &amp; \\pi(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a&#39;, s, s&#39;) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A\\}\\\\ &amp;=\\frac{\\exp[\\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a, s, s&#39;)]}{\\sum_{a&#39; \\in A} \\exp[\\pi(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a&#39;, s, s&#39;)]}. \\end{split} \\end{equation}\\] Moreover, we can also derive the mapping from the ex-ante value function to the ex-ante value function as follows: \\[\\begin{equation} \\begin{split} \\Phi(V) &amp;= \\mathbb{E}\\{\\max_{a \\in A} \\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a, s, s&#39;) + \\epsilon_{a}\\} \\\\ &amp;=\\log \\Bigg\\{\\sum_{a \\in A} \\exp[\\pi(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g(a, s, s&#39;)] \\Bigg\\} + \\gamma, \\end{split} \\end{equation}\\] where \\(\\gamma\\) is Euler’s constant. \\(\\Phi\\) is shown to be a contraction mapping as long as \\(\\beta &lt; 1\\). Thus, we can solve the model by starting from an arbitrary ex-ante value function \\(V\\) and by iterating the mapping \\(\\Phi\\) until the change in the ex-ante value functions is below a certain threshold. Rust (1987) gives the result for more general distributional assumption for \\(\\epsilon\\). 7.3 Identification 7.3.1 Unidentification Result The identification of the single-agent dynamic decision model is studied in Magnac &amp; Thesmar (2002). The model primitives are \\((\\Pi, F, \\beta, G)\\). The transition probability \\(G\\) is directly identified from the data because \\(a, s, s&#39;\\) are observed by econometrician. It is known that the model is in general not identified. The discount factor \\(\\beta\\) is hard to identify. It determines the weights between the current and future profits. Suppose that a firm makes a large investment. This may be because the firm overweights the future (high \\(\\beta\\)) or because the investment cost is low (\\(\\pi\\) is such that the investment cost is low). We cannot distinguish between these two possibilities. To identify it, you need some instruments that changes the future return to the investment but does not affect today’s payoff. 7.3.2 Identification when \\(\\beta\\) and \\(F\\) is Known We often fix \\(\\beta\\) and assume the distribution \\(F\\) and only consider the identification of \\(\\Pi\\). Note that the optimal conditional choice probability is directly identified from the data because \\(s\\) and \\(a\\) are observed. Then the optimality condition under the rational expectation hypothesis gives the following \\(KL\\) system of equations: \\[ p = \\Psi(p). \\] On the other hand, the dimension of parameter \\(\\Pi\\) is in general \\((K + 1)L\\) (the mean profit at a state and an action). One possible restriction is to assume that \\(\\pi(0, s)\\) are known for any \\(s\\). For example, assume that \\(a = 0\\) means that the firm is inactive and so \\(\\pi(0, s) = 0\\). 7.3.3 Crucial Assumptions for the Argument The following assumptions are crucial for the above argument. Conditional i.i.d. Unobservable: The profit shocks that are unobservable to econometrician are i.i.d. conditional on the observable state. Conditional Independence of Future Observable State: \\[\\begin{equation} \\mathbb{P}\\{s_{t + 1}|s_t, a_t, \\epsilon_t\\} = \\mathbb{P}\\{s_{t + 1}|s_t, a_t\\}. \\end{equation}\\] If the first assumption is violated, the choice probability cannot be written as a function only of the observable state of the period. If \\(\\epsilon_t\\) is serially correlated, to integrate over \\(\\epsilon_{t + 1}\\) we have to condition on \\(\\epsilon_t\\). Then, we may not be able to identify the optimal conditional choice probability from the data/ If the second assumption is violated, for the same reason, we may not be able to identify the state transition law. Kasahara &amp; Shimotsu (2009) proves the identification when the first assumption is violated: there is a player-specific fixed effect that has a finite-mixture structure. 7.4 Estimation by Nested Fixed-point Algorithm 7.4.1 Nested Fixed-Point Algorithm A straightforward way of estimating the single-agent dynamic model is to solve the optimal conditional choice probability by a fixed-point algorithm for each parameter and evaluate the likelihood function using the optimal conditional choice probability. Because a fixed-point algorithm is nested in the parameter search, Rust (1987) named it the nested fixed-point algorithm. 7.4.2 Solving for the Ex-ante Value Function Let \\(\\theta_1\\) be the parameters that determine \\(\\Pi\\), \\(\\theta_2\\) be the parameters that determine \\(G\\), and \\(\\theta = (\\theta_1&#39;, \\theta_2&#39;)&#39;\\). The ex-ante value function is a fixed point of a contraction mapping \\(\\Phi^\\theta\\) such that: \\[\\begin{equation} \\begin{split} \\Phi^\\theta(p) &amp;=\\log \\Bigg\\{\\sum_{a \\in A} \\exp[\\pi^{\\theta_1}(a , s) + \\beta \\sum_{s&#39; \\in S} V(s&#39;) g^{\\theta_2}(a, s, s&#39;)] \\Bigg\\} + \\gamma, \\end{split} \\end{equation}\\] Let \\(V^{\\theta (0)}\\) be an arbitrary initial function and define \\(V^{\\theta (r + 1)}\\) by: \\[ V^{\\theta (r + 1)} = \\Phi^{\\theta}(V^{\\theta (r)}). \\] We iterate this until \\(|V^{\\theta (r + 1)} - V^{\\theta (r)}|\\) is below a certain threshold. Let \\(V^{^\\theta (\\ast)}\\) be the solution to the fixed-point algorithm. Then, we can derive the optimal choice probability by: \\[\\begin{equation} \\begin{split} p^{\\theta (\\ast)} = \\varphi\\left[V^{\\theta (\\ast)}\\right]. \\end{split} \\end{equation}\\] These are the ex-ante value function and optimal conditional choice probabilities under parameters \\(\\theta\\). 7.4.3 Estimation by Nested Fixed-Point Algorithm The previous algorithm allows us to derive the optimal choice probability given parameters. Then it is straight forward to evaluate the likelihood function given observations \\(\\{a_t, s_t\\}_{t = 1}^T\\) by: \\[\\begin{equation} \\begin{split} &amp;L(\\theta; \\{a_t, s_t\\}_{t = 1}^T) =\\prod_{t = 1}^T \\prod_{a_t = 0}^1 p^{\\theta (\\ast)}(a_t|s_t)^{a_t} g^{\\theta_2} (s_{t + 1}|s_t, a_t). \\end{split} \\end{equation}\\] and so the log likelihood function is: \\[\\begin{equation} \\begin{split} &amp;l(\\theta; \\{a_t, s_t\\}_{t = 1}^T)\\\\ &amp;=\\sum_{t = 1}^T \\sum_{a_t = 0}^1 a_t \\log [p^{\\theta (\\ast)}(a_t|s_t)] + \\sum_{t = 1}^T \\log [g^{\\theta_2} (s_{t + 1}|s_t, a_t)]. \\end{split} \\end{equation}\\] 7.4.4 Full and Partial Likelihood We can find \\(\\theta\\) that maximizes the full log likelihood \\(l(\\theta; \\{a_t, s_t\\}_{t = 1}^T)\\) to estimate the model. However, the convergence takes longer as the number of parameters are larger. Parameters that govern the state transition is estimated by finding \\(\\theta_2\\) that maximizes the partial likelihood: \\[\\begin{equation} \\hat{\\theta}_2 = \\text{argmax}_{\\theta_2} \\sum_{t = 1}^T \\log g^{\\theta_2}(s_{t + 1}|s_t, a_t). \\end{equation}\\] Then we can estimate \\(\\theta_1\\) by finding \\(\\theta_1\\) that maximizes the partial likelihood: \\[\\begin{equation} \\hat{\\theta}_1 = \\text{argmax}_{\\theta_1} \\sum_{t = 1}^T \\sum_{a_t = 0}^1 a_t \\log [p^{(\\theta_1, \\hat{\\theta}_2) (\\ast)}(a_t|s_t)]. \\end{equation}\\] This causes some efficiency loss but speeds up the computation, because we can estimate \\(\\theta_2\\) without solving the fixed-point. 7.5 Estimation by Conditional Choice Probability (CCP) Approach 7.5.1 CCP Approach Conditional Choice Probability (CCP) approach suggested by Hotz &amp; Miller (1993) significantly reduces the computation time at the cost of some efficiency. This approach can be applied to many other settings. The idea is: We can identify the optimal conditional choice probability \\(p^\\theta\\) directly from the data. This is a reduced-form parameter (cf. \\(\\theta\\) is the structural parameters) of the model. The optimality condition \\(p^\\theta = \\Psi^\\theta(p^\\theta)\\) can be regarded as a moment condition. In the nested fixed-point algorithm, we find \\(p^\\theta\\) that solves the optimality condition given \\(\\theta\\) to compute the likelihood. In CCP approach, we find \\(\\theta\\) that solves the optimality condition given \\(p^\\theta\\) that is identified directly from the data. 7.5.2 First Step: Estimating CCP The first step of the CCP approach is to estimate the conditional choice probability and transition probability. If everything is discrete, it is nothing but the empirical distribution: \\[\\begin{equation} \\begin{split} &amp;\\hat{p}(a|s) = \\frac{\\sum_{i = 1}^N \\sum_{t = 1}^T 1\\{a_{it} = a, s_{it} = s\\}}{\\sum_{i = 1}^N \\sum_{t = 1} 1\\{s_{it} = s\\}},\\\\ &amp;\\hat{g}(s&#39;|s, a) = \\frac{\\sum_{i = 1}^N \\sum_{t = 1}^T 1\\{s_{i, t + 1} = s&#39;, s_{it} = s, a_{it} = a\\}}{\\sum_{i = 1}^N \\sum_{t = 1} 1\\{s_{it} = s, a_{it} = a\\}}. \\end{split} \\end{equation}\\] We can of course use a parametric model. For example, we may estimate the conditional choice probability with a multinomial logit models: \\[\\begin{equation} \\begin{split} &amp;\\hat{p}(a|s) = \\frac{\\exp[\\hat{\\beta} a + \\hat{\\gamma} s)]}{\\sum_{a&#39; \\in A} \\exp[\\hat{\\beta} a&#39; + \\hat{\\gamma} s)]}. \\end{split} \\end{equation}\\] 7.5.3 First Step: Estimating CCP What is the estimated CCP \\(\\hat{p}\\)? This is the optimal conditional choice probability at a particular equilibrium under a true parmaeter. If parameter changes, then the equilibrium changes. Then, the conditional choice probability also changes. The reduced-form parameter \\(\\hat{p}\\) embodies the information about behaviors under the actual equilibrium but does not tell anything about behaviors under hypothetical equilibria. Therefore, \\(\\hat{p}\\) is not sufficient to make counterfactual prediction. 7.5.4 Second Step: Estimating Structural Parameters Among structural parameters \\(\\theta\\), parameters in the transition probability \\(\\theta_2\\) is already identified from the data in the first step. How do we identify \\(\\theta_1\\), parameters in the profit function \\(\\pi\\)? If we fix \\(\\theta_1\\), in theory, we can compute: \\[\\begin{equation} \\begin{split} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s) = \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p}, s) = \\mathbb{E}\\Bigg[ \\sum_{t = 0}^\\infty \\beta^t \\sum_{a \\in A}\\hat{p}(a|s_t)\\Bigg(\\pi^{\\theta_1}(a, s_t) + \\epsilon_{ta}\\Bigg)\\Bigg|s\\Bigg],\\\\ \\end{split} \\end{equation}\\] although the expectation may not have a closed form solution. 7.5.5 Second Step: Estimating Structural Parameters In addition, if we fix \\(\\theta_1\\), in theory, we can compute: \\[\\begin{equation} \\begin{split} &amp;\\Lambda^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}, a, s)\\\\ &amp;\\equiv \\mathbb{P}\\Bigg\\{\\pi^{\\theta_1}(a , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) + \\epsilon_a\\\\ &amp;\\ge \\pi^{\\theta_1}(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a&#39;, s, s&#39;) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A \\Bigg\\} \\end{split} \\end{equation}\\] Combining these two mappings, we can compute: \\[\\begin{equation} \\Psi^{(\\theta_1, \\hat{\\theta_2})}(\\hat{p}) = \\Lambda^{(\\theta_1, \\hat{\\theta}_2)}[\\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p})]. \\end{equation}\\] Then, we can find \\(\\theta_1\\) that minimizes the distance between \\(\\hat{p}\\) and \\(\\Psi^{(\\theta_1, \\hat{\\theta_2})}(\\hat{p})\\) to find \\(\\theta_1\\) that is consistent with the observed conditional choice probabilities. 7.5.6 Type-I Extreme Value Distribution \\(\\Lambda\\) and \\(\\varphi\\) do no have closed form expressions in general. Exception is the case where the profitability shocks \\(\\epsilon_{ta}\\) is drawn from i.i.d. type-I extreme value distribution. First, we know that \\(\\Lambda\\) can be written as: \\[\\begin{equation} \\begin{split} &amp;\\Lambda^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}, a, s)\\\\ &amp;\\equiv \\mathbb{P}\\Bigg\\{\\pi^{\\theta_1}(a , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) + \\epsilon_a \\ge\\\\ &amp;\\pi^{\\theta_1}(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a&#39;, s, s&#39;) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A \\Bigg\\}\\\\ &amp;=\\frac{\\exp\\Big[\\pi^{\\theta_1}(a , s) + \\beta \\sum_{s&#39; \\in S}\\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;)\\Big]}{\\sum_{a&#39; = 0}^K \\exp\\Big[\\pi^{\\theta_1}(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a&#39;, s, s&#39;) \\Big]}. \\end{split} \\end{equation}\\] Second, we can show that \\(\\varphi\\) has a closed form solution: \\[\\begin{equation} \\begin{split} &amp;\\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p, s)\\\\ &amp;\\equiv \\mathbb{E}\\{V(s, \\epsilon)|s\\}\\\\ &amp;= \\mathbb{E}\\Bigg[ \\sum_{t = 0}^\\infty \\beta^t \\sum_{a \\in A}\\hat{p}(a|s_t)\\Bigg(\\pi^{\\theta_1}(a, s_t) + \\epsilon_{ta}\\Bigg)\\Bigg|s\\Bigg]\\\\ &amp;=\\mathbb{E}\\Bigg[\\sum_{a \\in A}\\hat{p}(a|s)\\Bigg(\\pi^{\\theta_1}(a, s) + \\epsilon_{a} + \\beta \\sum_{s&#39; \\in S} \\mathbb{E}\\{\\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s, \\epsilon)|s&#39;\\} g^{\\hat{\\theta}_2}(a, s, s&#39;) \\Bigg)\\Bigg|s\\Bigg]\\\\ &amp;=\\mathbb{E}\\Bigg[\\sum_{a \\in A}\\hat{p}(a|s)\\Bigg(\\pi^{\\theta_1}(a, s) + \\epsilon_{a} + \\beta \\sum_{s&#39; \\in S} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p}, s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) \\Bigg)\\Bigg|s\\Bigg]\\\\ &amp;=\\sum_{a \\in A}\\hat{p}(a|s)\\Bigg(\\pi^{\\theta_1}(a, s) + \\mathbb{E}[\\epsilon_{a}|s, a] + \\beta \\sum_{s&#39; \\in S} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p}, s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) \\Bigg) \\end{split} \\end{equation}\\] We need closed form expression of \\(\\mathbb{E}[\\epsilon_{a}|s, a]\\): the expected value of choice\\(-a\\) specific profitability shock conditional on that state is \\(s\\) and \\(a\\) is optimal (\\(\\neq\\) unconditional mean of \\(\\epsilon_a\\)). 7.5.7 Type-I Extreme Value Distribution If \\(\\epsilon^a\\) is drawn from i.i.d. type-I extreme value distribution, tt can be shown that: \\[\\begin{equation} \\begin{split} \\mathbb{E}[\\epsilon_{a}|s, a] &amp;= \\hat{p}(a|s)^{-1} \\int \\epsilon_a 1\\Bigg\\{\\pi^{\\theta_1}(a , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) + \\epsilon_a \\ge\\\\ &amp;\\pi^{\\theta_1}(a&#39; , s) + \\beta \\sum_{s&#39; \\in S} \\hat{V}^{(\\theta_1, \\hat{\\theta}_2)}(s&#39;) g^{\\hat{\\theta}_2}(a&#39;, s, s&#39;) + \\epsilon_{a&#39;}, \\forall a&#39; \\in A \\Bigg\\}dF(e)\\\\ &amp;= \\gamma - \\ln \\hat{p}(a|s), \\end{split} \\end{equation}\\] where \\(\\gamma\\) is Euler’s constant: \\[\\begin{equation} \\gamma \\equiv \\lim_{n \\to \\infty} \\Bigg(\\sum_{k = 1}^n \\frac{1}{k} - \\ln(n) \\Bigg) \\approx 0.57721... \\end{equation}\\] 7.5.8 Type-I Extreme Value Distribution Inserting this into the previous expression, we get: \\[\\begin{equation} \\begin{split} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p}, s) = \\sum_{a \\in A}\\hat{p}(a|s)\\Bigg(\\pi^{\\theta_1}(a, s) + \\gamma - \\ln \\hat{p}(a|s) + \\beta \\sum_{s&#39; \\in S} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(\\hat{p}, s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;) \\Bigg). \\end{split} \\end{equation}\\] 7.5.9 Type-I Extreme Value Distribution Write the continuation value in a matrix form: \\[\\begin{equation} \\begin{split} &amp; \\sum_{s&#39; \\in S} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p, s&#39;) g^{\\hat{\\theta}_2}(a, s, s&#39;)\\\\ &amp; = [g^{\\hat{\\theta}_2}(a, s, 1), \\cdots, g^{\\hat{\\theta}_2}(a, s, L)] \\underbrace{\\begin{bmatrix} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p, 1)\\\\ \\vdots\\\\ \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p, L). \\end{bmatrix}}_{\\equiv \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p)} \\end{split} \\end{equation}\\] 7.5.10 Type-I Extreme Value Distribution Write the ex-ante value function in a matrix form: \\[\\begin{equation} \\begin{split} &amp;\\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p, s)\\\\ &amp;=\\underbrace{[p(0|s), \\cdots, p(K|s)]}_{\\equiv p(s)&#39;} \\\\ &amp;\\times\\begin{bmatrix} \\underbrace{\\begin{bmatrix} \\pi^{\\theta_1}(0, s)\\\\ \\vdots\\\\ \\pi^{\\theta_1}(K, s) \\end{bmatrix}}_{\\equiv \\pi^{\\theta_1}(s)} + \\gamma - \\underbrace{\\begin{bmatrix} \\ln p(0|s)\\\\ \\vdots\\\\ \\ln p(K|s) \\end{bmatrix}}_{\\equiv \\ln p(s)} +\\beta \\underbrace{\\begin{bmatrix} g^{\\hat{\\theta}_2}(0, s, 1), \\cdots, g^{\\hat{\\theta}_2}(0, s, L)\\\\ \\vdots\\\\ g^{\\hat{\\theta}_2}(K, s, 1), \\cdots, g^{\\hat{\\theta}_2}(K, s, L) \\end{bmatrix}}_{\\equiv G^{\\hat{\\theta}_2}(s)} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p) \\end{bmatrix}\\\\ &amp;=p(s)&#39;[\\pi^{\\theta_1}(s) + \\gamma - \\ln p(s)] + \\beta p(s)&#39; G^{\\hat{\\theta}_2}(s) \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p) \\end{split} \\end{equation}\\] 7.5.11 Type-I Extreme Value Distribution Stacking up for \\(s\\), we get: \\[\\begin{equation} \\begin{split} &amp;\\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p) = \\begin{bmatrix} p(1)&#39;[\\pi^{\\theta_1}(1) + \\gamma - \\ln p(1)]\\\\ \\vdots\\\\ p(L)&#39;[\\pi^{\\theta_1}(L) + \\gamma - \\ln p(L)] \\end{bmatrix} +\\beta \\begin{bmatrix} p(1)&#39; G^{\\hat{\\theta}_2}(1)\\\\ \\vdots\\\\ p(L)&#39; G^{\\hat{\\theta}_2}(L) \\end{bmatrix} \\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p)\\\\ &amp;\\Leftrightarrow\\\\ &amp;\\varphi^{(\\theta_1, \\hat{\\theta}_2)}(p) = \\begin{bmatrix} I - \\beta \\begin{bmatrix} p(1)&#39; G^{\\hat{\\theta}_2}(1)\\\\ \\vdots\\\\ p(L)&#39; G^{\\hat{\\theta}_2}(L) \\end{bmatrix} \\end{bmatrix}^{-1} \\begin{bmatrix} p(1)&#39;[\\pi^{\\theta_1}(1) + \\gamma - \\ln p(1)]\\\\ \\vdots\\\\ p(L)&#39;[\\pi^{\\theta_1}(L) + \\gamma - \\ln p(L)] \\end{bmatrix}. \\end{split} \\end{equation}\\] Note that you can get this expression even if the profitability shocks are not type-I extreme value, although you need numerical integration for \\(\\mathbb{E}\\{\\epsilon^a|s, a\\}\\) instead of the analytical solution \\(\\gamma - \\ln p(a|s)\\). 7.5.12 General Distribution If the profitability shock \\(\\epsilon^a\\) is not an i.i.d. type-I extreme value random variable, you may need to compute \\(\\mathbb{E}\\{\\epsilon^a|s, a\\}\\) and \\(\\Lambda^{(\\theta_1, \\hat{\\theta}_2)}(V)\\) numerically. This may or may not feasible. References "]
]
